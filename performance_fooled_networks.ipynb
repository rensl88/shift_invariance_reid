{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"acc shifted.ipynb","provenance":[{"file_id":"1H88E0ZNIIpeUDNOdhJ0LCiuck_Z9kC69","timestamp":1575466061596}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XUUdKf4fnbPE","colab_type":"text"},"source":["# Shift in input \n"]},{"cell_type":"code","metadata":{"id":"gcE876G3Y-HU","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import os\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","import cv2\n","from PIL import Image\n","\n","import os\n","import random\n","import shutil\n","import time\n","import warnings\n","import sys\n","import numpy as np\n","import os\n","import pandas as pd\n","import h5py\n","import logging\n","import pickle\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.distributed as dist\n","import torch.optim\n","import torch.multiprocessing as mp\n","import torch.utils.data\n","import torch.utils.data.distributed\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from torchsummary import summary\n","from torch.utils.data import DataLoader\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3xjSN_EEG6K","colab_type":"code","outputId":"e447f1d5-c631-48c5-c441-5e4044ab57cd","executionInfo":{"status":"ok","timestamp":1575466273936,"user_tz":-60,"elapsed":23298,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H_A0VUsSEoaB","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/drive/My Drive/Thesis re-id/triplet-reid-master\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZFrry_x4OY0","colab_type":"code","colab":{}},"source":["from own_code.loss import TripletLoss\n","from own_code.triplet_selector import BatchHardTripletSelector\n","from own_code.batch_sampler import BatchSampler\n","from own_code.Market1501 import Market1501\n","#from own_code.Market1501_shift import Market1501\n","from own_code.backbone import EmbedNetwork\n","#from own_code.Market1501_no_aug import Market1501\n","from own_code.optimizer import AdamOptimWrapper\n","from logger import logger\n","from models_lpf import *\n","#import models_lpf.resnet\n","import models_lpf.resnet\n","from utils import pdist_np as pdist\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jkMNav6ZQiyx","colab_type":"code","colab":{}},"source":["# Labels\n","labels = pd.read_csv('data/market1501_train.csv', names = ['pid', 'fid'], header = None, dtype = str)\n","labels_query = pd.read_csv('data/market1501_query.csv', names = ['pid', 'fid'], header = None, dtype = str)\n","labels_test = pd.read_csv('data/market1501_test.csv', names = ['pid', 'fid'], header = None, dtype = str)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRDOEnZ9DPSI","colab_type":"text"},"source":["### Reading the h5py Files"]},{"cell_type":"code","metadata":{"id":"5h--hlJ7exXD","colab_type":"code","colab":{}},"source":["fileName = 'data_final.h5'\n","fileName_qt = 'data_qt.h5'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfbXc61suBbv","colab_type":"code","colab":{}},"source":["with h5py.File(fileName, \"a\") as out:\n","  X_train = np.asarray(out[\"X_train\"])\n","  #Y_train = np.asarray(out[\"Y_train\"])\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5aLjJp2ew8F","colab_type":"code","colab":{}},"source":["with h5py.File(fileName_qt, \"a\") as out:\n","  X_query = np.asarray(out[\"X_dev\"])\n","  X_test = np.asarray(out[\"X_test\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Omy7LeJygEb","colab_type":"code","outputId":"575266cd-8d4b-4952-e6f9-90ea452eff6f","executionInfo":{"status":"ok","timestamp":1575466329196,"user_tz":-60,"elapsed":34014,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_query.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3368, 128, 64, 3)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"gwcqAbiAyjr3","colab_type":"code","outputId":"fbc62063-9684-4bd2-85d8-3827d19f1e0c","executionInfo":{"status":"ok","timestamp":1575466329197,"user_tz":-60,"elapsed":34004,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sum(X_query).shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(128, 64, 3)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"SsRMLDrFn4hT","colab_type":"text"},"source":["## Embeddings\n"]},{"cell_type":"code","metadata":{"id":"j65bli6yr2wr","colab_type":"code","colab":{}},"source":["def create_emb(dataset, fids, net,  store_path, model_num):\n","  torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","  ## logging\n","  FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","  logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","  logger = logging.getLogger(__name__)\n","  ## restore model\n","  logger.info('restoring model')\n","  model = net\n","  #model = nn.DataParallel(model)\n","  model = net.cuda()\n","  model.module.load_state_dict(torch.load('./res/model{}.pkl'.format(model_num)))\n","  model = nn.DataParallel(model)\n","  model.eval()\n","\n","  ## load gallery dataset\n","  batchsize = 32\n","  ds = Market1501(pids_list=list(fids), array=dataset, is_train = False)\n","  dl = DataLoader(ds, batch_size = batchsize, drop_last = False, num_workers = 4)\n","\n","  ## embedding samples\n","  logger.info('start embedding')\n","  all_iter_nums = len(ds) // batchsize + 1\n","  embeddings = []\n","  label_ids = []\n","  label_cams = []\n","  for it, (img, lb_id, lb_cam) in enumerate(dl):\n","    print('\\r=======>  processing iter {} / {}'.format(it, all_iter_nums),\n","            end = '', flush = True)\n","    label_ids.append(lb_id)\n","    label_cams.append(lb_cam)\n","    embds = []\n","    for im in img:\n","        im = im.cuda()\n","        embd = model(im).detach().cpu().numpy()\n","        embds.append(embd)\n","    embed = sum(embds) / len(embds)\n","    embeddings.append(embed)\n","  print('  ...   completed')\n","\n","  embeddings = np.vstack(embeddings)\n","  label_ids = np.hstack(label_ids)\n","  label_cams = np.hstack(label_cams)\n","\n","  ## dump results\n","  logger.info('dump embeddings')\n","  embd_res = {'embeddings': embeddings, 'label_ids': label_ids, 'label_cams': label_cams}\n","  with open(store_path, 'wb') as fw:\n","    pickle.dump(embd_res, fw)\n","\n","  logger.info('embedding finished')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"efT-nTb7n7Gy","colab_type":"code","colab":{}},"source":["def create_emb_shift(dataset, fids, net, store_path, model_num):\n","  torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","  ## logging\n","  FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","  logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","  logger = logging.getLogger(__name__)\n","  ## restore model\n","  logger.info('restoring model')\n","  model = net\n","  #model = nn.DataParallel(model)\n","  model = net.cuda()\n","  model.module.load_state_dict(torch.load('./res/model{}.pkl'.format(model_num)))\n","  model = nn.DataParallel(model)\n","  model.eval()\n","\n","  ## load gallery dataset\n","  batchsize = 32\n","  ds = Market1501(pids_list=list(fids), array=dataset, is_train = False)\n","  dl = DataLoader(ds, batch_size = batchsize, drop_last = False, num_workers = 4)\n","\n","  ## embedding samples\n","  logger.info('start embedding')\n","  all_iter_nums = len(ds) // batchsize + 1\n","  embeddings = []\n","  label_ids = []\n","  label_cams = []\n","  \n","  for it, (img, lb_id, lb_cam) in enumerate(dl):\n","    print('\\r=======>  processing iter {} / {}'.format(it, all_iter_nums),\n","            end = '', flush = True)\n","    label_ids.append(lb_id)\n","    label_cams.append(lb_cam)\n","    embds = []\n","    #print(img.shape)\n","    offa = np.random.randint(32,size=2)\n","    offb = np.random.randint(16,size=2)\n","    #print(offa,offb)\n","    for im in img:\n","      #print(im.shape)\n","      im = im[:,:,offa[0]:offa[0]+224,offb[0]:offb[0]+112]\n","      #print(im)\n","      im = im.cuda()\n","      embd = model(im).detach().cpu().numpy()\n","      embds.append(embd)\n","      #print(\"end inner\")\n","    embed = sum(embds) / len(embds)\n","    embeddings.append(embed)\n","  print('  ...   completed')\n","\n","  embeddings = np.vstack(embeddings)\n","  label_ids = np.hstack(label_ids)\n","  label_cams = np.hstack(label_cams)\n","\n","  ## dump results\n","  logger.info('dump embeddings')\n","  embd_res = {'embeddings': embeddings, 'label_ids': label_ids, 'label_cams': label_cams}\n","  with open(store_path, 'wb') as fw:\n","    pickle.dump(embd_res, fw)\n","\n","  logger.info('embedding finished')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwdlBg0T3pQQ","colab_type":"code","colab":{}},"source":["from utils import pdist_np as pdist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ooSi6py4LGh","colab_type":"code","colab":{}},"source":["def evaluate(test_embs, query_embs, cmc_rank):\n","    ## logging\n","    FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","    logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","    logger = logging.getLogger(__name__)\n","\n","    ## load embeddings\n","    logger.info('loading gallery embeddings')\n","    with open(test_embs, 'rb') as fr:\n","        gallery_dict = pickle.load(fr)\n","        emb_gallery, lb_ids_gallery, lb_cams_gallery = gallery_dict['embeddings'], gallery_dict['label_ids'], gallery_dict['label_cams']\n","    logger.info('loading query embeddings')\n","    with open(query_embs, 'rb') as fr:\n","        query_dict = pickle.load(fr)\n","        emb_query, lb_ids_query, lb_cams_query = query_dict['embeddings'], query_dict['label_ids'], query_dict['label_cams']\n","\n","    ## compute and clean distance matrix\n","    dist_mtx = pdist(emb_query, emb_gallery)\n","    n_q, n_g = dist_mtx.shape\n","    indices = np.argsort(dist_mtx, axis = 1)\n","    matches = lb_ids_gallery[indices] == lb_ids_query[:, np.newaxis]\n","    matches = matches.astype(np.int32)\n","    all_aps = []\n","    all_cmcs = []\n","    logger.info('starting evaluating ...')\n","    for qidx in tqdm(range(n_q)):\n","        qpid = lb_ids_query[qidx]\n","        qcam = lb_cams_query[qidx]\n","\n","        order = indices[qidx]\n","        pid_diff = lb_ids_gallery[order] != qpid\n","        cam_diff = lb_cams_gallery[order] != qcam\n","        useful = lb_ids_gallery[order] != -1\n","        keep = np.logical_or(pid_diff, cam_diff)\n","        keep = np.logical_and(keep, useful)\n","        match = matches[qidx][keep]\n","\n","        if not np.any(match): continue\n","\n","        cmc = match.cumsum()\n","        cmc[cmc > 1] = 1\n","        all_cmcs.append(cmc[:cmc_rank])\n","\n","        num_real = match.sum()\n","        match_cum = match.cumsum()\n","        match_cum = [el / (1.0 + i) for i, el in enumerate(match_cum)]\n","        match_cum = np.array(match_cum) * match\n","        ap = match_cum.sum() / num_real\n","        all_aps.append(ap)\n","\n","    assert len(all_aps) > 0, \"NO QUERY MATCHED\"\n","    mAP = sum(all_aps) / len(all_aps)\n","    all_cmcs = np.array(all_cmcs, dtype = np.float32)\n","    cmc = np.mean(all_cmcs, axis = 0)\n","\n","    print('mAP is: {}, cmc is: {}'.format(mAP, cmc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycSY9JPqrsAb","colab_type":"text"},"source":["## Model 1"]},{"cell_type":"code","metadata":{"id":"8fGgot3hmcym","colab_type":"code","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HF_imTHacwCO","colab_type":"code","outputId":"28aed640-2eda-4e3f-e71e-471383430532","executionInfo":{"status":"ok","timestamp":1575469746956,"user_tz":-60,"elapsed":2064,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model_num = 1\n","logger.info('setting up backbone model and loss')\n","net = EmbedNetwork(pretrained_base=True).cuda()\n","net_1 = nn.DataParallel(net)\n","summary(net_1, (3,128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["setting up backbone model and loss\n"],"name":"stderr"},{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 32, 16]               0\n","            Conv2d-5           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-6           [-1, 64, 32, 16]             128\n","              ReLU-7           [-1, 64, 32, 16]               0\n","            Conv2d-8           [-1, 64, 32, 16]          36,864\n","       BatchNorm2d-9           [-1, 64, 32, 16]             128\n","             ReLU-10           [-1, 64, 32, 16]               0\n","           Conv2d-11          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-12          [-1, 256, 32, 16]             512\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","             ReLU-15          [-1, 256, 32, 16]               0\n","       Bottleneck-16          [-1, 256, 32, 16]               0\n","           Conv2d-17           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-18           [-1, 64, 32, 16]             128\n","             ReLU-19           [-1, 64, 32, 16]               0\n","           Conv2d-20           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-21           [-1, 64, 32, 16]             128\n","             ReLU-22           [-1, 64, 32, 16]               0\n","           Conv2d-23          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-24          [-1, 256, 32, 16]             512\n","             ReLU-25          [-1, 256, 32, 16]               0\n","       Bottleneck-26          [-1, 256, 32, 16]               0\n","           Conv2d-27           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-28           [-1, 64, 32, 16]             128\n","             ReLU-29           [-1, 64, 32, 16]               0\n","           Conv2d-30           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-31           [-1, 64, 32, 16]             128\n","             ReLU-32           [-1, 64, 32, 16]               0\n","           Conv2d-33          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-34          [-1, 256, 32, 16]             512\n","             ReLU-35          [-1, 256, 32, 16]               0\n","       Bottleneck-36          [-1, 256, 32, 16]               0\n","           Conv2d-37          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-38          [-1, 128, 32, 16]             256\n","             ReLU-39          [-1, 128, 32, 16]               0\n","           Conv2d-40           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-41           [-1, 128, 16, 8]             256\n","             ReLU-42           [-1, 128, 16, 8]               0\n","           Conv2d-43           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-44           [-1, 512, 16, 8]           1,024\n","           Conv2d-45           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-46           [-1, 512, 16, 8]           1,024\n","             ReLU-47           [-1, 512, 16, 8]               0\n","       Bottleneck-48           [-1, 512, 16, 8]               0\n","           Conv2d-49           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-50           [-1, 128, 16, 8]             256\n","             ReLU-51           [-1, 128, 16, 8]               0\n","           Conv2d-52           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-53           [-1, 128, 16, 8]             256\n","             ReLU-54           [-1, 128, 16, 8]               0\n","           Conv2d-55           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 512, 16, 8]           1,024\n","             ReLU-57           [-1, 512, 16, 8]               0\n","       Bottleneck-58           [-1, 512, 16, 8]               0\n","           Conv2d-59           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-60           [-1, 128, 16, 8]             256\n","             ReLU-61           [-1, 128, 16, 8]               0\n","           Conv2d-62           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-63           [-1, 128, 16, 8]             256\n","             ReLU-64           [-1, 128, 16, 8]               0\n","           Conv2d-65           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 512, 16, 8]           1,024\n","             ReLU-67           [-1, 512, 16, 8]               0\n","       Bottleneck-68           [-1, 512, 16, 8]               0\n","           Conv2d-69           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-70           [-1, 128, 16, 8]             256\n","             ReLU-71           [-1, 128, 16, 8]               0\n","           Conv2d-72           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-73           [-1, 128, 16, 8]             256\n","             ReLU-74           [-1, 128, 16, 8]               0\n","           Conv2d-75           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 512, 16, 8]           1,024\n","             ReLU-77           [-1, 512, 16, 8]               0\n","       Bottleneck-78           [-1, 512, 16, 8]               0\n","           Conv2d-79           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-80           [-1, 256, 16, 8]             512\n","             ReLU-81           [-1, 256, 16, 8]               0\n","           Conv2d-82            [-1, 256, 8, 4]         589,824\n","      BatchNorm2d-83            [-1, 256, 8, 4]             512\n","             ReLU-84            [-1, 256, 8, 4]               0\n","           Conv2d-85           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-86           [-1, 1024, 8, 4]           2,048\n","           Conv2d-87           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-88           [-1, 1024, 8, 4]           2,048\n","             ReLU-89           [-1, 1024, 8, 4]               0\n","       Bottleneck-90           [-1, 1024, 8, 4]               0\n","           Conv2d-91            [-1, 256, 8, 4]         262,144\n","      BatchNorm2d-92            [-1, 256, 8, 4]             512\n","             ReLU-93            [-1, 256, 8, 4]               0\n","           Conv2d-94            [-1, 256, 8, 4]         589,824\n","      BatchNorm2d-95            [-1, 256, 8, 4]             512\n","             ReLU-96            [-1, 256, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-142            [-1, 512, 8, 4]           1,024\n","            ReLU-143            [-1, 512, 8, 4]               0\n","          Conv2d-144            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-145            [-1, 512, 8, 4]           1,024\n","            ReLU-146            [-1, 512, 8, 4]               0\n","          Conv2d-147           [-1, 2048, 8, 4]       1,048,576\n","     BatchNorm2d-148           [-1, 2048, 8, 4]           4,096\n","          Conv2d-149           [-1, 2048, 8, 4]       2,097,152\n","     BatchNorm2d-150           [-1, 2048, 8, 4]           4,096\n","            ReLU-151           [-1, 2048, 8, 4]               0\n","      Bottleneck-152           [-1, 2048, 8, 4]               0\n","          Conv2d-153            [-1, 512, 8, 4]       1,048,576\n","     BatchNorm2d-154            [-1, 512, 8, 4]           1,024\n","            ReLU-155            [-1, 512, 8, 4]               0\n","          Conv2d-156            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-157            [-1, 512, 8, 4]           1,024\n","            ReLU-158            [-1, 512, 8, 4]               0\n","          Conv2d-159           [-1, 2048, 8, 4]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 8, 4]           4,096\n","            ReLU-161           [-1, 2048, 8, 4]               0\n","      Bottleneck-162           [-1, 2048, 8, 4]               0\n","          Conv2d-163            [-1, 512, 8, 4]       1,048,576\n","     BatchNorm2d-164            [-1, 512, 8, 4]           1,024\n","            ReLU-165            [-1, 512, 8, 4]               0\n","          Conv2d-166            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-167            [-1, 512, 8, 4]           1,024\n","            ReLU-168            [-1, 512, 8, 4]               0\n","          Conv2d-169           [-1, 2048, 8, 4]       1,048,576\n","     BatchNorm2d-170           [-1, 2048, 8, 4]           4,096\n","            ReLU-171           [-1, 2048, 8, 4]               0\n","      Bottleneck-172           [-1, 2048, 8, 4]               0\n","          Linear-173                 [-1, 1024]       2,098,176\n","     BatchNorm1d-174                 [-1, 1024]           2,048\n","            ReLU-175                 [-1, 1024]               0\n","   DenseNormReLU-176                 [-1, 1024]               0\n","          Linear-177                  [-1, 128]         131,200\n","    EmbedNetwork-178                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 53.47\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 151.75\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ekvFbIr0sec0","colab_type":"code","colab":{}},"source":["model_num = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_7EAbaVriZc","colab_type":"code","outputId":"4edcf539-f555-427b-ae2e-540123f6b068","executionInfo":{"status":"ok","timestamp":1575470958188,"user_tz":-60,"elapsed":1207251,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#create_emb_shift(dataset = X_query, net = net_1, fids = labels_query.fid, model_num = 1, store_path= \"./res/emb_query_shift{}.pkl\".format(model_num))\n","create_emb(dataset = X_test, net = net_1, fids = labels_test.fid, model_num = 1, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 616 / 617  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"3cd7805e-9767-4eb9-8e0a-334608792d59","executionInfo":{"status":"ok","timestamp":1575471126527,"user_tz":-60,"elapsed":1359586,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"4BnNdytrPyG_","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query_shift{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:41<00:00, 21.62it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.6982403809399482, cmc is: [0.8503563  0.89786226 0.9171615  0.932601   0.942696  ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nJzPmnkhzk20","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mbb33uNkyGO","colab_type":"text"},"source":["## model 6"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"24jsp6kzDF5u","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XtSmmChXC_vF","colab":{}},"source":["model_num = 6\n","filter_size = 3\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e7rwh247C_vI","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embed = nn.Linear(in_features = 1024, out_features = 128).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FBbdtvCrC_vL","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embed = embed\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embed(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"9a4d76e8-eef7-4049-dd7f-d422b3570d5a","executionInfo":{"status":"ok","timestamp":1575467121018,"user_tz":-60,"elapsed":707,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"jCfx6GOaC_vM","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net_6 = nn.DataParallel(model)\n","summary(net_6, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157           [-1, 512, 10, 6]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","          Linear-187                 [-1, 1024]       2,098,176\n","     BatchNorm1d-188                 [-1, 1024]           2,048\n","            ReLU-189                 [-1, 1024]               0\n","   DenseNormReLU-190                 [-1, 1024]               0\n","          Linear-191                  [-1, 128]         131,200\n","           Model-192                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 54.99\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 153.27\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1PmUgWWZzviS","colab_type":"code","colab":{}},"source":["model_num = 6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"58594cfb-6a3c-4915-8c56-dbedc27ce8b0","executionInfo":{"status":"ok","timestamp":1575467328257,"user_tz":-60,"elapsed":158468,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"JK5TCr9ozvpO","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["create_emb_shift(dataset = X_query, net = net_6, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query_shift{}.pkl\".format(model_num))\n","#create_emb(dataset = X_test, net = net_6, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 105 / 106  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VGwfwQwbzvpS","outputId":"197dea94-1c9e-4661-8529-9bfcf92028ab","executionInfo":{"status":"ok","timestamp":1575467496812,"user_tz":-60,"elapsed":322804,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query_shift{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:41<00:00, 20.85it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.7235935030377554, cmc is: [0.8586698  0.90290976 0.9213183  0.93319476 0.9397268 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zXo-c3UDGWfS"},"source":["### Model 7\n","AA model without GAP filter size = 2"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VMgtefm2GWfU","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oKkQrFZOGWfX","colab":{}},"source":["model_num = 7"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9BBOnNiDGWfY","colab":{}},"source":["filter_size = 2\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HvK4kfhaGWfd","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1_bT7AhzGWfg","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embedding = embedding\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embedding(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"f24af012-7de0-43b7-bef1-c35a3b709350","executionInfo":{"status":"ok","timestamp":1575467526704,"user_tz":-60,"elapsed":778,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"sWgPgwUYGWfi","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net_7 = nn.DataParallel(model)\n","summary(net_7, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 64, 32]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 33, 17]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 33, 17]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91           [-1, 256, 17, 9]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95           [-1, 512, 17, 9]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157            [-1, 512, 9, 5]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161           [-1, 1024, 9, 5]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","          Linear-187                 [-1, 1024]       2,098,176\n","     BatchNorm1d-188                 [-1, 1024]           2,048\n","            ReLU-189                 [-1, 1024]               0\n","   DenseNormReLU-190                 [-1, 1024]               0\n","          Linear-191                  [-1, 128]         131,200\n","           Model-192                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 54.46\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 152.74\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9u0AgLNlKw2c","colab":{}},"source":["model_num = 7"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"12d68301-7724-4248-ab10-a4d5f018cfc8","executionInfo":{"status":"ok","timestamp":1575467720875,"user_tz":-60,"elapsed":152803,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"ppgnm8kDKw2e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["create_emb_shift(dataset = X_query, net = net_7, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query_shift{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 105 / 106  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KzU8OcE4Kw2f","outputId":"760fe1ec-2cc2-4c3f-9e46-22d3e6543487","executionInfo":{"status":"ok","timestamp":1575467889819,"user_tz":-60,"elapsed":314323,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query_shift{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:41<00:00, 20.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.7252749732281969, cmc is: [0.85807604 0.90825415 0.9245843  0.93557006 0.945962  ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O7ekpTe6I9mf"},"source":["### Model 8\n","AA model without GAP, filter size = 5"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UI_G-hdhI9mw","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xkeG_6swI9m3","colab":{}},"source":["model_num = 8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y__Q93TBI9m8","colab":{}},"source":["filter_size = 5\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6PUIOTIoI9nA","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IPVLRocVI9nE","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embedding = embedding\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embedding(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zJ62EAMhI9nH","outputId":"b03d0ea9-e1b0-4ddf-dce5-bb3e1d15d32b","executionInfo":{"status":"ok","timestamp":1575467894887,"user_tz":-60,"elapsed":779,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net_8 = nn.DataParallel(model)\n","summary(net_8, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 67, 35]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 36, 20]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 36, 20]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91          [-1, 256, 20, 12]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95          [-1, 512, 20, 12]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157           [-1, 512, 12, 8]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161          [-1, 1024, 12, 8]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","          Linear-187                 [-1, 1024]       2,098,176\n","     BatchNorm1d-188                 [-1, 1024]           2,048\n","            ReLU-189                 [-1, 1024]               0\n","   DenseNormReLU-190                 [-1, 1024]               0\n","          Linear-191                  [-1, 128]         131,200\n","           Model-192                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 56.18\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 154.46\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e8kX7k-XL_Na","colab_type":"code","colab":{}},"source":["model_num = 8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"51745299-eec1-4255-fbd6-ae14f73908aa","executionInfo":{"status":"ok","timestamp":1575468061760,"user_tz":-60,"elapsed":167630,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"4GKB-USpL91a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["create_emb_shift(dataset = X_query, net = net_8, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query_shift{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 105 / 106  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W22yqYeNL91e","outputId":"ba11fc5b-7f70-4d88-fbe4-fb45e0ff3f3c","executionInfo":{"status":"ok","timestamp":1575468238460,"user_tz":-60,"elapsed":344323,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query_shift{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:48<00:00, 20.01it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.7165804445387892, cmc is: [0.85659146 0.9023159  0.92577195 0.939133   0.9465558 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zrFFZ9x4vx5C","colab_type":"text"},"source":["## try with no ten crops"]},{"cell_type":"code","metadata":{"id":"wq2PJuQdt5MX","colab_type":"code","colab":{}},"source":["from own_code.Market1501_shift import Market1501\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mQ94yAW8yIIM","colab_type":"text"},"source":["with no ten crop and random shifting for each im\n"]},{"cell_type":"code","metadata":{"id":"fM2MHmzOsgEJ","colab_type":"code","colab":{}},"source":["def create_emb_shift_no_crop1(dataset, fids, net, store_path, model_num):\n","  torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","  ## logging\n","  FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","  logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","  logger = logging.getLogger(__name__)\n","  ## restore model\n","  logger.info('restoring model')\n","  model = net\n","  #model = nn.DataParallel(model)\n","  model = net.cuda()\n","  model.module.load_state_dict(torch.load('./res/model{}.pkl'.format(model_num)))\n","  model = nn.DataParallel(model)\n","  model.eval()\n","\n","  ## load gallery dataset\n","  batchsize = 32\n","  ds = Market1501(pids_list=list(fids), array=dataset, is_train = False)\n","  dl = DataLoader(ds, batch_size = batchsize, drop_last = False, num_workers = 4)\n","\n","  ## embedding samples\n","  logger.info('start embedding')\n","  all_iter_nums = len(ds) // batchsize + 1\n","  embeddings = []\n","  label_ids = []\n","  label_cams = []\n","  for it, (img, lb_id, lb_cam) in enumerate(dl):\n","    print('\\r=======>  processing iter {} / {}'.format(it, all_iter_nums),\n","            end = '', flush = True)\n","    label_ids.append(lb_id)\n","    label_cams.append(lb_cam)\n","    embds = []\n","    for im in img:\n","      #print(im.shape)\n","      offa = np.random.randint(32,size=2)\n","      offb = np.random.randint(16,size=2)\n","      output0 = model(im[:,offa[0]:offa[0]+224,offb[0]:offb[0]+112])\n","      im = im.cuda()\n","      embd = model(im).detach().cpu().numpy()\n","      embeddings.append(embd)\n","    #embed = sum(embds) / len(embds)\n","    #embeddings.append(embed)\n","  print('  ...   completed')\n","\n","  embeddings = np.vstack(embeddings)\n","  label_ids = np.hstack(label_ids)\n","  label_cams = np.hstack(label_cams)\n","\n","  ## dump results\n","  logger.info('dump embeddings')\n","  embd_res = {'embeddings': embeddings, 'label_ids': label_ids, 'label_cams': label_cams}\n","  with open(store_path, 'wb') as fw:\n","    pickle.dump(embd_res, fw)\n","\n","  logger.info('embedding finished')\n"],"execution_count":0,"outputs":[]}]}