{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch self_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-EJgFosBl44B"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZU--6uXghq",
        "colab_type": "text"
      },
      "source": [
        "# Person re-id \n",
        "Before using this notebook it is expected that an HDF5 file is created. See notebook \"create hdf5.ipynb\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtT96bD6YSy7",
        "colab_type": "text"
      },
      "source": [
        "## Set-up\n",
        "first, import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKuUOjVlQUUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import cv2\n",
        "import datetime as dt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from logger import logger\n",
        "from torchsummary import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpSN0g67YcyC",
        "colab_type": "code",
        "outputId": "a0fa6699-a8fd-469d-b08a-88f3d56e3407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#mount the drive to be able to access images, functions and classes\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58S4gB92Wya3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Thesis re-id/triplet-reid-master\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAPzgn_EYuWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading local classes and functions\n",
        "from own_code.backbone_normal import EmbedNetwork\n",
        "from own_code.loss import TripletLoss\n",
        "from own_code.triplet_selector import BatchHardTripletSelector\n",
        "from own_code.batch_sampler import BatchSampler\n",
        "from own_code.Market1501 import Market1501\n",
        "from own_code.optimizer import AdamOptimWrapper\n",
        "#import the anti-aliased networks\n",
        "from models_lpf import *\n",
        "import models_lpf.resnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwS4OPOIX-VJ",
        "colab_type": "text"
      },
      "source": [
        "## Loading and preprocessing data\n",
        "The labels are stored in an csv file. These labels have to be loaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkMNav6ZQiyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Labels\n",
        "labels = pd.read_csv('data/market1501_train.csv', names = ['pid', 'fid'], header = None, dtype = str)\n",
        "labels_query = pd.read_csv('data/market1501_query.csv', names = ['pid', 'fid'], header = None, dtype = str)\n",
        "labels_test = pd.read_csv('data/market1501_test.csv', names = ['pid', 'fid'], header = None, dtype = str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8hrPCzrcjG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_images = len(labels)\n",
        "\n",
        "height = 128\n",
        "width = 64\n",
        "net_input_size = (128,64)\n",
        "channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRDOEnZ9DPSI",
        "colab_type": "text"
      },
      "source": [
        "### Reading the h5py Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h--hlJ7exXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = 'data_final.h5'\n",
        "fileName_qt = 'data_qt.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfbXc61suBbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(fileName, \"a\") as out:\n",
        "  X_train = np.asarray(out[\"X_train\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5aLjJp2ew8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(fileName_qt, \"a\") as out:\n",
        "  X_query = np.asarray(out[\"X_dev\"])\n",
        "  X_test = np.asarray(out[\"X_test\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk1zZDrUuBUi",
        "colab_type": "code",
        "outputId": "6d0f5bb6-faa3-41e8-88f7-e53083d830fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_query.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12936, 128, 64, 3)\n",
            "(3368, 128, 64, 3)\n",
            "(19732, 128, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn1NVYkvb7l1",
        "colab_type": "text"
      },
      "source": [
        "## Set up core functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pu8ZHficFiF",
        "colab_type": "text"
      },
      "source": [
        "### Train function\n",
        "All models are trained with the same parameters and loss functions. Therefore a general function is made to perform the training for all models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdqhs5phNOcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, model_num):\n",
        "  triplet_loss = TripletLoss(margin = None).cuda() # no margin means soft-margin\n",
        "\n",
        "  ## optimizer\n",
        "  logger.info('creating optimizer')\n",
        "  optim = AdamOptimWrapper(net.parameters(), lr = 3e-4, wd = 0, t0 = 15000, t1 = 25000)\n",
        "\n",
        "  ## dataloader\n",
        "  selector = BatchHardTripletSelector()\n",
        "\n",
        "  ds = Market1501(pids_list=list(labels.fid), array=X_train, is_train = True)\n",
        "  sampler = BatchSampler(ds, 18, 4)\n",
        "  dl = DataLoader(ds, batch_sampler = sampler, num_workers = 4)\n",
        "  diter = iter(dl)\n",
        "\n",
        "  logger.info('start training ...')\n",
        "  loss_avg = []\n",
        "  count = 0\n",
        "  t_start = time.time()\n",
        "  while True:\n",
        "    try:\n",
        "      imgs, lbs, _ = next(diter)\n",
        "    except StopIteration:\n",
        "      diter = iter(dl)\n",
        "      imgs, lbs, _ = next(diter)\n",
        "\n",
        "    net.train()\n",
        "    imgs = imgs.cuda()\n",
        "    lbs = lbs.cuda()\n",
        "    embds = net(imgs)\n",
        "    anchor, positives, negatives = selector(embds, lbs)\n",
        "\n",
        "    loss = triplet_loss(anchor, positives, negatives)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    loss_avg.append(loss.detach().cpu().numpy())\n",
        "    if count % 20 == 0 and count != 0:\n",
        "      loss_avg = sum(loss_avg) / len(loss_avg)\n",
        "      t_end = time.time()\n",
        "      time_interval = t_end - t_start\n",
        "      logger.info('iter: {}, loss: {:4f}, lr: {:4f}, time: {:3f}'.format(count, loss_avg, optim.lr, time_interval))\n",
        "      loss_avg = []\n",
        "      t_start = t_end\n",
        "\n",
        "    count += 1\n",
        "    if count == 25000: break\n",
        "\n",
        "  ## dump model\n",
        "  logger.info('saving trained model')\n",
        "  torch.save(net.module.state_dict(), './res/model{}.pkl'.format(model_num))\n",
        "\n",
        "  logger.info('everything finished')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPDSD1fmaw40",
        "colab_type": "text"
      },
      "source": [
        "## Create Embeddings\n",
        "When the model is trained, it can be applied to both the Query and Test images. This is done by the function create_emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDWd0nIDIf35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb(dataset, fids, store_path, model_num):\n",
        "  torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "  ## logging\n",
        "  FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n",
        "  logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "  logger = logging.getLogger(__name__)\n",
        "  ## restore model\n",
        "  logger.info('restoring model')\n",
        "  model = net\n",
        "  #model = nn.DataParallel(model)\n",
        "  model = net.cuda()\n",
        "  model.module.load_state_dict(torch.load('./res/model{}.pkl'.format(model_num)))\n",
        "  model = nn.DataParallel(model)\n",
        "  model.eval()\n",
        "\n",
        "  ## load gallery dataset\n",
        "  batchsize = 32\n",
        "  ds = Market1501(pids_list=list(fids), array=dataset, is_train = False)\n",
        "  dl = DataLoader(ds, batch_size = batchsize, drop_last = False, num_workers = 4)\n",
        "\n",
        "  ## embedding samples\n",
        "  logger.info('start embedding')\n",
        "  all_iter_nums = len(ds) // batchsize + 1\n",
        "  embeddings = []\n",
        "  label_ids = []\n",
        "  label_cams = []\n",
        "  for it, (img, lb_id, lb_cam) in enumerate(dl):\n",
        "    print('\\r=======>  processing iter {} / {}'.format(it, all_iter_nums),\n",
        "            end = '', flush = True)\n",
        "    label_ids.append(lb_id)\n",
        "    label_cams.append(lb_cam)\n",
        "    embds = []\n",
        "    for im in img:\n",
        "        im = im.cuda()\n",
        "        embd = model(im).detach().cpu().numpy()\n",
        "        embds.append(embd)\n",
        "    embed = sum(embds) / len(embds)\n",
        "    embeddings.append(embed)\n",
        "  print('  ...   completed')\n",
        "\n",
        "  embeddings = np.vstack(embeddings)\n",
        "  label_ids = np.hstack(label_ids)\n",
        "  label_cams = np.hstack(label_cams)\n",
        "\n",
        "  ## dump results\n",
        "  logger.info('dump embeddings')\n",
        "  embd_res = {'embeddings': embeddings, 'label_ids': label_ids, 'label_cams': label_cams}\n",
        "  with open(store_path, 'wb') as fw:\n",
        "    pickle.dump(embd_res, fw)\n",
        "\n",
        "  logger.info('embedding finished')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwdlBg0T3pQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import pdist_np as pdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ooSi6py4LGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(test_embs, query_embs, cmc_rank):\n",
        "    ## logging\n",
        "    FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n",
        "    logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    ## load embeddings\n",
        "    logger.info('loading gallery embeddings')\n",
        "    with open(test_embs, 'rb') as fr:\n",
        "        gallery_dict = pickle.load(fr)\n",
        "        emb_gallery, lb_ids_gallery, lb_cams_gallery = gallery_dict['embeddings'], gallery_dict['label_ids'], gallery_dict['label_cams']\n",
        "    logger.info('loading query embeddings')\n",
        "    with open(query_embs, 'rb') as fr:\n",
        "        query_dict = pickle.load(fr)\n",
        "        emb_query, lb_ids_query, lb_cams_query = query_dict['embeddings'], query_dict['label_ids'], query_dict['label_cams']\n",
        "\n",
        "    ## compute and clean distance matrix\n",
        "    dist_mtx = pdist(emb_query, emb_gallery)\n",
        "    n_q, n_g = dist_mtx.shape\n",
        "    indices = np.argsort(dist_mtx, axis = 1)\n",
        "    matches = lb_ids_gallery[indices] == lb_ids_query[:, np.newaxis]\n",
        "    matches = matches.astype(np.int32)\n",
        "    all_aps = []\n",
        "    all_cmcs = []\n",
        "    logger.info('starting evaluating ...')\n",
        "    for qidx in tqdm(range(n_q)):\n",
        "        qpid = lb_ids_query[qidx]\n",
        "        qcam = lb_cams_query[qidx]\n",
        "\n",
        "        order = indices[qidx]\n",
        "        pid_diff = lb_ids_gallery[order] != qpid\n",
        "        cam_diff = lb_cams_gallery[order] != qcam\n",
        "        useful = lb_ids_gallery[order] != -1\n",
        "        keep = np.logical_or(pid_diff, cam_diff)\n",
        "        keep = np.logical_and(keep, useful)\n",
        "        match = matches[qidx][keep]\n",
        "\n",
        "        if not np.any(match): continue\n",
        "\n",
        "        cmc = match.cumsum()\n",
        "        cmc[cmc > 1] = 1\n",
        "        all_cmcs.append(cmc[:cmc_rank])\n",
        "\n",
        "        num_real = match.sum()\n",
        "        match_cum = match.cumsum()\n",
        "        match_cum = [el / (1.0 + i) for i, el in enumerate(match_cum)]\n",
        "        match_cum = np.array(match_cum) * match\n",
        "        ap = match_cum.sum() / num_real\n",
        "        all_aps.append(ap)\n",
        "\n",
        "    assert len(all_aps) > 0, \"NO QUERY MATCHED\"\n",
        "    mAP = sum(all_aps) / len(all_aps)\n",
        "    all_cmcs = np.array(all_cmcs, dtype = np.float32)\n",
        "    cmc = np.mean(all_cmcs, axis = 0)\n",
        "\n",
        "    print('mAP is: {}, cmc is: {}'.format(mAP, cmc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3vkU2f84uJA",
        "colab_type": "text"
      },
      "source": [
        "## Model log\n",
        "Model 1 = baseline with stride = 2 in last layer \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model 6 = AA model with extra connected layers and stride = 2 in last layer\n",
        "\n",
        "\n",
        "Model 7 = AA model filter = 2\n",
        "\n",
        "Model 8 = AA Model filter = 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjn8kOBESVC6",
        "colab_type": "text"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCMIUYXFfkjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "if not os.path.exists('./res'): os.makedirs('./res')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MD8eF25cjCX",
        "colab_type": "text"
      },
      "source": [
        "### Model 1\n",
        "Baseline model with last conv layer using stride = 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mozPqtNKciJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from own_code.backbone_normal import EmbedNetwork"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_qaud5p5dvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF_imTHacwCO",
        "colab_type": "code",
        "outputId": "82d17aaa-36de-4053-b793-03edb9711236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "logger.info('setting up backbone model and loss')\n",
        "net = EmbedNetwork(pretrained_base=True).cuda()\n",
        "net_1 = nn.DataParallel(net)\n",
        "summary(net_1, (3,128,64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setting up backbone model and loss\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 179MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 32, 16]               0\n",
            "            Conv2d-5           [-1, 64, 32, 16]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 32, 16]             128\n",
            "              ReLU-7           [-1, 64, 32, 16]               0\n",
            "            Conv2d-8           [-1, 64, 32, 16]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 16]             128\n",
            "             ReLU-10           [-1, 64, 32, 16]               0\n",
            "           Conv2d-11          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 32, 16]             512\n",
            "           Conv2d-13          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 16]             512\n",
            "             ReLU-15          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-16          [-1, 256, 32, 16]               0\n",
            "           Conv2d-17           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 32, 16]             128\n",
            "             ReLU-19           [-1, 64, 32, 16]               0\n",
            "           Conv2d-20           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 32, 16]             128\n",
            "             ReLU-22           [-1, 64, 32, 16]               0\n",
            "           Conv2d-23          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 32, 16]             512\n",
            "             ReLU-25          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-26          [-1, 256, 32, 16]               0\n",
            "           Conv2d-27           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 32, 16]             128\n",
            "             ReLU-29           [-1, 64, 32, 16]               0\n",
            "           Conv2d-30           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 32, 16]             128\n",
            "             ReLU-32           [-1, 64, 32, 16]               0\n",
            "           Conv2d-33          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 32, 16]             512\n",
            "             ReLU-35          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-36          [-1, 256, 32, 16]               0\n",
            "           Conv2d-37          [-1, 128, 32, 16]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 32, 16]             256\n",
            "             ReLU-39          [-1, 128, 32, 16]               0\n",
            "           Conv2d-40           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-41           [-1, 128, 16, 8]             256\n",
            "             ReLU-42           [-1, 128, 16, 8]               0\n",
            "           Conv2d-43           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-44           [-1, 512, 16, 8]           1,024\n",
            "           Conv2d-45           [-1, 512, 16, 8]         131,072\n",
            "      BatchNorm2d-46           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-47           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-48           [-1, 512, 16, 8]               0\n",
            "           Conv2d-49           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-50           [-1, 128, 16, 8]             256\n",
            "             ReLU-51           [-1, 128, 16, 8]               0\n",
            "           Conv2d-52           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-53           [-1, 128, 16, 8]             256\n",
            "             ReLU-54           [-1, 128, 16, 8]               0\n",
            "           Conv2d-55           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-56           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-57           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-58           [-1, 512, 16, 8]               0\n",
            "           Conv2d-59           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-60           [-1, 128, 16, 8]             256\n",
            "             ReLU-61           [-1, 128, 16, 8]               0\n",
            "           Conv2d-62           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-63           [-1, 128, 16, 8]             256\n",
            "             ReLU-64           [-1, 128, 16, 8]               0\n",
            "           Conv2d-65           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-66           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-67           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-68           [-1, 512, 16, 8]               0\n",
            "           Conv2d-69           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-70           [-1, 128, 16, 8]             256\n",
            "             ReLU-71           [-1, 128, 16, 8]               0\n",
            "           Conv2d-72           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-73           [-1, 128, 16, 8]             256\n",
            "             ReLU-74           [-1, 128, 16, 8]               0\n",
            "           Conv2d-75           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-76           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-77           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-78           [-1, 512, 16, 8]               0\n",
            "           Conv2d-79           [-1, 256, 16, 8]         131,072\n",
            "      BatchNorm2d-80           [-1, 256, 16, 8]             512\n",
            "             ReLU-81           [-1, 256, 16, 8]               0\n",
            "           Conv2d-82            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 8, 4]             512\n",
            "             ReLU-84            [-1, 256, 8, 4]               0\n",
            "           Conv2d-85           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-86           [-1, 1024, 8, 4]           2,048\n",
            "           Conv2d-87           [-1, 1024, 8, 4]         524,288\n",
            "      BatchNorm2d-88           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-89           [-1, 1024, 8, 4]               0\n",
            "       Bottleneck-90           [-1, 1024, 8, 4]               0\n",
            "           Conv2d-91            [-1, 256, 8, 4]         262,144\n",
            "      BatchNorm2d-92            [-1, 256, 8, 4]             512\n",
            "             ReLU-93            [-1, 256, 8, 4]               0\n",
            "           Conv2d-94            [-1, 256, 8, 4]         589,824\n",
            "      BatchNorm2d-95            [-1, 256, 8, 4]             512\n",
            "             ReLU-96            [-1, 256, 8, 4]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-101            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 4]             512\n",
            "            ReLU-103            [-1, 256, 8, 4]               0\n",
            "          Conv2d-104            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 4]             512\n",
            "            ReLU-106            [-1, 256, 8, 4]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-111            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 4]             512\n",
            "            ReLU-113            [-1, 256, 8, 4]               0\n",
            "          Conv2d-114            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 4]             512\n",
            "            ReLU-116            [-1, 256, 8, 4]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-121            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 4]             512\n",
            "            ReLU-123            [-1, 256, 8, 4]               0\n",
            "          Conv2d-124            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 4]             512\n",
            "            ReLU-126            [-1, 256, 8, 4]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-131            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 4]             512\n",
            "            ReLU-133            [-1, 256, 8, 4]               0\n",
            "          Conv2d-134            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 4]             512\n",
            "            ReLU-136            [-1, 256, 8, 4]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-141            [-1, 512, 8, 4]         524,288\n",
            "     BatchNorm2d-142            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-143            [-1, 512, 8, 4]               0\n",
            "          Conv2d-144            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-146            [-1, 512, 4, 2]               0\n",
            "          Conv2d-147           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 4, 2]           4,096\n",
            "          Conv2d-149           [-1, 2048, 4, 2]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-151           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-152           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-153            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-155            [-1, 512, 4, 2]               0\n",
            "          Conv2d-156            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-158            [-1, 512, 4, 2]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-161           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-162           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-163            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-165            [-1, 512, 4, 2]               0\n",
            "          Conv2d-166            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-168            [-1, 512, 4, 2]               0\n",
            "          Conv2d-169           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-171           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-172           [-1, 2048, 4, 2]               0\n",
            "          Linear-173                 [-1, 1024]       2,098,176\n",
            "     BatchNorm1d-174                 [-1, 1024]           2,048\n",
            "            ReLU-175                 [-1, 1024]               0\n",
            "   DenseNormReLU-176                 [-1, 1024]               0\n",
            "          Linear-177                  [-1, 128]         131,200\n",
            "    EmbedNetwork-178                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 25,739,456\n",
            "Trainable params: 25,739,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 46.81\n",
            "Params size (MB): 98.19\n",
            "Estimated Total Size (MB): 145.10\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTZ4MoPGyGOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(net_1 = net, model_num = model_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rgipF2Cjyd6i",
        "colab": {}
      },
      "source": [
        "create_emb(dataset = X_query, fids = labels_query.fid, model_num = 1, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n",
        "create_emb(dataset = X_test, fids = labels_test.fid, model_num = 1, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zdyulZZhyd6k",
        "colab": {}
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-VtubLKfnXP",
        "colab_type": "text"
      },
      "source": [
        "## anti-aliased models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMvV8y-qC33i",
        "colab_type": "text"
      },
      "source": [
        "### Model 6\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24jsp6kzDF5u",
        "colab": {}
      },
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "if not os.path.exists('./res'): os.makedirs('./res')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VSz6tZQOC_vA",
        "colab": {}
      },
      "source": [
        "model_num = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XtSmmChXC_vF",
        "colab": {}
      },
      "source": [
        "filter_size = 3\n",
        "net = models_lpf.resnet.resnet50(filter_size=filter_size)\n",
        "net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n",
        "model = torch.nn.Sequential(*(list(net.children())[:-2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e7rwh247C_vI",
        "colab": {}
      },
      "source": [
        "class DenseNormReLU(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, *args, **kwargs):\n",
        "        super(DenseNormReLU, self).__init__(*args, **kwargs)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n",
        "        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n",
        "        self.relu = nn.ReLU(inplace = True).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n",
        "embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FBbdtvCrC_vL",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.base = model\n",
        "    self.fc_head = fc_head\n",
        "    self.embedding = embedding\n",
        "\n",
        "  def forward(self, x):\n",
        "    # shape [N, C, H, W]\n",
        "    x = self.base(x)\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.contiguous().view(-1, 2048 )\n",
        "    # shape [N, C]\n",
        "    x = self.fc_head(x)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1ae0531d-40aa-4571-8ce7-5eb5dcfe9637",
        "id": "jCfx6GOaC_vM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model()\n",
        "#model = Model().to(device)\n",
        "model = model.cuda()\n",
        "net = nn.DataParallel(model)\n",
        "summary(net, (3, 128,64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 63, 31]               0\n",
            "   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n",
            "        Downsample-6           [-1, 64, 32, 16]               0\n",
            "            Conv2d-7           [-1, 64, 32, 16]           4,096\n",
            "       BatchNorm2d-8           [-1, 64, 32, 16]             128\n",
            "              ReLU-9           [-1, 64, 32, 16]               0\n",
            "           Conv2d-10           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 16]             128\n",
            "             ReLU-12           [-1, 64, 32, 16]               0\n",
            "           Conv2d-13          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 16]             512\n",
            "           Conv2d-15          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-16          [-1, 256, 32, 16]             512\n",
            "             ReLU-17          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-18          [-1, 256, 32, 16]               0\n",
            "           Conv2d-19           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 16]             128\n",
            "             ReLU-21           [-1, 64, 32, 16]               0\n",
            "           Conv2d-22           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 32, 16]             128\n",
            "             ReLU-24           [-1, 64, 32, 16]               0\n",
            "           Conv2d-25          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-26          [-1, 256, 32, 16]             512\n",
            "             ReLU-27          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-28          [-1, 256, 32, 16]               0\n",
            "           Conv2d-29           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-30           [-1, 64, 32, 16]             128\n",
            "             ReLU-31           [-1, 64, 32, 16]               0\n",
            "           Conv2d-32           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 32, 16]             128\n",
            "             ReLU-34           [-1, 64, 32, 16]               0\n",
            "           Conv2d-35          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-36          [-1, 256, 32, 16]             512\n",
            "             ReLU-37          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-38          [-1, 256, 32, 16]               0\n",
            "           Conv2d-39          [-1, 128, 32, 16]          32,768\n",
            "      BatchNorm2d-40          [-1, 128, 32, 16]             256\n",
            "             ReLU-41          [-1, 128, 32, 16]               0\n",
            "           Conv2d-42          [-1, 128, 32, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 32, 16]             256\n",
            "             ReLU-44          [-1, 128, 32, 16]               0\n",
            "  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n",
            "       Downsample-46           [-1, 128, 16, 8]               0\n",
            "           Conv2d-47           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n",
            "  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n",
            "       Downsample-50           [-1, 256, 16, 8]               0\n",
            "           Conv2d-51           [-1, 512, 16, 8]         131,072\n",
            "      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-53           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-54           [-1, 512, 16, 8]               0\n",
            "           Conv2d-55           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-56           [-1, 128, 16, 8]             256\n",
            "             ReLU-57           [-1, 128, 16, 8]               0\n",
            "           Conv2d-58           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-59           [-1, 128, 16, 8]             256\n",
            "             ReLU-60           [-1, 128, 16, 8]               0\n",
            "           Conv2d-61           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-63           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-64           [-1, 512, 16, 8]               0\n",
            "           Conv2d-65           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-66           [-1, 128, 16, 8]             256\n",
            "             ReLU-67           [-1, 128, 16, 8]               0\n",
            "           Conv2d-68           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-69           [-1, 128, 16, 8]             256\n",
            "             ReLU-70           [-1, 128, 16, 8]               0\n",
            "           Conv2d-71           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-73           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-74           [-1, 512, 16, 8]               0\n",
            "           Conv2d-75           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-76           [-1, 128, 16, 8]             256\n",
            "             ReLU-77           [-1, 128, 16, 8]               0\n",
            "           Conv2d-78           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-79           [-1, 128, 16, 8]             256\n",
            "             ReLU-80           [-1, 128, 16, 8]               0\n",
            "           Conv2d-81           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-83           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-84           [-1, 512, 16, 8]               0\n",
            "           Conv2d-85           [-1, 256, 16, 8]         131,072\n",
            "      BatchNorm2d-86           [-1, 256, 16, 8]             512\n",
            "             ReLU-87           [-1, 256, 16, 8]               0\n",
            "           Conv2d-88           [-1, 256, 16, 8]         589,824\n",
            "      BatchNorm2d-89           [-1, 256, 16, 8]             512\n",
            "             ReLU-90           [-1, 256, 16, 8]               0\n",
            "  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n",
            "       Downsample-92            [-1, 256, 8, 4]               0\n",
            "           Conv2d-93           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n",
            "  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n",
            "       Downsample-96            [-1, 512, 8, 4]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 4]         524,288\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-101            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 4]             512\n",
            "            ReLU-103            [-1, 256, 8, 4]               0\n",
            "          Conv2d-104            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 4]             512\n",
            "            ReLU-106            [-1, 256, 8, 4]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-111            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 4]             512\n",
            "            ReLU-113            [-1, 256, 8, 4]               0\n",
            "          Conv2d-114            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 4]             512\n",
            "            ReLU-116            [-1, 256, 8, 4]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-121            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 4]             512\n",
            "            ReLU-123            [-1, 256, 8, 4]               0\n",
            "          Conv2d-124            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 4]             512\n",
            "            ReLU-126            [-1, 256, 8, 4]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-131            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 4]             512\n",
            "            ReLU-133            [-1, 256, 8, 4]               0\n",
            "          Conv2d-134            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 4]             512\n",
            "            ReLU-136            [-1, 256, 8, 4]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-141            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-142            [-1, 256, 8, 4]             512\n",
            "            ReLU-143            [-1, 256, 8, 4]               0\n",
            "          Conv2d-144            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 4]             512\n",
            "            ReLU-146            [-1, 256, 8, 4]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-149           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-150           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-151            [-1, 512, 8, 4]         524,288\n",
            "     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-153            [-1, 512, 8, 4]               0\n",
            "          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n",
            "     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-156            [-1, 512, 8, 4]               0\n",
            " ReflectionPad2d-157           [-1, 512, 10, 6]               0\n",
            "      Downsample-158            [-1, 512, 4, 2]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n",
            " ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n",
            "      Downsample-162           [-1, 1024, 4, 2]               0\n",
            "          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-165           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-166           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-169            [-1, 512, 4, 2]               0\n",
            "          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-172            [-1, 512, 4, 2]               0\n",
            "          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-175           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-176           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-179            [-1, 512, 4, 2]               0\n",
            "          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-182            [-1, 512, 4, 2]               0\n",
            "          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-185           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-186           [-1, 2048, 4, 2]               0\n",
            "          Linear-187                 [-1, 1024]       2,098,176\n",
            "     BatchNorm1d-188                 [-1, 1024]           2,048\n",
            "            ReLU-189                 [-1, 1024]               0\n",
            "   DenseNormReLU-190                 [-1, 1024]               0\n",
            "          Linear-191                  [-1, 128]         131,200\n",
            "           Model-192                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 25,739,456\n",
            "Trainable params: 25,739,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 54.99\n",
            "Params size (MB): 98.19\n",
            "Estimated Total Size (MB): 153.27\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PF3aFXuKC_vP",
        "colab": {}
      },
      "source": [
        "train(net = net, model_num = model_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dnRVXN3uC_vQ",
        "outputId": "0427794e-0bfb-4f4b-f9e6-5517468c2a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n",
        "create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======>  processing iter 105 / 106  ...   completed\n",
            "=======>  processing iter 616 / 617  ...   completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3YPjE6kaC_vT",
        "outputId": "d0d39bdc-e6fa-4d5b-dd8c-81052c8b8345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3368/3368 [02:09<00:00, 26.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mAP is: 0.7507753063839253, cmc is: [0.88182896 0.9162708  0.9334917  0.9435867  0.95190024]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zXo-c3UDGWfS"
      },
      "source": [
        "### Model 7\n",
        "AA model without GAP filter size = 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VMgtefm2GWfU",
        "colab": {}
      },
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "if not os.path.exists('./res'): os.makedirs('./res')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oKkQrFZOGWfX",
        "colab": {}
      },
      "source": [
        "model_num = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9BBOnNiDGWfY",
        "colab": {}
      },
      "source": [
        "filter_size = 2\n",
        "net = models_lpf.resnet.resnet50(filter_size=filter_size)\n",
        "net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n",
        "model = torch.nn.Sequential(*(list(net.children())[:-2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvK4kfhaGWfd",
        "colab": {}
      },
      "source": [
        "class DenseNormReLU(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, *args, **kwargs):\n",
        "        super(DenseNormReLU, self).__init__(*args, **kwargs)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n",
        "        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n",
        "        self.relu = nn.ReLU(inplace = True).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n",
        "embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1_bT7AhzGWfg",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.base = model\n",
        "    self.fc_head = fc_head\n",
        "    self.embedding = embedding\n",
        "\n",
        "  def forward(self, x):\n",
        "    # shape [N, C, H, W]\n",
        "    x = self.base(x)\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.contiguous().view(-1, 2048 )\n",
        "    # shape [N, C]\n",
        "    x = self.fc_head(x)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0db46bf0-1850-4b59-c2ed-cbf95f6733d5",
        "id": "sWgPgwUYGWfi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model()\n",
        "#model = Model().to(device)\n",
        "model = model.cuda()\n",
        "net = nn.DataParallel(model)\n",
        "summary(net, (3, 128,64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 63, 31]               0\n",
            "   ReflectionPad2d-5           [-1, 64, 64, 32]               0\n",
            "        Downsample-6           [-1, 64, 32, 16]               0\n",
            "            Conv2d-7           [-1, 64, 32, 16]           4,096\n",
            "       BatchNorm2d-8           [-1, 64, 32, 16]             128\n",
            "              ReLU-9           [-1, 64, 32, 16]               0\n",
            "           Conv2d-10           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 16]             128\n",
            "             ReLU-12           [-1, 64, 32, 16]               0\n",
            "           Conv2d-13          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 16]             512\n",
            "           Conv2d-15          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-16          [-1, 256, 32, 16]             512\n",
            "             ReLU-17          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-18          [-1, 256, 32, 16]               0\n",
            "           Conv2d-19           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 16]             128\n",
            "             ReLU-21           [-1, 64, 32, 16]               0\n",
            "           Conv2d-22           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 32, 16]             128\n",
            "             ReLU-24           [-1, 64, 32, 16]               0\n",
            "           Conv2d-25          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-26          [-1, 256, 32, 16]             512\n",
            "             ReLU-27          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-28          [-1, 256, 32, 16]               0\n",
            "           Conv2d-29           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-30           [-1, 64, 32, 16]             128\n",
            "             ReLU-31           [-1, 64, 32, 16]               0\n",
            "           Conv2d-32           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 32, 16]             128\n",
            "             ReLU-34           [-1, 64, 32, 16]               0\n",
            "           Conv2d-35          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-36          [-1, 256, 32, 16]             512\n",
            "             ReLU-37          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-38          [-1, 256, 32, 16]               0\n",
            "           Conv2d-39          [-1, 128, 32, 16]          32,768\n",
            "      BatchNorm2d-40          [-1, 128, 32, 16]             256\n",
            "             ReLU-41          [-1, 128, 32, 16]               0\n",
            "           Conv2d-42          [-1, 128, 32, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 32, 16]             256\n",
            "             ReLU-44          [-1, 128, 32, 16]               0\n",
            "  ReflectionPad2d-45          [-1, 128, 33, 17]               0\n",
            "       Downsample-46           [-1, 128, 16, 8]               0\n",
            "           Conv2d-47           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n",
            "  ReflectionPad2d-49          [-1, 256, 33, 17]               0\n",
            "       Downsample-50           [-1, 256, 16, 8]               0\n",
            "           Conv2d-51           [-1, 512, 16, 8]         131,072\n",
            "      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-53           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-54           [-1, 512, 16, 8]               0\n",
            "           Conv2d-55           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-56           [-1, 128, 16, 8]             256\n",
            "             ReLU-57           [-1, 128, 16, 8]               0\n",
            "           Conv2d-58           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-59           [-1, 128, 16, 8]             256\n",
            "             ReLU-60           [-1, 128, 16, 8]               0\n",
            "           Conv2d-61           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-63           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-64           [-1, 512, 16, 8]               0\n",
            "           Conv2d-65           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-66           [-1, 128, 16, 8]             256\n",
            "             ReLU-67           [-1, 128, 16, 8]               0\n",
            "           Conv2d-68           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-69           [-1, 128, 16, 8]             256\n",
            "             ReLU-70           [-1, 128, 16, 8]               0\n",
            "           Conv2d-71           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-73           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-74           [-1, 512, 16, 8]               0\n",
            "           Conv2d-75           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-76           [-1, 128, 16, 8]             256\n",
            "             ReLU-77           [-1, 128, 16, 8]               0\n",
            "           Conv2d-78           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-79           [-1, 128, 16, 8]             256\n",
            "             ReLU-80           [-1, 128, 16, 8]               0\n",
            "           Conv2d-81           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-83           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-84           [-1, 512, 16, 8]               0\n",
            "           Conv2d-85           [-1, 256, 16, 8]         131,072\n",
            "      BatchNorm2d-86           [-1, 256, 16, 8]             512\n",
            "             ReLU-87           [-1, 256, 16, 8]               0\n",
            "           Conv2d-88           [-1, 256, 16, 8]         589,824\n",
            "      BatchNorm2d-89           [-1, 256, 16, 8]             512\n",
            "             ReLU-90           [-1, 256, 16, 8]               0\n",
            "  ReflectionPad2d-91           [-1, 256, 17, 9]               0\n",
            "       Downsample-92            [-1, 256, 8, 4]               0\n",
            "           Conv2d-93           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n",
            "  ReflectionPad2d-95           [-1, 512, 17, 9]               0\n",
            "       Downsample-96            [-1, 512, 8, 4]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 4]         524,288\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-101            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 4]             512\n",
            "            ReLU-103            [-1, 256, 8, 4]               0\n",
            "          Conv2d-104            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 4]             512\n",
            "            ReLU-106            [-1, 256, 8, 4]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-111            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 4]             512\n",
            "            ReLU-113            [-1, 256, 8, 4]               0\n",
            "          Conv2d-114            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 4]             512\n",
            "            ReLU-116            [-1, 256, 8, 4]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-121            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 4]             512\n",
            "            ReLU-123            [-1, 256, 8, 4]               0\n",
            "          Conv2d-124            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 4]             512\n",
            "            ReLU-126            [-1, 256, 8, 4]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-131            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 4]             512\n",
            "            ReLU-133            [-1, 256, 8, 4]               0\n",
            "          Conv2d-134            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 4]             512\n",
            "            ReLU-136            [-1, 256, 8, 4]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-141            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-142            [-1, 256, 8, 4]             512\n",
            "            ReLU-143            [-1, 256, 8, 4]               0\n",
            "          Conv2d-144            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 4]             512\n",
            "            ReLU-146            [-1, 256, 8, 4]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-149           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-150           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-151            [-1, 512, 8, 4]         524,288\n",
            "     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-153            [-1, 512, 8, 4]               0\n",
            "          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n",
            "     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-156            [-1, 512, 8, 4]               0\n",
            " ReflectionPad2d-157            [-1, 512, 9, 5]               0\n",
            "      Downsample-158            [-1, 512, 4, 2]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n",
            " ReflectionPad2d-161           [-1, 1024, 9, 5]               0\n",
            "      Downsample-162           [-1, 1024, 4, 2]               0\n",
            "          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-165           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-166           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-169            [-1, 512, 4, 2]               0\n",
            "          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-172            [-1, 512, 4, 2]               0\n",
            "          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-175           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-176           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-179            [-1, 512, 4, 2]               0\n",
            "          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-182            [-1, 512, 4, 2]               0\n",
            "          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-185           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-186           [-1, 2048, 4, 2]               0\n",
            "          Linear-187                 [-1, 1024]       2,098,176\n",
            "     BatchNorm1d-188                 [-1, 1024]           2,048\n",
            "            ReLU-189                 [-1, 1024]               0\n",
            "   DenseNormReLU-190                 [-1, 1024]               0\n",
            "          Linear-191                  [-1, 128]         131,200\n",
            "           Model-192                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 25,739,456\n",
            "Trainable params: 25,739,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 54.46\n",
            "Params size (MB): 98.19\n",
            "Estimated Total Size (MB): 152.74\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KfYai-9UGWfk",
        "colab": {}
      },
      "source": [
        "train(net = net, model_num = model_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sa1FJErpGWfm",
        "colab": {}
      },
      "source": [
        "create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n",
        "create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V36PhvHpGWfo",
        "colab": {}
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O7ekpTe6I9mf"
      },
      "source": [
        "### Model 8\n",
        "AA model without GAP, filter size = 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UI_G-hdhI9mw",
        "colab": {}
      },
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "if not os.path.exists('./res'): os.makedirs('./res')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xkeG_6swI9m3",
        "colab": {}
      },
      "source": [
        "model_num = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y__Q93TBI9m8",
        "colab": {}
      },
      "source": [
        "filter_size = 5\n",
        "net = models_lpf.resnet.resnet50(filter_size=filter_size)\n",
        "net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n",
        "model = torch.nn.Sequential(*(list(net.children())[:-2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6PUIOTIoI9nA",
        "colab": {}
      },
      "source": [
        "class DenseNormReLU(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, *args, **kwargs):\n",
        "        super(DenseNormReLU, self).__init__(*args, **kwargs)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n",
        "        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n",
        "        self.relu = nn.ReLU(inplace = True).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n",
        "embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IPVLRocVI9nE",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.base = model\n",
        "    self.fc_head = fc_head\n",
        "    self.embedding = embedding\n",
        "\n",
        "  def forward(self, x):\n",
        "    # shape [N, C, H, W]\n",
        "    x = self.base(x)\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.contiguous().view(-1, 2048 )\n",
        "    # shape [N, C]\n",
        "    x = self.fc_head(x)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zJ62EAMhI9nH",
        "outputId": "6245067e-a3db-45e1-9258-cb9ad1bb20b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model()\n",
        "#model = Model().to(device)\n",
        "model = model.cuda()\n",
        "net = nn.DataParallel(model)\n",
        "summary(net, (3, 128,64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 63, 31]               0\n",
            "   ReflectionPad2d-5           [-1, 64, 67, 35]               0\n",
            "        Downsample-6           [-1, 64, 32, 16]               0\n",
            "            Conv2d-7           [-1, 64, 32, 16]           4,096\n",
            "       BatchNorm2d-8           [-1, 64, 32, 16]             128\n",
            "              ReLU-9           [-1, 64, 32, 16]               0\n",
            "           Conv2d-10           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 16]             128\n",
            "             ReLU-12           [-1, 64, 32, 16]               0\n",
            "           Conv2d-13          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 16]             512\n",
            "           Conv2d-15          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-16          [-1, 256, 32, 16]             512\n",
            "             ReLU-17          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-18          [-1, 256, 32, 16]               0\n",
            "           Conv2d-19           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 16]             128\n",
            "             ReLU-21           [-1, 64, 32, 16]               0\n",
            "           Conv2d-22           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 32, 16]             128\n",
            "             ReLU-24           [-1, 64, 32, 16]               0\n",
            "           Conv2d-25          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-26          [-1, 256, 32, 16]             512\n",
            "             ReLU-27          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-28          [-1, 256, 32, 16]               0\n",
            "           Conv2d-29           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-30           [-1, 64, 32, 16]             128\n",
            "             ReLU-31           [-1, 64, 32, 16]               0\n",
            "           Conv2d-32           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 32, 16]             128\n",
            "             ReLU-34           [-1, 64, 32, 16]               0\n",
            "           Conv2d-35          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-36          [-1, 256, 32, 16]             512\n",
            "             ReLU-37          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-38          [-1, 256, 32, 16]               0\n",
            "           Conv2d-39          [-1, 128, 32, 16]          32,768\n",
            "      BatchNorm2d-40          [-1, 128, 32, 16]             256\n",
            "             ReLU-41          [-1, 128, 32, 16]               0\n",
            "           Conv2d-42          [-1, 128, 32, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 32, 16]             256\n",
            "             ReLU-44          [-1, 128, 32, 16]               0\n",
            "  ReflectionPad2d-45          [-1, 128, 36, 20]               0\n",
            "       Downsample-46           [-1, 128, 16, 8]               0\n",
            "           Conv2d-47           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n",
            "  ReflectionPad2d-49          [-1, 256, 36, 20]               0\n",
            "       Downsample-50           [-1, 256, 16, 8]               0\n",
            "           Conv2d-51           [-1, 512, 16, 8]         131,072\n",
            "      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-53           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-54           [-1, 512, 16, 8]               0\n",
            "           Conv2d-55           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-56           [-1, 128, 16, 8]             256\n",
            "             ReLU-57           [-1, 128, 16, 8]               0\n",
            "           Conv2d-58           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-59           [-1, 128, 16, 8]             256\n",
            "             ReLU-60           [-1, 128, 16, 8]               0\n",
            "           Conv2d-61           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-63           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-64           [-1, 512, 16, 8]               0\n",
            "           Conv2d-65           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-66           [-1, 128, 16, 8]             256\n",
            "             ReLU-67           [-1, 128, 16, 8]               0\n",
            "           Conv2d-68           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-69           [-1, 128, 16, 8]             256\n",
            "             ReLU-70           [-1, 128, 16, 8]               0\n",
            "           Conv2d-71           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-73           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-74           [-1, 512, 16, 8]               0\n",
            "           Conv2d-75           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-76           [-1, 128, 16, 8]             256\n",
            "             ReLU-77           [-1, 128, 16, 8]               0\n",
            "           Conv2d-78           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-79           [-1, 128, 16, 8]             256\n",
            "             ReLU-80           [-1, 128, 16, 8]               0\n",
            "           Conv2d-81           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-83           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-84           [-1, 512, 16, 8]               0\n",
            "           Conv2d-85           [-1, 256, 16, 8]         131,072\n",
            "      BatchNorm2d-86           [-1, 256, 16, 8]             512\n",
            "             ReLU-87           [-1, 256, 16, 8]               0\n",
            "           Conv2d-88           [-1, 256, 16, 8]         589,824\n",
            "      BatchNorm2d-89           [-1, 256, 16, 8]             512\n",
            "             ReLU-90           [-1, 256, 16, 8]               0\n",
            "  ReflectionPad2d-91          [-1, 256, 20, 12]               0\n",
            "       Downsample-92            [-1, 256, 8, 4]               0\n",
            "           Conv2d-93           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n",
            "  ReflectionPad2d-95          [-1, 512, 20, 12]               0\n",
            "       Downsample-96            [-1, 512, 8, 4]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 4]         524,288\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-101            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 4]             512\n",
            "            ReLU-103            [-1, 256, 8, 4]               0\n",
            "          Conv2d-104            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 4]             512\n",
            "            ReLU-106            [-1, 256, 8, 4]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-111            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 4]             512\n",
            "            ReLU-113            [-1, 256, 8, 4]               0\n",
            "          Conv2d-114            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 4]             512\n",
            "            ReLU-116            [-1, 256, 8, 4]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-121            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 4]             512\n",
            "            ReLU-123            [-1, 256, 8, 4]               0\n",
            "          Conv2d-124            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 4]             512\n",
            "            ReLU-126            [-1, 256, 8, 4]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-131            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 4]             512\n",
            "            ReLU-133            [-1, 256, 8, 4]               0\n",
            "          Conv2d-134            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 4]             512\n",
            "            ReLU-136            [-1, 256, 8, 4]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-141            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-142            [-1, 256, 8, 4]             512\n",
            "            ReLU-143            [-1, 256, 8, 4]               0\n",
            "          Conv2d-144            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 4]             512\n",
            "            ReLU-146            [-1, 256, 8, 4]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-149           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-150           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-151            [-1, 512, 8, 4]         524,288\n",
            "     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-153            [-1, 512, 8, 4]               0\n",
            "          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n",
            "     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-156            [-1, 512, 8, 4]               0\n",
            " ReflectionPad2d-157           [-1, 512, 12, 8]               0\n",
            "      Downsample-158            [-1, 512, 4, 2]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n",
            " ReflectionPad2d-161          [-1, 1024, 12, 8]               0\n",
            "      Downsample-162           [-1, 1024, 4, 2]               0\n",
            "          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-165           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-166           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-169            [-1, 512, 4, 2]               0\n",
            "          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-172            [-1, 512, 4, 2]               0\n",
            "          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-175           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-176           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-179            [-1, 512, 4, 2]               0\n",
            "          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-182            [-1, 512, 4, 2]               0\n",
            "          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-185           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-186           [-1, 2048, 4, 2]               0\n",
            "          Linear-187                 [-1, 1024]       2,098,176\n",
            "     BatchNorm1d-188                 [-1, 1024]           2,048\n",
            "            ReLU-189                 [-1, 1024]               0\n",
            "   DenseNormReLU-190                 [-1, 1024]               0\n",
            "          Linear-191                  [-1, 128]         131,200\n",
            "           Model-192                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 25,739,456\n",
            "Trainable params: 25,739,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 56.18\n",
            "Params size (MB): 98.19\n",
            "Estimated Total Size (MB): 154.46\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NmaAoqCBI9nM",
        "colab": {}
      },
      "source": [
        "train(net = net, model_num = model_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ScMgSnCaI9nP",
        "outputId": "d72c2da0-f005-442e-aed0-761ad62181d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n",
        "create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======>  processing iter 105 / 106  ...   completed\n",
            "=======>  processing iter 616 / 617  ...   completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKHhH5NZI9nS",
        "outputId": "295eb752-26f1-4f6e-f83e-93a06548c911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3368/3368 [02:43<00:00, 21.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mAP is: 0.7417072430419447, cmc is: [0.8738124  0.91270787 0.9376485  0.9483373  0.9578385 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcW7wJMFsa2A",
        "colab_type": "text"
      },
      "source": [
        "## Extra work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "34579b2f-4e7b-43bb-df33-7d5b11f2d3d7",
        "id": "hmbJo6ZXp0Yl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "#checking GPU\n",
        "print(torch.cuda.current_device())\n",
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<torch.cuda.device object at 0x7f5d7975ff28>\n",
            "1\n",
            "Tesla P100-PCIE-16GB\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LfoES3PIeEy",
        "colab_type": "text"
      },
      "source": [
        "Visualizing filters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E--uAbaXIdkR",
        "colab_type": "code",
        "outputId": "8333d43d-9067-491e-8004-e455b31d5ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "a = np.array([1., 1.])\n",
        "filt_2 = a[:,None]*a[None,:]\n",
        "b = np.array([1., 2., 1.])\n",
        "filt_3 = b[:,None]*b[None,:]\n",
        "c = np.array([1., 4., 6., 4., 1.])\n",
        "filt_5 = c[:,None]*c[None,:]\n",
        "\n",
        "print(filt_2, \"\\n\",\n",
        "      filt_3, \"\\n\", filt_5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]] \n",
            " [[1. 2. 1.]\n",
            " [2. 4. 2.]\n",
            " [1. 2. 1.]] \n",
            " [[ 1.  4.  6.  4.  1.]\n",
            " [ 4. 16. 24. 16.  4.]\n",
            " [ 6. 24. 36. 24.  6.]\n",
            " [ 4. 16. 24. 16.  4.]\n",
            " [ 1.  4.  6.  4.  1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eS-FobdnHNv",
        "colab_type": "text"
      },
      "source": [
        "## Models not included in the paper\n",
        "\n",
        "Model 2 = baseline with stride = 1 in last layer\n",
        "\n",
        "Model 3 = Anti-aliased model, re-trained with stride = 2 in last layer\n",
        "\n",
        "Model 4 = AA model with extra connected layers and stride = 2 in last layer. Model uses wrong way of cutting of layers.\n",
        "\n",
        "Model 5 = AA model with extra connected layers, stride = 2 in last layer blurring before GAP. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jaAc9kiAm7hn"
      },
      "source": [
        "### Model 2 \n",
        "\n",
        "Baseline model with last conv layer using stride = 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4v7nuUyum7hk",
        "colab": {}
      },
      "source": [
        "from own_code.backbone import EmbedNetwork"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_w6-jxWQm7hh",
        "colab": {}
      },
      "source": [
        "model_num = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRTyj63Om7hc",
        "colab": {}
      },
      "source": [
        "## model and loss\n",
        "logger.info('setting up backbone model and loss')\n",
        "net = EmbedNetwork(pretrained_base=True).cuda()\n",
        "net = nn.DataParallel(net)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sM4h2pYmm7ha",
        "colab": {}
      },
      "source": [
        "summary(net, (3,128,64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D5HZxWJcm7hX",
        "colab": {}
      },
      "source": [
        "train(net = net, model_num = 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0J3Oulrcm7hQ",
        "colab": {}
      },
      "source": [
        "embed(dataset = X_query, fids = labels_query.fid, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vDD04GDxm7hJ",
        "colab": {}
      },
      "source": [
        "embed(dataset = X_test, fids = labels_test.fid, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3k1Cj8E_m7g_",
        "colab": {}
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EJgFosBl44B",
        "colab_type": "text"
      },
      "source": [
        "### Model 3\n",
        "AA model with stride = 2 on last layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-G1Cssk5wI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy9ZwPHjXjyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.base = model\n",
        "\n",
        "  def forward(self, x):\n",
        "    # shape [N, C, H, W]\n",
        "    x = self.base(x)\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    # shape [N, C]\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zHxA-6tqhAjr",
        "colab": {}
      },
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model()\n",
        "#model = Model().to(device)\n",
        "model = model.cuda()\n",
        "net = nn.DataParallel(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvEN6Se2pIBL",
        "colab_type": "text"
      },
      "source": [
        "### Model 4\n",
        "AA model with stride = 2 on last layer and extra layers added, similar to baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pn5cUMQ5tpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = 4 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSzrsYZSaIER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filter_size = 3\n",
        "net = models_lpf.resnet.resnet50(filter_size=filter_size)\n",
        "net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n",
        "model = torch.nn.Sequential(*(list(net.children())[:-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNTgXfzrru47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseNormReLU(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, *args, **kwargs):\n",
        "        super(DenseNormReLU, self).__init__(*args, **kwargs)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n",
        "        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n",
        "        self.relu = nn.ReLU(inplace = True).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n",
        "embed = nn.Linear(in_features = 1024, out_features = 128).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyEO3jdBsZMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.base = model\n",
        "    self.fc_head = fc_head\n",
        "    self.embed = embed\n",
        "\n",
        "  def forward(self, x):\n",
        "    # shape [N, C, H, W]\n",
        "    x = self.base(x)\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.contiguous().view(-1, 2048 )\n",
        "    # shape [N, C]\n",
        "    x = self.fc_head(x)\n",
        "    x = self.embed(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JegAvkNcX-VA",
        "colab_type": "code",
        "outputId": "2f173c91-b8af-464c-c1ac-8e92431711fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model()\n",
        "#model = Model().to(device)\n",
        "model = model.cuda()\n",
        "net = nn.DataParallel(model)\n",
        "summary(net, (3, 128,64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 63, 31]               0\n",
            "   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n",
            "        Downsample-6           [-1, 64, 32, 16]               0\n",
            "            Conv2d-7           [-1, 64, 32, 16]           4,096\n",
            "       BatchNorm2d-8           [-1, 64, 32, 16]             128\n",
            "              ReLU-9           [-1, 64, 32, 16]               0\n",
            "           Conv2d-10           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 16]             128\n",
            "             ReLU-12           [-1, 64, 32, 16]               0\n",
            "           Conv2d-13          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 16]             512\n",
            "           Conv2d-15          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-16          [-1, 256, 32, 16]             512\n",
            "             ReLU-17          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-18          [-1, 256, 32, 16]               0\n",
            "           Conv2d-19           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 16]             128\n",
            "             ReLU-21           [-1, 64, 32, 16]               0\n",
            "           Conv2d-22           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 32, 16]             128\n",
            "             ReLU-24           [-1, 64, 32, 16]               0\n",
            "           Conv2d-25          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-26          [-1, 256, 32, 16]             512\n",
            "             ReLU-27          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-28          [-1, 256, 32, 16]               0\n",
            "           Conv2d-29           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-30           [-1, 64, 32, 16]             128\n",
            "             ReLU-31           [-1, 64, 32, 16]               0\n",
            "           Conv2d-32           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 32, 16]             128\n",
            "             ReLU-34           [-1, 64, 32, 16]               0\n",
            "           Conv2d-35          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-36          [-1, 256, 32, 16]             512\n",
            "             ReLU-37          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-38          [-1, 256, 32, 16]               0\n",
            "           Conv2d-39          [-1, 128, 32, 16]          32,768\n",
            "      BatchNorm2d-40          [-1, 128, 32, 16]             256\n",
            "             ReLU-41          [-1, 128, 32, 16]               0\n",
            "           Conv2d-42          [-1, 128, 32, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 32, 16]             256\n",
            "             ReLU-44          [-1, 128, 32, 16]               0\n",
            "  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n",
            "       Downsample-46           [-1, 128, 16, 8]               0\n",
            "           Conv2d-47           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n",
            "  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n",
            "       Downsample-50           [-1, 256, 16, 8]               0\n",
            "           Conv2d-51           [-1, 512, 16, 8]         131,072\n",
            "      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-53           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-54           [-1, 512, 16, 8]               0\n",
            "           Conv2d-55           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-56           [-1, 128, 16, 8]             256\n",
            "             ReLU-57           [-1, 128, 16, 8]               0\n",
            "           Conv2d-58           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-59           [-1, 128, 16, 8]             256\n",
            "             ReLU-60           [-1, 128, 16, 8]               0\n",
            "           Conv2d-61           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-63           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-64           [-1, 512, 16, 8]               0\n",
            "           Conv2d-65           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-66           [-1, 128, 16, 8]             256\n",
            "             ReLU-67           [-1, 128, 16, 8]               0\n",
            "           Conv2d-68           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-69           [-1, 128, 16, 8]             256\n",
            "             ReLU-70           [-1, 128, 16, 8]               0\n",
            "           Conv2d-71           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-73           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-74           [-1, 512, 16, 8]               0\n",
            "           Conv2d-75           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-76           [-1, 128, 16, 8]             256\n",
            "             ReLU-77           [-1, 128, 16, 8]               0\n",
            "           Conv2d-78           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-79           [-1, 128, 16, 8]             256\n",
            "             ReLU-80           [-1, 128, 16, 8]               0\n",
            "           Conv2d-81           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-83           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-84           [-1, 512, 16, 8]               0\n",
            "           Conv2d-85           [-1, 256, 16, 8]         131,072\n",
            "      BatchNorm2d-86           [-1, 256, 16, 8]             512\n",
            "             ReLU-87           [-1, 256, 16, 8]               0\n",
            "           Conv2d-88           [-1, 256, 16, 8]         589,824\n",
            "      BatchNorm2d-89           [-1, 256, 16, 8]             512\n",
            "             ReLU-90           [-1, 256, 16, 8]               0\n",
            "  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n",
            "       Downsample-92            [-1, 256, 8, 4]               0\n",
            "           Conv2d-93           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n",
            "  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n",
            "       Downsample-96            [-1, 512, 8, 4]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 4]         524,288\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-101            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 4]             512\n",
            "            ReLU-103            [-1, 256, 8, 4]               0\n",
            "          Conv2d-104            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 4]             512\n",
            "            ReLU-106            [-1, 256, 8, 4]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-111            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 4]             512\n",
            "            ReLU-113            [-1, 256, 8, 4]               0\n",
            "          Conv2d-114            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 4]             512\n",
            "            ReLU-116            [-1, 256, 8, 4]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-121            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 4]             512\n",
            "            ReLU-123            [-1, 256, 8, 4]               0\n",
            "          Conv2d-124            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 4]             512\n",
            "            ReLU-126            [-1, 256, 8, 4]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-131            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 4]             512\n",
            "            ReLU-133            [-1, 256, 8, 4]               0\n",
            "          Conv2d-134            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 4]             512\n",
            "            ReLU-136            [-1, 256, 8, 4]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-141            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-142            [-1, 256, 8, 4]             512\n",
            "            ReLU-143            [-1, 256, 8, 4]               0\n",
            "          Conv2d-144            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 4]             512\n",
            "            ReLU-146            [-1, 256, 8, 4]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-149           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-150           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-151            [-1, 512, 8, 4]         524,288\n",
            "     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-153            [-1, 512, 8, 4]               0\n",
            "          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n",
            "     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-156            [-1, 512, 8, 4]               0\n",
            " ReflectionPad2d-157           [-1, 512, 10, 6]               0\n",
            "      Downsample-158            [-1, 512, 4, 2]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n",
            " ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n",
            "      Downsample-162           [-1, 1024, 4, 2]               0\n",
            "          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-165           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-166           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-169            [-1, 512, 4, 2]               0\n",
            "          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-172            [-1, 512, 4, 2]               0\n",
            "          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-175           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-176           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-179            [-1, 512, 4, 2]               0\n",
            "          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-182            [-1, 512, 4, 2]               0\n",
            "          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-185           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-186           [-1, 2048, 4, 2]               0\n",
            "AdaptiveAvgPool2d-187           [-1, 2048, 1, 1]               0\n",
            "          Linear-188                 [-1, 1024]       2,098,176\n",
            "     BatchNorm1d-189                 [-1, 1024]           2,048\n",
            "            ReLU-190                 [-1, 1024]               0\n",
            "   DenseNormReLU-191                 [-1, 1024]               0\n",
            "          Linear-192                  [-1, 128]         131,200\n",
            "           Model-193                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 25,739,456\n",
            "Trainable params: 25,739,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 55.01\n",
            "Params size (MB): 98.19\n",
            "Estimated Total Size (MB): 153.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BxzSMWb2fr0W",
        "colab": {}
      },
      "source": [
        "train(net = net, model_num = model_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H5nZYyLshrHb",
        "colab": {}
      },
      "source": [
        "create_embs(dataset = X_query, fids = labels_query.fid, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ba1e9ee4-7556-42f5-8593-415a2b5e66c5",
        "id": "5buITy7mhrHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embed(dataset = X_test, fids = labels_test.fid, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======>  processing iter 616 / 617  ...   completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8233be9a-c372-4ad5-d16f-5a6a80775814",
        "id": "tqNX7KDChrHh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3368/3368 [02:35<00:00, 21.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mAP is: 0.7430370803791962, cmc is: [0.8770784  0.9141924  0.93171024 0.94447744 0.9513064 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9OrIRc2k4Sd",
        "colab_type": "text"
      },
      "source": [
        "### Model 5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYrMMLp4pSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "if not os.path.exists('./res'): os.makedirs('./res')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW7xCSIFk3g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_num = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zTX9DX0Mk_oZ",
        "colab": {}
      },
      "source": [
        "filter_size = 3\n",
        "net = models_lpf.resnet.resnet50(filter_size=filter_size)\n",
        "net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n",
        "model = torch.nn.Sequential(*(list(net.children())[:-2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XjUlncFAk_od",
        "colab": {}
      },
      "source": [
        "class DenseNormReLU(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, *args, **kwargs):\n",
        "        super(DenseNormReLU, self).__init__(*args, **kwargs)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n",
        "        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n",
        "        self.relu = nn.ReLU(inplace = True).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "  \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n",
        "embed = nn.Linear(in_features = 1024, out_features = 128).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiwkK5yhlPi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blur = nn.Sequential(Downsample(filt_size = filter_size, channels = 2048, stride = 1),nn.ReLU(inplace=True), )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_L73vzaBk_of",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.base = model\n",
        "    self.blur = blur\n",
        "    self.fc_head = fc_head\n",
        "    self.embed = embed\n",
        "\n",
        "  def forward(self, x):\n",
        "    # shape [N, C, H, W]\n",
        "    x = self.base(x)\n",
        "    x = self.blur(x)\n",
        "    x = F.avg_pool2d(x, x.size()[2:])\n",
        "    x = x.contiguous().view(-1, 2048)\n",
        "    # shape [N, C]\n",
        "    x = self.fc_head(x)\n",
        "    x = self.embed(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cdNLcz3pk_oh",
        "outputId": "41cac7fc-165e-4d0c-f979-9c95b383f67e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model()\n",
        "#model = Model().to(device)\n",
        "model = model.cuda()\n",
        "net = nn.DataParallel(model)\n",
        "summary(net, (3, 128,64))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 32]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 32]             128\n",
            "              ReLU-3           [-1, 64, 64, 32]               0\n",
            "         MaxPool2d-4           [-1, 64, 63, 31]               0\n",
            "   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n",
            "        Downsample-6           [-1, 64, 32, 16]               0\n",
            "            Conv2d-7           [-1, 64, 32, 16]           4,096\n",
            "       BatchNorm2d-8           [-1, 64, 32, 16]             128\n",
            "              ReLU-9           [-1, 64, 32, 16]               0\n",
            "           Conv2d-10           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 16]             128\n",
            "             ReLU-12           [-1, 64, 32, 16]               0\n",
            "           Conv2d-13          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 16]             512\n",
            "           Conv2d-15          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-16          [-1, 256, 32, 16]             512\n",
            "             ReLU-17          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-18          [-1, 256, 32, 16]               0\n",
            "           Conv2d-19           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-20           [-1, 64, 32, 16]             128\n",
            "             ReLU-21           [-1, 64, 32, 16]               0\n",
            "           Conv2d-22           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-23           [-1, 64, 32, 16]             128\n",
            "             ReLU-24           [-1, 64, 32, 16]               0\n",
            "           Conv2d-25          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-26          [-1, 256, 32, 16]             512\n",
            "             ReLU-27          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-28          [-1, 256, 32, 16]               0\n",
            "           Conv2d-29           [-1, 64, 32, 16]          16,384\n",
            "      BatchNorm2d-30           [-1, 64, 32, 16]             128\n",
            "             ReLU-31           [-1, 64, 32, 16]               0\n",
            "           Conv2d-32           [-1, 64, 32, 16]          36,864\n",
            "      BatchNorm2d-33           [-1, 64, 32, 16]             128\n",
            "             ReLU-34           [-1, 64, 32, 16]               0\n",
            "           Conv2d-35          [-1, 256, 32, 16]          16,384\n",
            "      BatchNorm2d-36          [-1, 256, 32, 16]             512\n",
            "             ReLU-37          [-1, 256, 32, 16]               0\n",
            "       Bottleneck-38          [-1, 256, 32, 16]               0\n",
            "           Conv2d-39          [-1, 128, 32, 16]          32,768\n",
            "      BatchNorm2d-40          [-1, 128, 32, 16]             256\n",
            "             ReLU-41          [-1, 128, 32, 16]               0\n",
            "           Conv2d-42          [-1, 128, 32, 16]         147,456\n",
            "      BatchNorm2d-43          [-1, 128, 32, 16]             256\n",
            "             ReLU-44          [-1, 128, 32, 16]               0\n",
            "  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n",
            "       Downsample-46           [-1, 128, 16, 8]               0\n",
            "           Conv2d-47           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n",
            "  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n",
            "       Downsample-50           [-1, 256, 16, 8]               0\n",
            "           Conv2d-51           [-1, 512, 16, 8]         131,072\n",
            "      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-53           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-54           [-1, 512, 16, 8]               0\n",
            "           Conv2d-55           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-56           [-1, 128, 16, 8]             256\n",
            "             ReLU-57           [-1, 128, 16, 8]               0\n",
            "           Conv2d-58           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-59           [-1, 128, 16, 8]             256\n",
            "             ReLU-60           [-1, 128, 16, 8]               0\n",
            "           Conv2d-61           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-63           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-64           [-1, 512, 16, 8]               0\n",
            "           Conv2d-65           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-66           [-1, 128, 16, 8]             256\n",
            "             ReLU-67           [-1, 128, 16, 8]               0\n",
            "           Conv2d-68           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-69           [-1, 128, 16, 8]             256\n",
            "             ReLU-70           [-1, 128, 16, 8]               0\n",
            "           Conv2d-71           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-73           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-74           [-1, 512, 16, 8]               0\n",
            "           Conv2d-75           [-1, 128, 16, 8]          65,536\n",
            "      BatchNorm2d-76           [-1, 128, 16, 8]             256\n",
            "             ReLU-77           [-1, 128, 16, 8]               0\n",
            "           Conv2d-78           [-1, 128, 16, 8]         147,456\n",
            "      BatchNorm2d-79           [-1, 128, 16, 8]             256\n",
            "             ReLU-80           [-1, 128, 16, 8]               0\n",
            "           Conv2d-81           [-1, 512, 16, 8]          65,536\n",
            "      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n",
            "             ReLU-83           [-1, 512, 16, 8]               0\n",
            "       Bottleneck-84           [-1, 512, 16, 8]               0\n",
            "           Conv2d-85           [-1, 256, 16, 8]         131,072\n",
            "      BatchNorm2d-86           [-1, 256, 16, 8]             512\n",
            "             ReLU-87           [-1, 256, 16, 8]               0\n",
            "           Conv2d-88           [-1, 256, 16, 8]         589,824\n",
            "      BatchNorm2d-89           [-1, 256, 16, 8]             512\n",
            "             ReLU-90           [-1, 256, 16, 8]               0\n",
            "  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n",
            "       Downsample-92            [-1, 256, 8, 4]               0\n",
            "           Conv2d-93           [-1, 1024, 8, 4]         262,144\n",
            "      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n",
            "  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n",
            "       Downsample-96            [-1, 512, 8, 4]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 4]         524,288\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-101            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-102            [-1, 256, 8, 4]             512\n",
            "            ReLU-103            [-1, 256, 8, 4]               0\n",
            "          Conv2d-104            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-105            [-1, 256, 8, 4]             512\n",
            "            ReLU-106            [-1, 256, 8, 4]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-111            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-112            [-1, 256, 8, 4]             512\n",
            "            ReLU-113            [-1, 256, 8, 4]               0\n",
            "          Conv2d-114            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-115            [-1, 256, 8, 4]             512\n",
            "            ReLU-116            [-1, 256, 8, 4]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-121            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-122            [-1, 256, 8, 4]             512\n",
            "            ReLU-123            [-1, 256, 8, 4]               0\n",
            "          Conv2d-124            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-125            [-1, 256, 8, 4]             512\n",
            "            ReLU-126            [-1, 256, 8, 4]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-131            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-132            [-1, 256, 8, 4]             512\n",
            "            ReLU-133            [-1, 256, 8, 4]               0\n",
            "          Conv2d-134            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-135            [-1, 256, 8, 4]             512\n",
            "            ReLU-136            [-1, 256, 8, 4]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-141            [-1, 256, 8, 4]         262,144\n",
            "     BatchNorm2d-142            [-1, 256, 8, 4]             512\n",
            "            ReLU-143            [-1, 256, 8, 4]               0\n",
            "          Conv2d-144            [-1, 256, 8, 4]         589,824\n",
            "     BatchNorm2d-145            [-1, 256, 8, 4]             512\n",
            "            ReLU-146            [-1, 256, 8, 4]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 4]         262,144\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n",
            "            ReLU-149           [-1, 1024, 8, 4]               0\n",
            "      Bottleneck-150           [-1, 1024, 8, 4]               0\n",
            "          Conv2d-151            [-1, 512, 8, 4]         524,288\n",
            "     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-153            [-1, 512, 8, 4]               0\n",
            "          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n",
            "     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n",
            "            ReLU-156            [-1, 512, 8, 4]               0\n",
            " ReflectionPad2d-157           [-1, 512, 10, 6]               0\n",
            "      Downsample-158            [-1, 512, 4, 2]               0\n",
            "          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n",
            " ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n",
            "      Downsample-162           [-1, 1024, 4, 2]               0\n",
            "          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-165           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-166           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-169            [-1, 512, 4, 2]               0\n",
            "          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-172            [-1, 512, 4, 2]               0\n",
            "          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-175           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-176           [-1, 2048, 4, 2]               0\n",
            "          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n",
            "     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-179            [-1, 512, 4, 2]               0\n",
            "          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n",
            "     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n",
            "            ReLU-182            [-1, 512, 4, 2]               0\n",
            "          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n",
            "     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n",
            "            ReLU-185           [-1, 2048, 4, 2]               0\n",
            "      Bottleneck-186           [-1, 2048, 4, 2]               0\n",
            " ReflectionPad2d-187           [-1, 2048, 6, 4]               0\n",
            "      Downsample-188           [-1, 2048, 4, 2]               0\n",
            "            ReLU-189           [-1, 2048, 4, 2]               0\n",
            "          Linear-190                 [-1, 1024]       2,098,176\n",
            "     BatchNorm1d-191                 [-1, 1024]           2,048\n",
            "            ReLU-192                 [-1, 1024]               0\n",
            "   DenseNormReLU-193                 [-1, 1024]               0\n",
            "          Linear-194                  [-1, 128]         131,200\n",
            "           Model-195                  [-1, 128]               0\n",
            "================================================================\n",
            "Total params: 25,739,456\n",
            "Trainable params: 25,739,456\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 55.62\n",
            "Params size (MB): 98.19\n",
            "Estimated Total Size (MB): 153.90\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CKagf4GS4OLV",
        "colab": {}
      },
      "source": [
        "train(net = net, model_num = model_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ymTf9cYG4OLZ",
        "outputId": "4881e30d-d4a4-4579-9ad0-f7de668b780c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n",
        "create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======>  processing iter 105 / 106  ...   completed\n",
            "=======>  processing iter 616 / 617  ...   completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6a6Kpq94OLf",
        "outputId": "9e960042-313d-4e82-fca3-139b1027ee56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n",
        "query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n",
        "cmc_rank = 5\n",
        "evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3368/3368 [02:32<00:00, 22.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mAP is: 0.747139206882985, cmc is: [0.87321854 0.91567695 0.9340855  0.94447744 0.952791  ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}