{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch self_implementation.ipynb","provenance":[{"file_id":"13ho6vdlollIlY1yFjXrWqqGeCE7gw9nj","timestamp":1573556494098},{"file_id":"1mnjUZcaxrJmmCB7K4OgEZVcruMKrIqJM","timestamp":1572432073987}],"collapsed_sections":["4K2_eKts75g4","-EJgFosBl44B"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SSZU--6uXghq","colab_type":"text"},"source":["# Person re-id \n","Before using this notebook it is expected that an HDF5 file is created. See notebook \"create hdf5.ipynb\""]},{"cell_type":"markdown","metadata":{"id":"xtT96bD6YSy7","colab_type":"text"},"source":["## Set-up\n","first, import packages"]},{"cell_type":"code","metadata":{"id":"WKuUOjVlQUUH","colab_type":"code","colab":{}},"source":["# import packages\n","import h5py\n","import os\n","import pandas as pd\n","from google.colab import drive\n","import cv2\n","import datetime as dt\n","from PIL import Image\n","import numpy as np\n","import sys\n","import logging\n","import time\n","import itertools\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.utils.data import DataLoader\n","import pickle\n","from tqdm import tqdm\n","from logger import logger\n","from torchsummary import summary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpSN0g67YcyC","colab_type":"code","outputId":"a0fa6699-a8fd-469d-b08a-88f3d56e3407","executionInfo":{"status":"ok","timestamp":1574756037938,"user_tz":-60,"elapsed":278313,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["#mount the drive to be able to access images, functions and classes\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"58S4gB92Wya3","colab_type":"code","colab":{}},"source":["os.chdir(\"/content/drive/My Drive/Thesis re-id/triplet-reid-master\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAPzgn_EYuWs","colab_type":"code","colab":{}},"source":["# Loading local classes and functions\n","from own_code.backbone_normal import EmbedNetwork\n","from own_code.loss import TripletLoss\n","from own_code.triplet_selector import BatchHardTripletSelector\n","from own_code.batch_sampler import BatchSampler\n","from own_code.Market1501 import Market1501\n","from own_code.optimizer import AdamOptimWrapper\n","#import the anti-aliased networks\n","from models_lpf import *\n","import models_lpf.resnet"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwS4OPOIX-VJ","colab_type":"text"},"source":["## Loading and preprocessing data"]},{"cell_type":"code","metadata":{"id":"jkMNav6ZQiyx","colab_type":"code","colab":{}},"source":["# Labels\n","labels = pd.read_csv('data/market1501_train.csv', names = ['pid', 'fid'], header = None, dtype = str)\n","labels_query = pd.read_csv('data/market1501_query.csv', names = ['pid', 'fid'], header = None, dtype = str)\n","labels_test = pd.read_csv('data/market1501_test.csv', names = ['pid', 'fid'], header = None, dtype = str)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8hrPCzrcjG7","colab_type":"code","colab":{}},"source":["num_images = len(labels)\n","\n","height = 128\n","width = 64\n","net_input_size = (128,64)\n","channels = 3"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRDOEnZ9DPSI","colab_type":"text"},"source":["### Reading the h5py Files"]},{"cell_type":"code","metadata":{"id":"5h--hlJ7exXD","colab_type":"code","colab":{}},"source":["fileName = 'data_final.h5'\n","fileName_qt = 'data_qt.h5'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfbXc61suBbv","colab_type":"code","colab":{}},"source":["with h5py.File(fileName, \"a\") as out:\n","  X_train = np.asarray(out[\"X_train\"])\n","  #Y_train = np.asarray(out[\"Y_train\"])\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5aLjJp2ew8F","colab_type":"code","colab":{}},"source":["with h5py.File(fileName_qt, \"a\") as out:\n","  X_query = np.asarray(out[\"X_dev\"])\n","  X_test = np.asarray(out[\"X_test\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zk1zZDrUuBUi","colab_type":"code","outputId":"6d0f5bb6-faa3-41e8-88f7-e53083d830fb","executionInfo":{"status":"ok","timestamp":1574757126435,"user_tz":-60,"elapsed":23032,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(X_train.shape)\n","print(X_query.shape)\n","print(X_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(12936, 128, 64, 3)\n","(3368, 128, 64, 3)\n","(19732, 128, 64, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nn1NVYkvb7l1","colab_type":"text"},"source":["## Set up core functions\n"]},{"cell_type":"markdown","metadata":{"id":"0Pu8ZHficFiF","colab_type":"text"},"source":["### Train function\n","All models are trained with the same parameters and loss functions. Therefore a general function is made to perform the training for all models. "]},{"cell_type":"code","metadata":{"id":"qdqhs5phNOcf","colab_type":"code","colab":{}},"source":["def train(net, model_num):\n","  triplet_loss = TripletLoss(margin = None).cuda() # no margin means soft-margin\n","\n","  ## optimizer\n","  logger.info('creating optimizer')\n","  optim = AdamOptimWrapper(net.parameters(), lr = 3e-4, wd = 0, t0 = 15000, t1 = 25000)\n","\n","  ## dataloader\n","  selector = BatchHardTripletSelector()\n","\n","  ds = Market1501(pids_list=list(labels.fid), array=X_train, is_train = True)\n","  sampler = BatchSampler(ds, 18, 4)\n","  dl = DataLoader(ds, batch_sampler = sampler, num_workers = 4)\n","  diter = iter(dl)\n","\n","  logger.info('start training ...')\n","  loss_avg = []\n","  count = 0\n","  t_start = time.time()\n","  while True:\n","    try:\n","      imgs, lbs, _ = next(diter)\n","    except StopIteration:\n","      diter = iter(dl)\n","      imgs, lbs, _ = next(diter)\n","\n","    net.train()\n","    imgs = imgs.cuda()\n","    lbs = lbs.cuda()\n","    embds = net(imgs)\n","    anchor, positives, negatives = selector(embds, lbs)\n","\n","    loss = triplet_loss(anchor, positives, negatives)\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","\n","    loss_avg.append(loss.detach().cpu().numpy())\n","    if count % 20 == 0 and count != 0:\n","      loss_avg = sum(loss_avg) / len(loss_avg)\n","      t_end = time.time()\n","      time_interval = t_end - t_start\n","      logger.info('iter: {}, loss: {:4f}, lr: {:4f}, time: {:3f}'.format(count, loss_avg, optim.lr, time_interval))\n","      loss_avg = []\n","      t_start = t_end\n","\n","    count += 1\n","    if count == 25000: break\n","\n","  ## dump model\n","  logger.info('saving trained model')\n","  torch.save(net.module.state_dict(), './res/model{}.pkl'.format(model_num))\n","\n","  logger.info('everything finished')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPDSD1fmaw40","colab_type":"text"},"source":["## Create Embeddings\n","When the model is trained, it can be applied to both the Query and Test images. This is done by the function create_emb"]},{"cell_type":"code","metadata":{"id":"TDWd0nIDIf35","colab_type":"code","colab":{}},"source":["def create_emb(dataset, fids, store_path, model_num):\n","  torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","  ## logging\n","  FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","  logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","  logger = logging.getLogger(__name__)\n","  ## restore model\n","  logger.info('restoring model')\n","  model = net\n","  #model = nn.DataParallel(model)\n","  model = net.cuda()\n","  model.module.load_state_dict(torch.load('./res/model{}.pkl'.format(model_num)))\n","  model = nn.DataParallel(model)\n","  model.eval()\n","\n","  ## load gallery dataset\n","  batchsize = 32\n","  ds = Market1501(pids_list=list(fids), array=dataset, is_train = False)\n","  dl = DataLoader(ds, batch_size = batchsize, drop_last = False, num_workers = 4)\n","\n","  ## embedding samples\n","  logger.info('start embedding')\n","  all_iter_nums = len(ds) // batchsize + 1\n","  embeddings = []\n","  label_ids = []\n","  label_cams = []\n","  for it, (img, lb_id, lb_cam) in enumerate(dl):\n","    print('\\r=======>  processing iter {} / {}'.format(it, all_iter_nums),\n","            end = '', flush = True)\n","    label_ids.append(lb_id)\n","    label_cams.append(lb_cam)\n","    embds = []\n","    for im in img:\n","        im = im.cuda()\n","        embd = model(im).detach().cpu().numpy()\n","        embds.append(embd)\n","    embed = sum(embds) / len(embds)\n","    embeddings.append(embed)\n","  print('  ...   completed')\n","\n","  embeddings = np.vstack(embeddings)\n","  label_ids = np.hstack(label_ids)\n","  label_cams = np.hstack(label_cams)\n","\n","  ## dump results\n","  logger.info('dump embeddings')\n","  embd_res = {'embeddings': embeddings, 'label_ids': label_ids, 'label_cams': label_cams}\n","  with open(store_path, 'wb') as fw:\n","    pickle.dump(embd_res, fw)\n","\n","  logger.info('embedding finished')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwdlBg0T3pQQ","colab_type":"code","colab":{}},"source":["from utils import pdist_np as pdist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ooSi6py4LGh","colab_type":"code","colab":{}},"source":["def evaluate(test_embs, query_embs, cmc_rank):\n","    ## logging\n","    FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","    logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","    logger = logging.getLogger(__name__)\n","\n","    ## load embeddings\n","    logger.info('loading gallery embeddings')\n","    with open(test_embs, 'rb') as fr:\n","        gallery_dict = pickle.load(fr)\n","        emb_gallery, lb_ids_gallery, lb_cams_gallery = gallery_dict['embeddings'], gallery_dict['label_ids'], gallery_dict['label_cams']\n","    logger.info('loading query embeddings')\n","    with open(query_embs, 'rb') as fr:\n","        query_dict = pickle.load(fr)\n","        emb_query, lb_ids_query, lb_cams_query = query_dict['embeddings'], query_dict['label_ids'], query_dict['label_cams']\n","\n","    ## compute and clean distance matrix\n","    dist_mtx = pdist(emb_query, emb_gallery)\n","    n_q, n_g = dist_mtx.shape\n","    indices = np.argsort(dist_mtx, axis = 1)\n","    matches = lb_ids_gallery[indices] == lb_ids_query[:, np.newaxis]\n","    matches = matches.astype(np.int32)\n","    all_aps = []\n","    all_cmcs = []\n","    logger.info('starting evaluating ...')\n","    for qidx in tqdm(range(n_q)):\n","        qpid = lb_ids_query[qidx]\n","        qcam = lb_cams_query[qidx]\n","\n","        order = indices[qidx]\n","        pid_diff = lb_ids_gallery[order] != qpid\n","        cam_diff = lb_cams_gallery[order] != qcam\n","        useful = lb_ids_gallery[order] != -1\n","        keep = np.logical_or(pid_diff, cam_diff)\n","        keep = np.logical_and(keep, useful)\n","        match = matches[qidx][keep]\n","\n","        if not np.any(match): continue\n","\n","        cmc = match.cumsum()\n","        cmc[cmc > 1] = 1\n","        all_cmcs.append(cmc[:cmc_rank])\n","\n","        num_real = match.sum()\n","        match_cum = match.cumsum()\n","        match_cum = [el / (1.0 + i) for i, el in enumerate(match_cum)]\n","        match_cum = np.array(match_cum) * match\n","        ap = match_cum.sum() / num_real\n","        all_aps.append(ap)\n","\n","    assert len(all_aps) > 0, \"NO QUERY MATCHED\"\n","    mAP = sum(all_aps) / len(all_aps)\n","    all_cmcs = np.array(all_cmcs, dtype = np.float32)\n","    cmc = np.mean(all_cmcs, axis = 0)\n","\n","    print('mAP is: {}, cmc is: {}'.format(mAP, cmc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e3vkU2f84uJA","colab_type":"text"},"source":["## Model log\n","Model 1 = baseline with stride = 2 in last layer \n","\n","\n","\n","\n","\n","\n","\n","model 6 = AA model with extra connected layers and stride = 2 in last layer\n","\n","\n","Model 7 = AA model filter = 2\n","\n","Model 8 = AA Model filter = 5\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yjn8kOBESVC6","colab_type":"text"},"source":["## Baseline"]},{"cell_type":"code","metadata":{"id":"jCMIUYXFfkjX","colab_type":"code","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MD8eF25cjCX","colab_type":"text"},"source":["### Model 1\n","Baseline model with last conv layer using stride = 1."]},{"cell_type":"code","metadata":{"id":"mozPqtNKciJ_","colab_type":"code","colab":{}},"source":["from own_code.backbone_normal import EmbedNetwork"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_qaud5p5dvT","colab_type":"code","colab":{}},"source":["model_num = 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HF_imTHacwCO","colab_type":"code","outputId":"82d17aaa-36de-4053-b793-03edb9711236","executionInfo":{"status":"ok","timestamp":1574321635230,"user_tz":-60,"elapsed":28126,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["logger.info('setting up backbone model and loss')\n","net = EmbedNetwork(pretrained_base=True).cuda()\n","net_1 = nn.DataParallel(net)\n","summary(net_1, (3,128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["setting up backbone model and loss\n","Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 179MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 32, 16]               0\n","            Conv2d-5           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-6           [-1, 64, 32, 16]             128\n","              ReLU-7           [-1, 64, 32, 16]               0\n","            Conv2d-8           [-1, 64, 32, 16]          36,864\n","       BatchNorm2d-9           [-1, 64, 32, 16]             128\n","             ReLU-10           [-1, 64, 32, 16]               0\n","           Conv2d-11          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-12          [-1, 256, 32, 16]             512\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","             ReLU-15          [-1, 256, 32, 16]               0\n","       Bottleneck-16          [-1, 256, 32, 16]               0\n","           Conv2d-17           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-18           [-1, 64, 32, 16]             128\n","             ReLU-19           [-1, 64, 32, 16]               0\n","           Conv2d-20           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-21           [-1, 64, 32, 16]             128\n","             ReLU-22           [-1, 64, 32, 16]               0\n","           Conv2d-23          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-24          [-1, 256, 32, 16]             512\n","             ReLU-25          [-1, 256, 32, 16]               0\n","       Bottleneck-26          [-1, 256, 32, 16]               0\n","           Conv2d-27           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-28           [-1, 64, 32, 16]             128\n","             ReLU-29           [-1, 64, 32, 16]               0\n","           Conv2d-30           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-31           [-1, 64, 32, 16]             128\n","             ReLU-32           [-1, 64, 32, 16]               0\n","           Conv2d-33          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-34          [-1, 256, 32, 16]             512\n","             ReLU-35          [-1, 256, 32, 16]               0\n","       Bottleneck-36          [-1, 256, 32, 16]               0\n","           Conv2d-37          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-38          [-1, 128, 32, 16]             256\n","             ReLU-39          [-1, 128, 32, 16]               0\n","           Conv2d-40           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-41           [-1, 128, 16, 8]             256\n","             ReLU-42           [-1, 128, 16, 8]               0\n","           Conv2d-43           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-44           [-1, 512, 16, 8]           1,024\n","           Conv2d-45           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-46           [-1, 512, 16, 8]           1,024\n","             ReLU-47           [-1, 512, 16, 8]               0\n","       Bottleneck-48           [-1, 512, 16, 8]               0\n","           Conv2d-49           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-50           [-1, 128, 16, 8]             256\n","             ReLU-51           [-1, 128, 16, 8]               0\n","           Conv2d-52           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-53           [-1, 128, 16, 8]             256\n","             ReLU-54           [-1, 128, 16, 8]               0\n","           Conv2d-55           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 512, 16, 8]           1,024\n","             ReLU-57           [-1, 512, 16, 8]               0\n","       Bottleneck-58           [-1, 512, 16, 8]               0\n","           Conv2d-59           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-60           [-1, 128, 16, 8]             256\n","             ReLU-61           [-1, 128, 16, 8]               0\n","           Conv2d-62           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-63           [-1, 128, 16, 8]             256\n","             ReLU-64           [-1, 128, 16, 8]               0\n","           Conv2d-65           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 512, 16, 8]           1,024\n","             ReLU-67           [-1, 512, 16, 8]               0\n","       Bottleneck-68           [-1, 512, 16, 8]               0\n","           Conv2d-69           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-70           [-1, 128, 16, 8]             256\n","             ReLU-71           [-1, 128, 16, 8]               0\n","           Conv2d-72           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-73           [-1, 128, 16, 8]             256\n","             ReLU-74           [-1, 128, 16, 8]               0\n","           Conv2d-75           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 512, 16, 8]           1,024\n","             ReLU-77           [-1, 512, 16, 8]               0\n","       Bottleneck-78           [-1, 512, 16, 8]               0\n","           Conv2d-79           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-80           [-1, 256, 16, 8]             512\n","             ReLU-81           [-1, 256, 16, 8]               0\n","           Conv2d-82            [-1, 256, 8, 4]         589,824\n","      BatchNorm2d-83            [-1, 256, 8, 4]             512\n","             ReLU-84            [-1, 256, 8, 4]               0\n","           Conv2d-85           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-86           [-1, 1024, 8, 4]           2,048\n","           Conv2d-87           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-88           [-1, 1024, 8, 4]           2,048\n","             ReLU-89           [-1, 1024, 8, 4]               0\n","       Bottleneck-90           [-1, 1024, 8, 4]               0\n","           Conv2d-91            [-1, 256, 8, 4]         262,144\n","      BatchNorm2d-92            [-1, 256, 8, 4]             512\n","             ReLU-93            [-1, 256, 8, 4]               0\n","           Conv2d-94            [-1, 256, 8, 4]         589,824\n","      BatchNorm2d-95            [-1, 256, 8, 4]             512\n","             ReLU-96            [-1, 256, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-142            [-1, 512, 8, 4]           1,024\n","            ReLU-143            [-1, 512, 8, 4]               0\n","          Conv2d-144            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-145            [-1, 512, 4, 2]           1,024\n","            ReLU-146            [-1, 512, 4, 2]               0\n","          Conv2d-147           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-148           [-1, 2048, 4, 2]           4,096\n","          Conv2d-149           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-150           [-1, 2048, 4, 2]           4,096\n","            ReLU-151           [-1, 2048, 4, 2]               0\n","      Bottleneck-152           [-1, 2048, 4, 2]               0\n","          Conv2d-153            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-154            [-1, 512, 4, 2]           1,024\n","            ReLU-155            [-1, 512, 4, 2]               0\n","          Conv2d-156            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-157            [-1, 512, 4, 2]           1,024\n","            ReLU-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n","            ReLU-161           [-1, 2048, 4, 2]               0\n","      Bottleneck-162           [-1, 2048, 4, 2]               0\n","          Conv2d-163            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-164            [-1, 512, 4, 2]           1,024\n","            ReLU-165            [-1, 512, 4, 2]               0\n","          Conv2d-166            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-167            [-1, 512, 4, 2]           1,024\n","            ReLU-168            [-1, 512, 4, 2]               0\n","          Conv2d-169           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-170           [-1, 2048, 4, 2]           4,096\n","            ReLU-171           [-1, 2048, 4, 2]               0\n","      Bottleneck-172           [-1, 2048, 4, 2]               0\n","          Linear-173                 [-1, 1024]       2,098,176\n","     BatchNorm1d-174                 [-1, 1024]           2,048\n","            ReLU-175                 [-1, 1024]               0\n","   DenseNormReLU-176                 [-1, 1024]               0\n","          Linear-177                  [-1, 128]         131,200\n","    EmbedNetwork-178                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 46.81\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 145.10\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NTZ4MoPGyGOp","colab_type":"code","colab":{}},"source":["train(net_1 = net, model_num = model_num)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rgipF2Cjyd6i","colab":{}},"source":["create_emb(dataset = X_query, fids = labels_query.fid, model_num = 1, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n","create_emb(dataset = X_test, fids = labels_test.fid, model_num = 1, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zdyulZZhyd6k","colab":{}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-VtubLKfnXP","colab_type":"text"},"source":["## anti-aliased models\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KDgaNN9Tfr0E","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMvV8y-qC33i","colab_type":"text"},"source":["### Model 6\n","model 4 but with better cutoff"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"24jsp6kzDF5u","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VSz6tZQOC_vA","colab":{}},"source":["model_num = 6"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XtSmmChXC_vF","colab":{}},"source":["filter_size = 3\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e7rwh247C_vI","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FBbdtvCrC_vL","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embedding = embedding\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embedding(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"1ae0531d-40aa-4571-8ce7-5eb5dcfe9637","executionInfo":{"status":"ok","timestamp":1574291491576,"user_tz":-60,"elapsed":519,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"jCfx6GOaC_vM","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net = nn.DataParallel(model)\n","summary(net, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157           [-1, 512, 10, 6]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","          Linear-187                 [-1, 1024]       2,098,176\n","     BatchNorm1d-188                 [-1, 1024]           2,048\n","            ReLU-189                 [-1, 1024]               0\n","   DenseNormReLU-190                 [-1, 1024]               0\n","          Linear-191                  [-1, 128]         131,200\n","           Model-192                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 54.99\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 153.27\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UD9VRVB6DdpH","colab_type":"code","outputId":"7661ba2d-ccd7-40db-f278-518241822dbe","executionInfo":{"status":"ok","timestamp":1574291496659,"user_tz":-60,"elapsed":529,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model_num"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PF3aFXuKC_vP","outputId":"79ab2caf-9ab7-48f0-cd5e-10ed623531ea","executionInfo":{"status":"ok","timestamp":1574290043298,"user_tz":-60,"elapsed":10480331,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(net = net, model_num = model_num)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["creating optimizer\n","start training ...\n","iter: 20, loss: 1.483976, lr: 0.000300, time: 9.350335\n","iter: 40, loss: 1.257860, lr: 0.000300, time: 7.966144\n","iter: 60, loss: 1.101345, lr: 0.000300, time: 8.704069\n","iter: 80, loss: 0.953883, lr: 0.000300, time: 7.978421\n","iter: 100, loss: 0.882383, lr: 0.000300, time: 8.690558\n","iter: 120, loss: 0.795802, lr: 0.000300, time: 7.976625\n","iter: 140, loss: 0.777704, lr: 0.000300, time: 8.725727\n","iter: 160, loss: 0.756218, lr: 0.000300, time: 7.983282\n","iter: 180, loss: 0.738109, lr: 0.000300, time: 8.759724\n","iter: 200, loss: 0.723922, lr: 0.000300, time: 8.003211\n","iter: 220, loss: 0.737764, lr: 0.000300, time: 8.589634\n","iter: 240, loss: 0.733623, lr: 0.000300, time: 7.997406\n","iter: 260, loss: 0.716943, lr: 0.000300, time: 8.688859\n","iter: 280, loss: 0.718820, lr: 0.000300, time: 8.004509\n","iter: 300, loss: 0.694791, lr: 0.000300, time: 8.710466\n","iter: 320, loss: 0.718917, lr: 0.000300, time: 8.005412\n","iter: 340, loss: 0.724665, lr: 0.000300, time: 8.683016\n","iter: 360, loss: 0.720671, lr: 0.000300, time: 8.008687\n","iter: 380, loss: 0.714178, lr: 0.000300, time: 8.711879\n","iter: 400, loss: 0.718457, lr: 0.000300, time: 8.008006\n","iter: 420, loss: 0.704837, lr: 0.000300, time: 8.591947\n","iter: 440, loss: 0.707716, lr: 0.000300, time: 8.008882\n","iter: 460, loss: 0.677738, lr: 0.000300, time: 8.527810\n","iter: 480, loss: 0.709929, lr: 0.000300, time: 8.016537\n","iter: 500, loss: 0.704272, lr: 0.000300, time: 8.701922\n","iter: 520, loss: 0.686999, lr: 0.000300, time: 8.016925\n","iter: 540, loss: 0.655229, lr: 0.000300, time: 8.675281\n","iter: 560, loss: 0.630451, lr: 0.000300, time: 8.009925\n","iter: 580, loss: 0.691773, lr: 0.000300, time: 8.707939\n","iter: 600, loss: 0.652494, lr: 0.000300, time: 8.011132\n","iter: 620, loss: 0.614731, lr: 0.000300, time: 8.651863\n","iter: 640, loss: 0.642227, lr: 0.000300, time: 8.028902\n","iter: 660, loss: 0.599445, lr: 0.000300, time: 8.699713\n","iter: 680, loss: 0.535287, lr: 0.000300, time: 8.006356\n","iter: 700, loss: 0.670862, lr: 0.000300, time: 8.721911\n","iter: 720, loss: 0.604889, lr: 0.000300, time: 8.018264\n","iter: 740, loss: 0.646655, lr: 0.000300, time: 8.708704\n","iter: 760, loss: 0.584132, lr: 0.000300, time: 8.006366\n","iter: 780, loss: 0.636904, lr: 0.000300, time: 8.703721\n","iter: 800, loss: 0.608852, lr: 0.000300, time: 8.020463\n","iter: 820, loss: 0.616523, lr: 0.000300, time: 8.714502\n","iter: 840, loss: 0.544306, lr: 0.000300, time: 8.022725\n","iter: 860, loss: 0.577836, lr: 0.000300, time: 7.979810\n","iter: 880, loss: 0.604752, lr: 0.000300, time: 8.768194\n","iter: 900, loss: 0.596273, lr: 0.000300, time: 7.971544\n","iter: 920, loss: 0.600087, lr: 0.000300, time: 8.771322\n","iter: 940, loss: 0.561604, lr: 0.000300, time: 7.981961\n","iter: 960, loss: 0.562390, lr: 0.000300, time: 8.712715\n","iter: 980, loss: 0.544797, lr: 0.000300, time: 7.986562\n","iter: 1000, loss: 0.543293, lr: 0.000300, time: 8.715862\n","iter: 1020, loss: 0.516287, lr: 0.000300, time: 7.987385\n","iter: 1040, loss: 0.518422, lr: 0.000300, time: 8.728167\n","iter: 1060, loss: 0.529782, lr: 0.000300, time: 8.005582\n","iter: 1080, loss: 0.539732, lr: 0.000300, time: 8.716561\n","iter: 1100, loss: 0.531603, lr: 0.000300, time: 8.013987\n","iter: 1120, loss: 0.586087, lr: 0.000300, time: 8.654072\n","iter: 1140, loss: 0.524305, lr: 0.000300, time: 7.998519\n","iter: 1160, loss: 0.534115, lr: 0.000300, time: 8.744603\n","iter: 1180, loss: 0.512896, lr: 0.000300, time: 8.014664\n","iter: 1200, loss: 0.429583, lr: 0.000300, time: 8.718253\n","iter: 1220, loss: 0.443687, lr: 0.000300, time: 8.023157\n","iter: 1240, loss: 0.474573, lr: 0.000300, time: 8.713156\n","iter: 1260, loss: 0.512230, lr: 0.000300, time: 8.024651\n","iter: 1280, loss: 0.451819, lr: 0.000300, time: 8.766042\n","iter: 1300, loss: 0.477660, lr: 0.000300, time: 8.013551\n","iter: 1320, loss: 0.481470, lr: 0.000300, time: 8.721028\n","iter: 1340, loss: 0.486891, lr: 0.000300, time: 8.022089\n","iter: 1360, loss: 0.487641, lr: 0.000300, time: 8.696590\n","iter: 1380, loss: 0.453737, lr: 0.000300, time: 8.018497\n","iter: 1400, loss: 0.449745, lr: 0.000300, time: 8.642226\n","iter: 1420, loss: 0.422242, lr: 0.000300, time: 8.021402\n","iter: 1440, loss: 0.445084, lr: 0.000300, time: 8.675465\n","iter: 1460, loss: 0.438686, lr: 0.000300, time: 8.030892\n","iter: 1480, loss: 0.411364, lr: 0.000300, time: 8.665438\n","iter: 1500, loss: 0.413134, lr: 0.000300, time: 8.013150\n","iter: 1520, loss: 0.495128, lr: 0.000300, time: 8.693144\n","iter: 1540, loss: 0.423002, lr: 0.000300, time: 8.015393\n","iter: 1560, loss: 0.401376, lr: 0.000300, time: 8.652637\n","iter: 1580, loss: 0.429792, lr: 0.000300, time: 7.999658\n","iter: 1600, loss: 0.458882, lr: 0.000300, time: 8.725777\n","iter: 1620, loss: 0.475653, lr: 0.000300, time: 8.016327\n","iter: 1640, loss: 0.463924, lr: 0.000300, time: 8.704968\n","iter: 1660, loss: 0.339302, lr: 0.000300, time: 8.018396\n","iter: 1680, loss: 0.465164, lr: 0.000300, time: 7.973790\n","iter: 1700, loss: 0.433970, lr: 0.000300, time: 8.766889\n","iter: 1720, loss: 0.398164, lr: 0.000300, time: 7.980490\n","iter: 1740, loss: 0.443700, lr: 0.000300, time: 8.720054\n","iter: 1760, loss: 0.391733, lr: 0.000300, time: 7.982829\n","iter: 1780, loss: 0.359120, lr: 0.000300, time: 8.742515\n","iter: 1800, loss: 0.388930, lr: 0.000300, time: 7.986730\n","iter: 1820, loss: 0.408138, lr: 0.000300, time: 8.725256\n","iter: 1840, loss: 0.326557, lr: 0.000300, time: 7.995521\n","iter: 1860, loss: 0.426909, lr: 0.000300, time: 8.721045\n","iter: 1880, loss: 0.357542, lr: 0.000300, time: 7.998641\n","iter: 1900, loss: 0.408903, lr: 0.000300, time: 8.671819\n","iter: 1920, loss: 0.373250, lr: 0.000300, time: 8.009913\n","iter: 1940, loss: 0.370835, lr: 0.000300, time: 8.718204\n","iter: 1960, loss: 0.377145, lr: 0.000300, time: 8.005822\n","iter: 1980, loss: 0.373017, lr: 0.000300, time: 8.717858\n","iter: 2000, loss: 0.419784, lr: 0.000300, time: 8.014071\n","iter: 2020, loss: 0.405254, lr: 0.000300, time: 8.717334\n","iter: 2040, loss: 0.375816, lr: 0.000300, time: 8.031416\n","iter: 2060, loss: 0.392638, lr: 0.000300, time: 8.774913\n","iter: 2080, loss: 0.311594, lr: 0.000300, time: 8.016813\n","iter: 2100, loss: 0.344459, lr: 0.000300, time: 8.687275\n","iter: 2120, loss: 0.336248, lr: 0.000300, time: 8.015163\n","iter: 2140, loss: 0.313089, lr: 0.000300, time: 8.685928\n","iter: 2160, loss: 0.372133, lr: 0.000300, time: 8.017130\n","iter: 2180, loss: 0.330333, lr: 0.000300, time: 8.712834\n","iter: 2200, loss: 0.295704, lr: 0.000300, time: 8.030551\n","iter: 2220, loss: 0.367789, lr: 0.000300, time: 8.738122\n","iter: 2240, loss: 0.420141, lr: 0.000300, time: 8.016329\n","iter: 2260, loss: 0.419399, lr: 0.000300, time: 8.729958\n","iter: 2280, loss: 0.337841, lr: 0.000300, time: 8.025475\n","iter: 2300, loss: 0.367216, lr: 0.000300, time: 8.740185\n","iter: 2320, loss: 0.345219, lr: 0.000300, time: 8.017061\n","iter: 2340, loss: 0.318007, lr: 0.000300, time: 8.735682\n","iter: 2360, loss: 0.309313, lr: 0.000300, time: 8.014876\n","iter: 2380, loss: 0.276288, lr: 0.000300, time: 8.655444\n","iter: 2400, loss: 0.329382, lr: 0.000300, time: 8.020019\n","iter: 2420, loss: 0.251148, lr: 0.000300, time: 8.692641\n","iter: 2440, loss: 0.246696, lr: 0.000300, time: 8.024622\n","iter: 2460, loss: 0.282459, lr: 0.000300, time: 8.713605\n","iter: 2480, loss: 0.250028, lr: 0.000300, time: 8.018691\n","iter: 2500, loss: 0.276315, lr: 0.000300, time: 7.977024\n","iter: 2520, loss: 0.299800, lr: 0.000300, time: 8.801750\n","iter: 2540, loss: 0.316801, lr: 0.000300, time: 7.976454\n","iter: 2560, loss: 0.264536, lr: 0.000300, time: 8.664326\n","iter: 2580, loss: 0.278044, lr: 0.000300, time: 7.986651\n","iter: 2600, loss: 0.287042, lr: 0.000300, time: 8.643857\n","iter: 2620, loss: 0.289600, lr: 0.000300, time: 7.997638\n","iter: 2640, loss: 0.321196, lr: 0.000300, time: 8.753709\n","iter: 2660, loss: 0.283836, lr: 0.000300, time: 7.998994\n","iter: 2680, loss: 0.260596, lr: 0.000300, time: 8.763745\n","iter: 2700, loss: 0.289980, lr: 0.000300, time: 8.012569\n","iter: 2720, loss: 0.311336, lr: 0.000300, time: 8.626124\n","iter: 2740, loss: 0.291682, lr: 0.000300, time: 8.000333\n","iter: 2760, loss: 0.316324, lr: 0.000300, time: 8.761666\n","iter: 2780, loss: 0.312425, lr: 0.000300, time: 8.019639\n","iter: 2800, loss: 0.317676, lr: 0.000300, time: 8.749444\n","iter: 2820, loss: 0.346933, lr: 0.000300, time: 8.026092\n","iter: 2840, loss: 0.288776, lr: 0.000300, time: 8.747291\n","iter: 2860, loss: 0.289344, lr: 0.000300, time: 8.025878\n","iter: 2880, loss: 0.244712, lr: 0.000300, time: 8.747311\n","iter: 2900, loss: 0.304177, lr: 0.000300, time: 8.018230\n","iter: 2920, loss: 0.286122, lr: 0.000300, time: 8.621270\n","iter: 2940, loss: 0.310464, lr: 0.000300, time: 8.037517\n","iter: 2960, loss: 0.363824, lr: 0.000300, time: 8.727679\n","iter: 2980, loss: 0.268122, lr: 0.000300, time: 8.008564\n","iter: 3000, loss: 0.314993, lr: 0.000300, time: 8.739640\n","iter: 3020, loss: 0.292180, lr: 0.000300, time: 8.022045\n","iter: 3040, loss: 0.264906, lr: 0.000300, time: 8.709138\n","iter: 3060, loss: 0.273059, lr: 0.000300, time: 8.016989\n","iter: 3080, loss: 0.297893, lr: 0.000300, time: 8.744355\n","iter: 3100, loss: 0.311690, lr: 0.000300, time: 8.020683\n","iter: 3120, loss: 0.348278, lr: 0.000300, time: 8.676557\n","iter: 3140, loss: 0.250545, lr: 0.000300, time: 8.014359\n","iter: 3160, loss: 0.278025, lr: 0.000300, time: 8.729916\n","iter: 3180, loss: 0.262954, lr: 0.000300, time: 8.014756\n","iter: 3200, loss: 0.319492, lr: 0.000300, time: 8.722311\n","iter: 3220, loss: 0.283971, lr: 0.000300, time: 8.014352\n","iter: 3240, loss: 0.287201, lr: 0.000300, time: 8.725828\n","iter: 3260, loss: 0.266608, lr: 0.000300, time: 8.021455\n","iter: 3280, loss: 0.264365, lr: 0.000300, time: 8.724777\n","iter: 3300, loss: 0.255500, lr: 0.000300, time: 8.014917\n","iter: 3320, loss: 0.242269, lr: 0.000300, time: 7.976296\n","iter: 3340, loss: 0.246788, lr: 0.000300, time: 8.747891\n","iter: 3360, loss: 0.231381, lr: 0.000300, time: 7.986374\n","iter: 3380, loss: 0.268323, lr: 0.000300, time: 8.712546\n","iter: 3400, loss: 0.286356, lr: 0.000300, time: 7.984112\n","iter: 3420, loss: 0.250800, lr: 0.000300, time: 8.765973\n","iter: 3440, loss: 0.266287, lr: 0.000300, time: 7.997155\n","iter: 3460, loss: 0.214563, lr: 0.000300, time: 8.733907\n","iter: 3480, loss: 0.239166, lr: 0.000300, time: 8.000483\n","iter: 3500, loss: 0.257406, lr: 0.000300, time: 8.777312\n","iter: 3520, loss: 0.241816, lr: 0.000300, time: 8.007824\n","iter: 3540, loss: 0.248940, lr: 0.000300, time: 8.736030\n","iter: 3560, loss: 0.206614, lr: 0.000300, time: 8.013381\n","iter: 3580, loss: 0.254193, lr: 0.000300, time: 8.681959\n","iter: 3600, loss: 0.250323, lr: 0.000300, time: 8.026525\n","iter: 3620, loss: 0.280117, lr: 0.000300, time: 8.690071\n","iter: 3640, loss: 0.254955, lr: 0.000300, time: 8.040423\n","iter: 3660, loss: 0.304248, lr: 0.000300, time: 8.648121\n","iter: 3680, loss: 0.296915, lr: 0.000300, time: 8.023337\n","iter: 3700, loss: 0.248889, lr: 0.000300, time: 8.744862\n","iter: 3720, loss: 0.352593, lr: 0.000300, time: 8.020232\n","iter: 3740, loss: 0.261576, lr: 0.000300, time: 8.745004\n","iter: 3760, loss: 0.233608, lr: 0.000300, time: 8.003528\n","iter: 3780, loss: 0.271999, lr: 0.000300, time: 8.707181\n","iter: 3800, loss: 0.234801, lr: 0.000300, time: 8.017110\n","iter: 3820, loss: 0.215187, lr: 0.000300, time: 8.697700\n","iter: 3840, loss: 0.210130, lr: 0.000300, time: 8.023800\n","iter: 3860, loss: 0.231524, lr: 0.000300, time: 8.680896\n","iter: 3880, loss: 0.264507, lr: 0.000300, time: 8.028413\n","iter: 3900, loss: 0.224460, lr: 0.000300, time: 8.652448\n","iter: 3920, loss: 0.200394, lr: 0.000300, time: 8.000786\n","iter: 3940, loss: 0.218079, lr: 0.000300, time: 8.757204\n","iter: 3960, loss: 0.209762, lr: 0.000300, time: 8.031245\n","iter: 3980, loss: 0.187652, lr: 0.000300, time: 8.728367\n","iter: 4000, loss: 0.203833, lr: 0.000300, time: 8.009143\n","iter: 4020, loss: 0.204412, lr: 0.000300, time: 8.737662\n","iter: 4040, loss: 0.238939, lr: 0.000300, time: 8.030048\n","iter: 4060, loss: 0.217011, lr: 0.000300, time: 8.715573\n","iter: 4080, loss: 0.189315, lr: 0.000300, time: 8.012675\n","iter: 4100, loss: 0.230276, lr: 0.000300, time: 8.729848\n","iter: 4120, loss: 0.244077, lr: 0.000300, time: 8.020106\n","iter: 4140, loss: 0.200711, lr: 0.000300, time: 7.976038\n","iter: 4160, loss: 0.195502, lr: 0.000300, time: 8.781039\n","iter: 4180, loss: 0.216047, lr: 0.000300, time: 7.955309\n","iter: 4200, loss: 0.229744, lr: 0.000300, time: 8.703607\n","iter: 4220, loss: 0.257104, lr: 0.000300, time: 7.965090\n","iter: 4240, loss: 0.200667, lr: 0.000300, time: 8.729496\n","iter: 4260, loss: 0.266016, lr: 0.000300, time: 7.973266\n","iter: 4280, loss: 0.215744, lr: 0.000300, time: 8.694323\n","iter: 4300, loss: 0.201787, lr: 0.000300, time: 7.965430\n","iter: 4320, loss: 0.223409, lr: 0.000300, time: 8.620921\n","iter: 4340, loss: 0.257405, lr: 0.000300, time: 8.001341\n","iter: 4360, loss: 0.225421, lr: 0.000300, time: 8.686195\n","iter: 4380, loss: 0.250764, lr: 0.000300, time: 7.994691\n","iter: 4400, loss: 0.219638, lr: 0.000300, time: 8.706708\n","iter: 4420, loss: 0.193439, lr: 0.000300, time: 7.998876\n","iter: 4440, loss: 0.204312, lr: 0.000300, time: 8.710543\n","iter: 4460, loss: 0.193258, lr: 0.000300, time: 8.031799\n","iter: 4480, loss: 0.245779, lr: 0.000300, time: 8.581548\n","iter: 4500, loss: 0.231868, lr: 0.000300, time: 8.008722\n","iter: 4520, loss: 0.179790, lr: 0.000300, time: 8.703376\n","iter: 4540, loss: 0.159501, lr: 0.000300, time: 8.021312\n","iter: 4560, loss: 0.176010, lr: 0.000300, time: 8.686259\n","iter: 4580, loss: 0.201621, lr: 0.000300, time: 8.023092\n","iter: 4600, loss: 0.242562, lr: 0.000300, time: 8.730006\n","iter: 4620, loss: 0.185332, lr: 0.000300, time: 8.019968\n","iter: 4640, loss: 0.177998, lr: 0.000300, time: 8.747012\n","iter: 4660, loss: 0.204582, lr: 0.000300, time: 8.027745\n","iter: 4680, loss: 0.189843, lr: 0.000300, time: 8.760848\n","iter: 4700, loss: 0.198951, lr: 0.000300, time: 8.019197\n","iter: 4720, loss: 0.218990, lr: 0.000300, time: 8.723817\n","iter: 4740, loss: 0.206323, lr: 0.000300, time: 8.033174\n","iter: 4760, loss: 0.213073, lr: 0.000300, time: 8.569344\n","iter: 4780, loss: 0.295797, lr: 0.000300, time: 8.021644\n","iter: 4800, loss: 0.187631, lr: 0.000300, time: 8.734107\n","iter: 4820, loss: 0.249844, lr: 0.000300, time: 8.033681\n","iter: 4840, loss: 0.218178, lr: 0.000300, time: 8.746536\n","iter: 4860, loss: 0.248780, lr: 0.000300, time: 8.016341\n","iter: 4880, loss: 0.170492, lr: 0.000300, time: 8.703526\n","iter: 4900, loss: 0.193821, lr: 0.000300, time: 8.018475\n","iter: 4920, loss: 0.222079, lr: 0.000300, time: 8.597480\n","iter: 4940, loss: 0.202907, lr: 0.000300, time: 8.074741\n","iter: 4960, loss: 0.223445, lr: 0.000300, time: 7.994933\n","iter: 4980, loss: 0.248304, lr: 0.000300, time: 8.747352\n","iter: 5000, loss: 0.205866, lr: 0.000300, time: 8.001843\n","iter: 5020, loss: 0.188248, lr: 0.000300, time: 8.740218\n","iter: 5040, loss: 0.153790, lr: 0.000300, time: 8.007968\n","iter: 5060, loss: 0.196916, lr: 0.000300, time: 8.734401\n","iter: 5080, loss: 0.163967, lr: 0.000300, time: 8.005275\n","iter: 5100, loss: 0.203723, lr: 0.000300, time: 8.651727\n","iter: 5120, loss: 0.276429, lr: 0.000300, time: 8.007662\n","iter: 5140, loss: 0.223394, lr: 0.000300, time: 8.709242\n","iter: 5160, loss: 0.198447, lr: 0.000300, time: 8.009841\n","iter: 5180, loss: 0.253322, lr: 0.000300, time: 8.760207\n","iter: 5200, loss: 0.175048, lr: 0.000300, time: 8.015671\n","iter: 5220, loss: 0.203089, lr: 0.000300, time: 8.754667\n","iter: 5240, loss: 0.198870, lr: 0.000300, time: 8.021835\n","iter: 5260, loss: 0.181120, lr: 0.000300, time: 8.716084\n","iter: 5280, loss: 0.166918, lr: 0.000300, time: 8.013304\n","iter: 5300, loss: 0.204650, lr: 0.000300, time: 8.783680\n","iter: 5320, loss: 0.164662, lr: 0.000300, time: 8.028164\n","iter: 5340, loss: 0.179562, lr: 0.000300, time: 8.716681\n","iter: 5360, loss: 0.235650, lr: 0.000300, time: 8.037244\n","iter: 5380, loss: 0.148448, lr: 0.000300, time: 8.751495\n","iter: 5400, loss: 0.187042, lr: 0.000300, time: 8.033194\n","iter: 5420, loss: 0.221579, lr: 0.000300, time: 8.703122\n","iter: 5440, loss: 0.183309, lr: 0.000300, time: 8.031666\n","iter: 5460, loss: 0.186827, lr: 0.000300, time: 8.712776\n","iter: 5480, loss: 0.136512, lr: 0.000300, time: 8.024210\n","iter: 5500, loss: 0.173999, lr: 0.000300, time: 8.753371\n","iter: 5520, loss: 0.216345, lr: 0.000300, time: 8.042695\n","iter: 5540, loss: 0.158445, lr: 0.000300, time: 8.747711\n","iter: 5560, loss: 0.235562, lr: 0.000300, time: 8.024086\n","iter: 5580, loss: 0.187566, lr: 0.000300, time: 8.765417\n","iter: 5600, loss: 0.195875, lr: 0.000300, time: 8.032073\n","iter: 5620, loss: 0.201466, lr: 0.000300, time: 8.679310\n","iter: 5640, loss: 0.181746, lr: 0.000300, time: 8.022640\n","iter: 5660, loss: 0.179654, lr: 0.000300, time: 8.700770\n","iter: 5680, loss: 0.151542, lr: 0.000300, time: 8.035598\n","iter: 5700, loss: 0.184414, lr: 0.000300, time: 8.731097\n","iter: 5720, loss: 0.174415, lr: 0.000300, time: 8.030185\n","iter: 5740, loss: 0.158408, lr: 0.000300, time: 8.734783\n","iter: 5760, loss: 0.150757, lr: 0.000300, time: 8.027700\n","iter: 5780, loss: 0.146022, lr: 0.000300, time: 7.991974\n","iter: 5800, loss: 0.152411, lr: 0.000300, time: 8.812716\n","iter: 5820, loss: 0.176285, lr: 0.000300, time: 7.979568\n","iter: 5840, loss: 0.126838, lr: 0.000300, time: 8.710588\n","iter: 5860, loss: 0.161295, lr: 0.000300, time: 7.991154\n","iter: 5880, loss: 0.189616, lr: 0.000300, time: 8.764156\n","iter: 5900, loss: 0.213943, lr: 0.000300, time: 8.011190\n","iter: 5920, loss: 0.178375, lr: 0.000300, time: 8.740344\n","iter: 5940, loss: 0.256524, lr: 0.000300, time: 8.014102\n","iter: 5960, loss: 0.160477, lr: 0.000300, time: 8.785600\n","iter: 5980, loss: 0.171998, lr: 0.000300, time: 8.012993\n","iter: 6000, loss: 0.160153, lr: 0.000300, time: 8.716368\n","iter: 6020, loss: 0.174768, lr: 0.000300, time: 8.008266\n","iter: 6040, loss: 0.193534, lr: 0.000300, time: 8.736866\n","iter: 6060, loss: 0.174886, lr: 0.000300, time: 8.026441\n","iter: 6080, loss: 0.155177, lr: 0.000300, time: 8.593159\n","iter: 6100, loss: 0.204431, lr: 0.000300, time: 8.023096\n","iter: 6120, loss: 0.182465, lr: 0.000300, time: 8.724628\n","iter: 6140, loss: 0.191099, lr: 0.000300, time: 8.031096\n","iter: 6160, loss: 0.200776, lr: 0.000300, time: 8.745363\n","iter: 6180, loss: 0.170334, lr: 0.000300, time: 8.017656\n","iter: 6200, loss: 0.203023, lr: 0.000300, time: 8.751763\n","iter: 6220, loss: 0.146890, lr: 0.000300, time: 8.025659\n","iter: 6240, loss: 0.180132, lr: 0.000300, time: 8.732148\n","iter: 6260, loss: 0.145755, lr: 0.000300, time: 8.015581\n","iter: 6280, loss: 0.151341, lr: 0.000300, time: 8.706501\n","iter: 6300, loss: 0.180384, lr: 0.000300, time: 8.033947\n","iter: 6320, loss: 0.169659, lr: 0.000300, time: 8.752524\n","iter: 6340, loss: 0.114451, lr: 0.000300, time: 8.030675\n","iter: 6360, loss: 0.101450, lr: 0.000300, time: 8.751091\n","iter: 6380, loss: 0.216006, lr: 0.000300, time: 8.027043\n","iter: 6400, loss: 0.168877, lr: 0.000300, time: 8.733533\n","iter: 6420, loss: 0.150142, lr: 0.000300, time: 8.026508\n","iter: 6440, loss: 0.164302, lr: 0.000300, time: 8.737692\n","iter: 6460, loss: 0.171174, lr: 0.000300, time: 8.044303\n","iter: 6480, loss: 0.134876, lr: 0.000300, time: 8.750030\n","iter: 6500, loss: 0.167616, lr: 0.000300, time: 8.046616\n","iter: 6520, loss: 0.216932, lr: 0.000300, time: 8.677273\n","iter: 6540, loss: 0.152058, lr: 0.000300, time: 8.041028\n","iter: 6560, loss: 0.169272, lr: 0.000300, time: 8.732690\n","iter: 6580, loss: 0.171415, lr: 0.000300, time: 8.061573\n","iter: 6600, loss: 0.209489, lr: 0.000300, time: 7.982495\n","iter: 6620, loss: 0.182639, lr: 0.000300, time: 8.701933\n","iter: 6640, loss: 0.177628, lr: 0.000300, time: 7.992404\n","iter: 6660, loss: 0.156566, lr: 0.000300, time: 8.761489\n","iter: 6680, loss: 0.177401, lr: 0.000300, time: 7.993965\n","iter: 6700, loss: 0.132567, lr: 0.000300, time: 8.765468\n","iter: 6720, loss: 0.183311, lr: 0.000300, time: 7.992882\n","iter: 6740, loss: 0.150440, lr: 0.000300, time: 8.792848\n","iter: 6760, loss: 0.169339, lr: 0.000300, time: 8.013903\n","iter: 6780, loss: 0.142437, lr: 0.000300, time: 8.764222\n","iter: 6800, loss: 0.169257, lr: 0.000300, time: 8.007170\n","iter: 6820, loss: 0.200304, lr: 0.000300, time: 8.704166\n","iter: 6840, loss: 0.154848, lr: 0.000300, time: 8.012836\n","iter: 6860, loss: 0.159748, lr: 0.000300, time: 8.701068\n","iter: 6880, loss: 0.150134, lr: 0.000300, time: 8.036673\n","iter: 6900, loss: 0.126181, lr: 0.000300, time: 8.771365\n","iter: 6920, loss: 0.205683, lr: 0.000300, time: 8.027019\n","iter: 6940, loss: 0.179181, lr: 0.000300, time: 8.760530\n","iter: 6960, loss: 0.150741, lr: 0.000300, time: 8.028444\n","iter: 6980, loss: 0.179945, lr: 0.000300, time: 8.722030\n","iter: 7000, loss: 0.142104, lr: 0.000300, time: 8.026283\n","iter: 7020, loss: 0.137994, lr: 0.000300, time: 8.768619\n","iter: 7040, loss: 0.192091, lr: 0.000300, time: 8.047190\n","iter: 7060, loss: 0.155962, lr: 0.000300, time: 8.716463\n","iter: 7080, loss: 0.144009, lr: 0.000300, time: 8.021786\n","iter: 7100, loss: 0.152417, lr: 0.000300, time: 8.740381\n","iter: 7120, loss: 0.188874, lr: 0.000300, time: 8.043291\n","iter: 7140, loss: 0.232770, lr: 0.000300, time: 8.614417\n","iter: 7160, loss: 0.165443, lr: 0.000300, time: 8.038831\n","iter: 7180, loss: 0.146378, lr: 0.000300, time: 8.785935\n","iter: 7200, loss: 0.135800, lr: 0.000300, time: 8.023219\n","iter: 7220, loss: 0.115961, lr: 0.000300, time: 8.802515\n","iter: 7240, loss: 0.127173, lr: 0.000300, time: 8.042857\n","iter: 7260, loss: 0.169875, lr: 0.000300, time: 8.754316\n","iter: 7280, loss: 0.118586, lr: 0.000300, time: 8.031438\n","iter: 7300, loss: 0.135361, lr: 0.000300, time: 8.726833\n","iter: 7320, loss: 0.119700, lr: 0.000300, time: 8.035833\n","iter: 7340, loss: 0.145449, lr: 0.000300, time: 8.736766\n","iter: 7360, loss: 0.162946, lr: 0.000300, time: 8.026047\n","iter: 7380, loss: 0.129196, lr: 0.000300, time: 8.658216\n","iter: 7400, loss: 0.215593, lr: 0.000300, time: 8.047814\n","iter: 7420, loss: 0.179320, lr: 0.000300, time: 7.980492\n","iter: 7440, loss: 0.143100, lr: 0.000300, time: 8.814124\n","iter: 7460, loss: 0.152516, lr: 0.000300, time: 7.998294\n","iter: 7480, loss: 0.142407, lr: 0.000300, time: 8.797137\n","iter: 7500, loss: 0.198716, lr: 0.000300, time: 8.019424\n","iter: 7520, loss: 0.143763, lr: 0.000300, time: 8.776963\n","iter: 7540, loss: 0.131649, lr: 0.000300, time: 7.995126\n","iter: 7560, loss: 0.229094, lr: 0.000300, time: 8.763239\n","iter: 7580, loss: 0.180605, lr: 0.000300, time: 8.008164\n","iter: 7600, loss: 0.173865, lr: 0.000300, time: 8.748445\n","iter: 7620, loss: 0.147930, lr: 0.000300, time: 8.004586\n","iter: 7640, loss: 0.220874, lr: 0.000300, time: 8.699405\n","iter: 7660, loss: 0.223325, lr: 0.000300, time: 8.027743\n","iter: 7680, loss: 0.169041, lr: 0.000300, time: 8.780805\n","iter: 7700, loss: 0.166159, lr: 0.000300, time: 8.028785\n","iter: 7720, loss: 0.204230, lr: 0.000300, time: 8.753671\n","iter: 7740, loss: 0.129799, lr: 0.000300, time: 8.030081\n","iter: 7760, loss: 0.141692, lr: 0.000300, time: 8.776583\n","iter: 7780, loss: 0.192594, lr: 0.000300, time: 8.030774\n","iter: 7800, loss: 0.156069, lr: 0.000300, time: 8.760503\n","iter: 7820, loss: 0.164960, lr: 0.000300, time: 8.018279\n","iter: 7840, loss: 0.174157, lr: 0.000300, time: 8.748590\n","iter: 7860, loss: 0.167751, lr: 0.000300, time: 8.038207\n","iter: 7880, loss: 0.159456, lr: 0.000300, time: 8.791893\n","iter: 7900, loss: 0.144885, lr: 0.000300, time: 8.039478\n","iter: 7920, loss: 0.107248, lr: 0.000300, time: 8.670351\n","iter: 7940, loss: 0.197321, lr: 0.000300, time: 8.035012\n","iter: 7960, loss: 0.170597, lr: 0.000300, time: 8.753531\n","iter: 7980, loss: 0.149586, lr: 0.000300, time: 8.017380\n","iter: 8000, loss: 0.154482, lr: 0.000300, time: 8.739989\n","iter: 8020, loss: 0.147573, lr: 0.000300, time: 8.037511\n","iter: 8040, loss: 0.145813, lr: 0.000300, time: 8.738827\n","iter: 8060, loss: 0.135818, lr: 0.000300, time: 8.029864\n","iter: 8080, loss: 0.192881, lr: 0.000300, time: 8.822023\n","iter: 8100, loss: 0.162567, lr: 0.000300, time: 8.025827\n","iter: 8120, loss: 0.191472, lr: 0.000300, time: 8.748827\n","iter: 8140, loss: 0.138857, lr: 0.000300, time: 8.037976\n","iter: 8160, loss: 0.113728, lr: 0.000300, time: 8.777307\n","iter: 8180, loss: 0.147257, lr: 0.000300, time: 8.030351\n","iter: 8200, loss: 0.137067, lr: 0.000300, time: 8.739720\n","iter: 8220, loss: 0.128772, lr: 0.000300, time: 8.035221\n","iter: 8240, loss: 0.158328, lr: 0.000300, time: 7.991455\n","iter: 8260, loss: 0.159806, lr: 0.000300, time: 8.769038\n","iter: 8280, loss: 0.148846, lr: 0.000300, time: 7.998754\n","iter: 8300, loss: 0.104531, lr: 0.000300, time: 8.781727\n","iter: 8320, loss: 0.108051, lr: 0.000300, time: 8.002299\n","iter: 8340, loss: 0.116580, lr: 0.000300, time: 8.668504\n","iter: 8360, loss: 0.098886, lr: 0.000300, time: 8.009909\n","iter: 8380, loss: 0.126459, lr: 0.000300, time: 8.727360\n","iter: 8400, loss: 0.118815, lr: 0.000300, time: 8.007359\n","iter: 8420, loss: 0.149185, lr: 0.000300, time: 8.796604\n","iter: 8440, loss: 0.159318, lr: 0.000300, time: 8.015449\n","iter: 8460, loss: 0.100804, lr: 0.000300, time: 8.730974\n","iter: 8480, loss: 0.166158, lr: 0.000300, time: 8.034690\n","iter: 8500, loss: 0.143356, lr: 0.000300, time: 8.765971\n","iter: 8520, loss: 0.107991, lr: 0.000300, time: 8.020889\n","iter: 8540, loss: 0.104957, lr: 0.000300, time: 8.722179\n","iter: 8560, loss: 0.119571, lr: 0.000300, time: 8.038925\n","iter: 8580, loss: 0.093825, lr: 0.000300, time: 8.767284\n","iter: 8600, loss: 0.123827, lr: 0.000300, time: 8.029228\n","iter: 8620, loss: 0.090708, lr: 0.000300, time: 8.691103\n","iter: 8640, loss: 0.136118, lr: 0.000300, time: 8.035690\n","iter: 8660, loss: 0.117446, lr: 0.000300, time: 8.782299\n","iter: 8680, loss: 0.101809, lr: 0.000300, time: 8.023018\n","iter: 8700, loss: 0.162019, lr: 0.000300, time: 8.789711\n","iter: 8720, loss: 0.130891, lr: 0.000300, time: 8.023815\n","iter: 8740, loss: 0.122623, lr: 0.000300, time: 8.703072\n","iter: 8760, loss: 0.146600, lr: 0.000300, time: 8.014023\n","iter: 8780, loss: 0.165884, lr: 0.000300, time: 8.709270\n","iter: 8800, loss: 0.132793, lr: 0.000300, time: 8.001712\n","iter: 8820, loss: 0.162451, lr: 0.000300, time: 8.702361\n","iter: 8840, loss: 0.124267, lr: 0.000300, time: 7.997159\n","iter: 8860, loss: 0.106567, lr: 0.000300, time: 8.733327\n","iter: 8880, loss: 0.128627, lr: 0.000300, time: 8.042296\n","iter: 8900, loss: 0.136736, lr: 0.000300, time: 8.739395\n","iter: 8920, loss: 0.116828, lr: 0.000300, time: 8.040445\n","iter: 8940, loss: 0.146562, lr: 0.000300, time: 8.728638\n","iter: 8960, loss: 0.130668, lr: 0.000300, time: 8.028968\n","iter: 8980, loss: 0.125172, lr: 0.000300, time: 8.769516\n","iter: 9000, loss: 0.149876, lr: 0.000300, time: 8.025223\n","iter: 9020, loss: 0.183439, lr: 0.000300, time: 8.682544\n","iter: 9040, loss: 0.184252, lr: 0.000300, time: 8.042517\n","iter: 9060, loss: 0.146712, lr: 0.000300, time: 7.985179\n","iter: 9080, loss: 0.182163, lr: 0.000300, time: 8.841089\n","iter: 9100, loss: 0.168480, lr: 0.000300, time: 7.999948\n","iter: 9120, loss: 0.158827, lr: 0.000300, time: 8.775230\n","iter: 9140, loss: 0.112286, lr: 0.000300, time: 7.996038\n","iter: 9160, loss: 0.102544, lr: 0.000300, time: 8.798129\n","iter: 9180, loss: 0.149243, lr: 0.000300, time: 8.009629\n","iter: 9200, loss: 0.099751, lr: 0.000300, time: 8.769391\n","iter: 9220, loss: 0.131851, lr: 0.000300, time: 8.017067\n","iter: 9240, loss: 0.115956, lr: 0.000300, time: 8.798627\n","iter: 9260, loss: 0.098342, lr: 0.000300, time: 8.015800\n","iter: 9280, loss: 0.102117, lr: 0.000300, time: 8.782855\n","iter: 9300, loss: 0.148990, lr: 0.000300, time: 8.018932\n","iter: 9320, loss: 0.128304, lr: 0.000300, time: 8.809512\n","iter: 9340, loss: 0.109658, lr: 0.000300, time: 8.029399\n","iter: 9360, loss: 0.111099, lr: 0.000300, time: 8.770183\n","iter: 9380, loss: 0.086692, lr: 0.000300, time: 8.037815\n","iter: 9400, loss: 0.079099, lr: 0.000300, time: 8.783958\n","iter: 9420, loss: 0.115090, lr: 0.000300, time: 8.033088\n","iter: 9440, loss: 0.116908, lr: 0.000300, time: 8.783635\n","iter: 9460, loss: 0.191551, lr: 0.000300, time: 8.039112\n","iter: 9480, loss: 0.146472, lr: 0.000300, time: 8.740513\n","iter: 9500, loss: 0.134375, lr: 0.000300, time: 8.020103\n","iter: 9520, loss: 0.100997, lr: 0.000300, time: 8.768422\n","iter: 9540, loss: 0.161763, lr: 0.000300, time: 8.036267\n","iter: 9560, loss: 0.131098, lr: 0.000300, time: 8.711974\n","iter: 9580, loss: 0.169485, lr: 0.000300, time: 8.041420\n","iter: 9600, loss: 0.154086, lr: 0.000300, time: 8.761645\n","iter: 9620, loss: 0.144710, lr: 0.000300, time: 8.041640\n","iter: 9640, loss: 0.104732, lr: 0.000300, time: 8.748019\n","iter: 9660, loss: 0.137882, lr: 0.000300, time: 8.021067\n","iter: 9680, loss: 0.159203, lr: 0.000300, time: 8.777716\n","iter: 9700, loss: 0.154458, lr: 0.000300, time: 8.035800\n","iter: 9720, loss: 0.134763, lr: 0.000300, time: 8.626609\n","iter: 9740, loss: 0.118360, lr: 0.000300, time: 8.037813\n","iter: 9760, loss: 0.159608, lr: 0.000300, time: 8.750669\n","iter: 9780, loss: 0.134790, lr: 0.000300, time: 8.035957\n","iter: 9800, loss: 0.155661, lr: 0.000300, time: 8.762440\n","iter: 9820, loss: 0.120197, lr: 0.000300, time: 8.035691\n","iter: 9840, loss: 0.101309, lr: 0.000300, time: 8.713399\n","iter: 9860, loss: 0.087621, lr: 0.000300, time: 8.075787\n","iter: 9880, loss: 0.135276, lr: 0.000300, time: 7.996789\n","iter: 9900, loss: 0.140179, lr: 0.000300, time: 8.701325\n","iter: 9920, loss: 0.132459, lr: 0.000300, time: 7.996534\n","iter: 9940, loss: 0.100416, lr: 0.000300, time: 8.642483\n","iter: 9960, loss: 0.084983, lr: 0.000300, time: 8.009509\n","iter: 9980, loss: 0.145084, lr: 0.000300, time: 8.791130\n","iter: 10000, loss: 0.124654, lr: 0.000300, time: 8.010841\n","iter: 10020, loss: 0.090863, lr: 0.000300, time: 8.684124\n","iter: 10040, loss: 0.171226, lr: 0.000300, time: 8.012623\n","iter: 10060, loss: 0.124875, lr: 0.000300, time: 8.798090\n","iter: 10080, loss: 0.118673, lr: 0.000300, time: 8.016766\n","iter: 10100, loss: 0.099962, lr: 0.000300, time: 8.735814\n","iter: 10120, loss: 0.133868, lr: 0.000300, time: 8.029878\n","iter: 10140, loss: 0.172424, lr: 0.000300, time: 8.791170\n","iter: 10160, loss: 0.155595, lr: 0.000300, time: 8.020547\n","iter: 10180, loss: 0.158762, lr: 0.000300, time: 8.753738\n","iter: 10200, loss: 0.126526, lr: 0.000300, time: 8.037659\n","iter: 10220, loss: 0.101899, lr: 0.000300, time: 8.764562\n","iter: 10240, loss: 0.144879, lr: 0.000300, time: 8.038011\n","iter: 10260, loss: 0.135936, lr: 0.000300, time: 8.751147\n","iter: 10280, loss: 0.132043, lr: 0.000300, time: 8.028845\n","iter: 10300, loss: 0.151867, lr: 0.000300, time: 8.770813\n","iter: 10320, loss: 0.138334, lr: 0.000300, time: 8.035416\n","iter: 10340, loss: 0.145976, lr: 0.000300, time: 8.742129\n","iter: 10360, loss: 0.099801, lr: 0.000300, time: 8.046048\n","iter: 10380, loss: 0.126671, lr: 0.000300, time: 8.791316\n","iter: 10400, loss: 0.111618, lr: 0.000300, time: 8.041884\n","iter: 10420, loss: 0.159882, lr: 0.000300, time: 8.748267\n","iter: 10440, loss: 0.114834, lr: 0.000300, time: 8.031542\n","iter: 10460, loss: 0.085622, lr: 0.000300, time: 8.766477\n","iter: 10480, loss: 0.118134, lr: 0.000300, time: 8.042113\n","iter: 10500, loss: 0.157492, lr: 0.000300, time: 8.719745\n","iter: 10520, loss: 0.104389, lr: 0.000300, time: 8.028004\n","iter: 10540, loss: 0.140154, lr: 0.000300, time: 8.758299\n","iter: 10560, loss: 0.145609, lr: 0.000300, time: 8.042706\n","iter: 10580, loss: 0.140578, lr: 0.000300, time: 8.781733\n","iter: 10600, loss: 0.135553, lr: 0.000300, time: 8.038773\n","iter: 10620, loss: 0.146107, lr: 0.000300, time: 8.796113\n","iter: 10640, loss: 0.110084, lr: 0.000300, time: 8.047335\n","iter: 10660, loss: 0.112480, lr: 0.000300, time: 8.684119\n","iter: 10680, loss: 0.098431, lr: 0.000300, time: 8.063947\n","iter: 10700, loss: 0.104666, lr: 0.000300, time: 7.985352\n","iter: 10720, loss: 0.106198, lr: 0.000300, time: 8.817914\n","iter: 10740, loss: 0.082589, lr: 0.000300, time: 7.997505\n","iter: 10760, loss: 0.136917, lr: 0.000300, time: 8.763862\n","iter: 10780, loss: 0.101394, lr: 0.000300, time: 7.998883\n","iter: 10800, loss: 0.127198, lr: 0.000300, time: 8.741520\n","iter: 10820, loss: 0.114295, lr: 0.000300, time: 8.010198\n","iter: 10840, loss: 0.151222, lr: 0.000300, time: 8.668603\n","iter: 10860, loss: 0.098396, lr: 0.000300, time: 8.010324\n","iter: 10880, loss: 0.084280, lr: 0.000300, time: 8.685752\n","iter: 10900, loss: 0.137470, lr: 0.000300, time: 8.021470\n","iter: 10920, loss: 0.100728, lr: 0.000300, time: 8.750719\n","iter: 10940, loss: 0.121660, lr: 0.000300, time: 8.023649\n","iter: 10960, loss: 0.086722, lr: 0.000300, time: 8.748731\n","iter: 10980, loss: 0.151691, lr: 0.000300, time: 8.028292\n","iter: 11000, loss: 0.094671, lr: 0.000300, time: 8.729981\n","iter: 11020, loss: 0.108339, lr: 0.000300, time: 8.028542\n","iter: 11040, loss: 0.114616, lr: 0.000300, time: 8.742564\n","iter: 11060, loss: 0.124246, lr: 0.000300, time: 8.032130\n","iter: 11080, loss: 0.130686, lr: 0.000300, time: 8.771778\n","iter: 11100, loss: 0.083520, lr: 0.000300, time: 8.038100\n","iter: 11120, loss: 0.127573, lr: 0.000300, time: 8.737905\n","iter: 11140, loss: 0.101951, lr: 0.000300, time: 8.030970\n","iter: 11160, loss: 0.119088, lr: 0.000300, time: 8.743668\n","iter: 11180, loss: 0.166044, lr: 0.000300, time: 8.010531\n","iter: 11200, loss: 0.116173, lr: 0.000300, time: 8.651765\n","iter: 11220, loss: 0.106144, lr: 0.000300, time: 8.018288\n","iter: 11240, loss: 0.131779, lr: 0.000300, time: 8.730291\n","iter: 11260, loss: 0.133362, lr: 0.000300, time: 8.035918\n","iter: 11280, loss: 0.111218, lr: 0.000300, time: 8.771769\n","iter: 11300, loss: 0.177680, lr: 0.000300, time: 8.016225\n","iter: 11320, loss: 0.153352, lr: 0.000300, time: 8.752807\n","iter: 11340, loss: 0.097331, lr: 0.000300, time: 8.036828\n","iter: 11360, loss: 0.110001, lr: 0.000300, time: 8.765146\n","iter: 11380, loss: 0.091405, lr: 0.000300, time: 8.039212\n","iter: 11400, loss: 0.161776, lr: 0.000300, time: 8.697560\n","iter: 11420, loss: 0.116566, lr: 0.000300, time: 8.043825\n","iter: 11440, loss: 0.122418, lr: 0.000300, time: 8.756192\n","iter: 11460, loss: 0.132975, lr: 0.000300, time: 8.046741\n","iter: 11480, loss: 0.156630, lr: 0.000300, time: 8.737796\n","iter: 11500, loss: 0.106452, lr: 0.000300, time: 8.030210\n","iter: 11520, loss: 0.157538, lr: 0.000300, time: 7.993386\n","iter: 11540, loss: 0.124845, lr: 0.000300, time: 8.728311\n","iter: 11560, loss: 0.162128, lr: 0.000300, time: 7.985978\n","iter: 11580, loss: 0.082469, lr: 0.000300, time: 8.797088\n","iter: 11600, loss: 0.125470, lr: 0.000300, time: 7.999310\n","iter: 11620, loss: 0.122567, lr: 0.000300, time: 8.804348\n","iter: 11640, loss: 0.145716, lr: 0.000300, time: 7.994668\n","iter: 11660, loss: 0.146203, lr: 0.000300, time: 8.771987\n","iter: 11680, loss: 0.106147, lr: 0.000300, time: 8.029197\n","iter: 11700, loss: 0.137620, lr: 0.000300, time: 8.836287\n","iter: 11720, loss: 0.149567, lr: 0.000300, time: 8.026595\n","iter: 11740, loss: 0.125091, lr: 0.000300, time: 8.846592\n","iter: 11760, loss: 0.112136, lr: 0.000300, time: 8.037631\n","iter: 11780, loss: 0.127921, lr: 0.000300, time: 8.774699\n","iter: 11800, loss: 0.102332, lr: 0.000300, time: 8.031413\n","iter: 11820, loss: 0.074334, lr: 0.000300, time: 8.745077\n","iter: 11840, loss: 0.121382, lr: 0.000300, time: 8.037350\n","iter: 11860, loss: 0.118514, lr: 0.000300, time: 8.759315\n","iter: 11880, loss: 0.115848, lr: 0.000300, time: 8.033719\n","iter: 11900, loss: 0.165342, lr: 0.000300, time: 8.811558\n","iter: 11920, loss: 0.116670, lr: 0.000300, time: 8.041350\n","iter: 11940, loss: 0.132954, lr: 0.000300, time: 8.739011\n","iter: 11960, loss: 0.135016, lr: 0.000300, time: 8.046424\n","iter: 11980, loss: 0.111385, lr: 0.000300, time: 8.747820\n","iter: 12000, loss: 0.118811, lr: 0.000300, time: 8.040724\n","iter: 12020, loss: 0.074985, lr: 0.000300, time: 8.772170\n","iter: 12040, loss: 0.083661, lr: 0.000300, time: 8.041772\n","iter: 12060, loss: 0.123955, lr: 0.000300, time: 8.767123\n","iter: 12080, loss: 0.101395, lr: 0.000300, time: 8.041575\n","iter: 12100, loss: 0.116809, lr: 0.000300, time: 8.756228\n","iter: 12120, loss: 0.131121, lr: 0.000300, time: 8.027493\n","iter: 12140, loss: 0.114612, lr: 0.000300, time: 8.729595\n","iter: 12160, loss: 0.123728, lr: 0.000300, time: 8.036963\n","iter: 12180, loss: 0.114436, lr: 0.000300, time: 8.725197\n","iter: 12200, loss: 0.133493, lr: 0.000300, time: 8.043561\n","iter: 12220, loss: 0.098528, lr: 0.000300, time: 8.796811\n","iter: 12240, loss: 0.105230, lr: 0.000300, time: 8.034853\n","iter: 12260, loss: 0.116630, lr: 0.000300, time: 8.766404\n","iter: 12280, loss: 0.098038, lr: 0.000300, time: 8.044811\n","iter: 12300, loss: 0.073779, lr: 0.000300, time: 8.743923\n","iter: 12320, loss: 0.100557, lr: 0.000300, time: 8.056882\n","iter: 12340, loss: 0.099229, lr: 0.000300, time: 8.001439\n","iter: 12360, loss: 0.112479, lr: 0.000300, time: 8.784459\n","iter: 12380, loss: 0.111867, lr: 0.000300, time: 7.997183\n","iter: 12400, loss: 0.115853, lr: 0.000300, time: 8.777239\n","iter: 12420, loss: 0.064411, lr: 0.000300, time: 8.030789\n","iter: 12440, loss: 0.115520, lr: 0.000300, time: 8.774075\n","iter: 12460, loss: 0.112303, lr: 0.000300, time: 8.012886\n","iter: 12480, loss: 0.086611, lr: 0.000300, time: 8.779420\n","iter: 12500, loss: 0.075585, lr: 0.000300, time: 8.021347\n","iter: 12520, loss: 0.100866, lr: 0.000300, time: 8.849571\n","iter: 12540, loss: 0.119694, lr: 0.000300, time: 8.019539\n","iter: 12560, loss: 0.083711, lr: 0.000300, time: 8.801597\n","iter: 12580, loss: 0.096239, lr: 0.000300, time: 8.024863\n","iter: 12600, loss: 0.095253, lr: 0.000300, time: 8.795941\n","iter: 12620, loss: 0.140614, lr: 0.000300, time: 8.036582\n","iter: 12640, loss: 0.112546, lr: 0.000300, time: 8.756974\n","iter: 12660, loss: 0.133093, lr: 0.000300, time: 8.030846\n","iter: 12680, loss: 0.074185, lr: 0.000300, time: 8.741571\n","iter: 12700, loss: 0.082312, lr: 0.000300, time: 8.044249\n","iter: 12720, loss: 0.108546, lr: 0.000300, time: 8.765843\n","iter: 12740, loss: 0.083048, lr: 0.000300, time: 8.039276\n","iter: 12760, loss: 0.070712, lr: 0.000300, time: 8.781062\n","iter: 12780, loss: 0.097216, lr: 0.000300, time: 8.037039\n","iter: 12800, loss: 0.090161, lr: 0.000300, time: 8.723789\n","iter: 12820, loss: 0.096649, lr: 0.000300, time: 8.060004\n","iter: 12840, loss: 0.120418, lr: 0.000300, time: 8.763068\n","iter: 12860, loss: 0.079081, lr: 0.000300, time: 8.033037\n","iter: 12880, loss: 0.147774, lr: 0.000300, time: 8.758541\n","iter: 12900, loss: 0.121454, lr: 0.000300, time: 8.032820\n","iter: 12920, loss: 0.128642, lr: 0.000300, time: 8.751966\n","iter: 12940, loss: 0.148221, lr: 0.000300, time: 8.042454\n","iter: 12960, loss: 0.108942, lr: 0.000300, time: 8.799925\n","iter: 12980, loss: 0.092191, lr: 0.000300, time: 8.041651\n","iter: 13000, loss: 0.092857, lr: 0.000300, time: 8.756469\n","iter: 13020, loss: 0.125399, lr: 0.000300, time: 8.036042\n","iter: 13040, loss: 0.098770, lr: 0.000300, time: 8.741325\n","iter: 13060, loss: 0.113785, lr: 0.000300, time: 8.039341\n","iter: 13080, loss: 0.093874, lr: 0.000300, time: 8.748656\n","iter: 13100, loss: 0.087666, lr: 0.000300, time: 8.055778\n","iter: 13120, loss: 0.097741, lr: 0.000300, time: 8.781874\n","iter: 13140, loss: 0.067005, lr: 0.000300, time: 8.061528\n","iter: 13160, loss: 0.079226, lr: 0.000300, time: 7.999191\n","iter: 13180, loss: 0.085392, lr: 0.000300, time: 8.831712\n","iter: 13200, loss: 0.068349, lr: 0.000300, time: 8.005063\n","iter: 13220, loss: 0.089985, lr: 0.000300, time: 8.676563\n","iter: 13240, loss: 0.126065, lr: 0.000300, time: 8.009785\n","iter: 13260, loss: 0.124457, lr: 0.000300, time: 8.697691\n","iter: 13280, loss: 0.105258, lr: 0.000300, time: 8.017200\n","iter: 13300, loss: 0.114313, lr: 0.000300, time: 8.817005\n","iter: 13320, loss: 0.104858, lr: 0.000300, time: 8.026098\n","iter: 13340, loss: 0.095808, lr: 0.000300, time: 8.741028\n","iter: 13360, loss: 0.102043, lr: 0.000300, time: 8.029385\n","iter: 13380, loss: 0.079656, lr: 0.000300, time: 8.810636\n","iter: 13400, loss: 0.072527, lr: 0.000300, time: 8.043151\n","iter: 13420, loss: 0.106855, lr: 0.000300, time: 8.743333\n","iter: 13440, loss: 0.101773, lr: 0.000300, time: 8.049488\n","iter: 13460, loss: 0.090259, lr: 0.000300, time: 8.738158\n","iter: 13480, loss: 0.114827, lr: 0.000300, time: 8.048809\n","iter: 13500, loss: 0.069775, lr: 0.000300, time: 8.820194\n","iter: 13520, loss: 0.127167, lr: 0.000300, time: 8.058919\n","iter: 13540, loss: 0.077204, lr: 0.000300, time: 8.721399\n","iter: 13560, loss: 0.134822, lr: 0.000300, time: 8.054098\n","iter: 13580, loss: 0.138273, lr: 0.000300, time: 8.744071\n","iter: 13600, loss: 0.109113, lr: 0.000300, time: 8.035769\n","iter: 13620, loss: 0.142915, lr: 0.000300, time: 8.760659\n","iter: 13640, loss: 0.127542, lr: 0.000300, time: 8.049812\n","iter: 13660, loss: 0.142753, lr: 0.000300, time: 8.813757\n","iter: 13680, loss: 0.089483, lr: 0.000300, time: 8.041339\n","iter: 13700, loss: 0.100269, lr: 0.000300, time: 8.610584\n","iter: 13720, loss: 0.074326, lr: 0.000300, time: 8.030805\n","iter: 13740, loss: 0.061794, lr: 0.000300, time: 8.808141\n","iter: 13760, loss: 0.102132, lr: 0.000300, time: 8.048443\n","iter: 13780, loss: 0.097493, lr: 0.000300, time: 8.800564\n","iter: 13800, loss: 0.091455, lr: 0.000300, time: 8.047415\n","iter: 13820, loss: 0.122535, lr: 0.000300, time: 8.799242\n","iter: 13840, loss: 0.082348, lr: 0.000300, time: 8.048999\n","iter: 13860, loss: 0.102436, lr: 0.000300, time: 8.754393\n","iter: 13880, loss: 0.079393, lr: 0.000300, time: 8.044938\n","iter: 13900, loss: 0.122174, lr: 0.000300, time: 8.779281\n","iter: 13920, loss: 0.097047, lr: 0.000300, time: 8.046658\n","iter: 13940, loss: 0.070931, lr: 0.000300, time: 8.779767\n","iter: 13960, loss: 0.129324, lr: 0.000300, time: 8.051915\n","iter: 13980, loss: 0.126500, lr: 0.000300, time: 7.998158\n","iter: 14000, loss: 0.106015, lr: 0.000300, time: 8.816571\n","iter: 14020, loss: 0.086676, lr: 0.000300, time: 8.016568\n","iter: 14040, loss: 0.093823, lr: 0.000300, time: 8.862062\n","iter: 14060, loss: 0.098159, lr: 0.000300, time: 8.014489\n","iter: 14080, loss: 0.100934, lr: 0.000300, time: 8.817786\n","iter: 14100, loss: 0.101150, lr: 0.000300, time: 8.021093\n","iter: 14120, loss: 0.154477, lr: 0.000300, time: 8.808167\n","iter: 14140, loss: 0.116690, lr: 0.000300, time: 8.015574\n","iter: 14160, loss: 0.123472, lr: 0.000300, time: 8.808437\n","iter: 14180, loss: 0.124968, lr: 0.000300, time: 8.021754\n","iter: 14200, loss: 0.086771, lr: 0.000300, time: 8.683792\n","iter: 14220, loss: 0.080644, lr: 0.000300, time: 8.027119\n","iter: 14240, loss: 0.094459, lr: 0.000300, time: 8.834274\n","iter: 14260, loss: 0.069006, lr: 0.000300, time: 8.046133\n","iter: 14280, loss: 0.097247, lr: 0.000300, time: 8.844073\n","iter: 14300, loss: 0.089922, lr: 0.000300, time: 8.037152\n","iter: 14320, loss: 0.111264, lr: 0.000300, time: 8.789702\n","iter: 14340, loss: 0.078227, lr: 0.000300, time: 8.042347\n","iter: 14360, loss: 0.056576, lr: 0.000300, time: 8.763079\n","iter: 14380, loss: 0.064321, lr: 0.000300, time: 8.044597\n","iter: 14400, loss: 0.085186, lr: 0.000300, time: 8.835731\n","iter: 14420, loss: 0.051985, lr: 0.000300, time: 8.045967\n","iter: 14440, loss: 0.058410, lr: 0.000300, time: 8.749081\n","iter: 14460, loss: 0.062597, lr: 0.000300, time: 8.039343\n","iter: 14480, loss: 0.065573, lr: 0.000300, time: 8.771379\n","iter: 14500, loss: 0.091767, lr: 0.000300, time: 8.039469\n","iter: 14520, loss: 0.067058, lr: 0.000300, time: 8.778029\n","iter: 14540, loss: 0.072480, lr: 0.000300, time: 8.051203\n","iter: 14560, loss: 0.076346, lr: 0.000300, time: 8.823579\n","iter: 14580, loss: 0.071615, lr: 0.000300, time: 8.033998\n","iter: 14600, loss: 0.100198, lr: 0.000300, time: 8.684952\n","iter: 14620, loss: 0.114526, lr: 0.000300, time: 8.052416\n","iter: 14640, loss: 0.094326, lr: 0.000300, time: 8.799073\n","iter: 14660, loss: 0.095370, lr: 0.000300, time: 8.048432\n","iter: 14680, loss: 0.098175, lr: 0.000300, time: 8.801346\n","iter: 14700, loss: 0.072654, lr: 0.000300, time: 8.041716\n","iter: 14720, loss: 0.097813, lr: 0.000300, time: 8.724674\n","iter: 14740, loss: 0.061964, lr: 0.000300, time: 8.038510\n","iter: 14760, loss: 0.092298, lr: 0.000300, time: 8.757222\n","iter: 14780, loss: 0.080082, lr: 0.000300, time: 8.045839\n","iter: 14800, loss: 0.104205, lr: 0.000300, time: 8.003971\n","iter: 14820, loss: 0.096083, lr: 0.000300, time: 8.877287\n","iter: 14840, loss: 0.077322, lr: 0.000300, time: 8.008392\n","iter: 14860, loss: 0.104636, lr: 0.000300, time: 8.835256\n","iter: 14880, loss: 0.094846, lr: 0.000300, time: 8.011412\n","iter: 14900, loss: 0.061721, lr: 0.000300, time: 8.772384\n","iter: 14920, loss: 0.071277, lr: 0.000300, time: 8.017771\n","iter: 14940, loss: 0.137494, lr: 0.000300, time: 8.665727\n","iter: 14960, loss: 0.103787, lr: 0.000300, time: 8.035831\n","iter: 14980, loss: 0.085524, lr: 0.000300, time: 8.828593\n","==> changing adam betas from (0.9, 0.999) to (0.5, 0.999)\n","==> start droping lr exponentially\n","iter: 15000, loss: 0.099426, lr: 0.000300, time: 8.048903\n","iter: 15020, loss: 0.082451, lr: 0.000295, time: 8.724659\n","iter: 15040, loss: 0.115722, lr: 0.000291, time: 8.034859\n","iter: 15060, loss: 0.104207, lr: 0.000287, time: 8.743817\n","iter: 15080, loss: 0.128303, lr: 0.000283, time: 8.051194\n","iter: 15100, loss: 0.107345, lr: 0.000280, time: 8.710332\n","iter: 15120, loss: 0.096508, lr: 0.000276, time: 8.040092\n","iter: 15140, loss: 0.079810, lr: 0.000272, time: 8.747599\n","iter: 15160, loss: 0.077594, lr: 0.000268, time: 8.051681\n","iter: 15180, loss: 0.089741, lr: 0.000265, time: 8.819608\n","iter: 15200, loss: 0.088331, lr: 0.000261, time: 8.051780\n","iter: 15220, loss: 0.078654, lr: 0.000257, time: 8.743044\n","iter: 15240, loss: 0.067755, lr: 0.000254, time: 8.042634\n","iter: 15260, loss: 0.077518, lr: 0.000250, time: 8.752266\n","iter: 15280, loss: 0.049649, lr: 0.000247, time: 8.042803\n","iter: 15300, loss: 0.107536, lr: 0.000244, time: 8.778042\n","iter: 15320, loss: 0.064561, lr: 0.000240, time: 8.039494\n","iter: 15340, loss: 0.041706, lr: 0.000237, time: 8.657711\n","iter: 15360, loss: 0.059603, lr: 0.000234, time: 8.043131\n","iter: 15380, loss: 0.056146, lr: 0.000230, time: 8.778961\n","iter: 15400, loss: 0.050152, lr: 0.000227, time: 8.034873\n","iter: 15420, loss: 0.097799, lr: 0.000224, time: 8.766841\n","iter: 15440, loss: 0.075321, lr: 0.000221, time: 8.019366\n","iter: 15460, loss: 0.048936, lr: 0.000218, time: 8.766971\n","iter: 15480, loss: 0.058776, lr: 0.000215, time: 8.042351\n","iter: 15500, loss: 0.078235, lr: 0.000212, time: 8.737395\n","iter: 15520, loss: 0.077694, lr: 0.000209, time: 8.029098\n","iter: 15540, loss: 0.052062, lr: 0.000206, time: 8.760403\n","iter: 15560, loss: 0.070143, lr: 0.000203, time: 8.019557\n","iter: 15580, loss: 0.043456, lr: 0.000201, time: 8.713722\n","iter: 15600, loss: 0.052963, lr: 0.000198, time: 8.025721\n","iter: 15620, loss: 0.046931, lr: 0.000195, time: 7.966719\n","iter: 15640, loss: 0.044109, lr: 0.000193, time: 8.761516\n","iter: 15660, loss: 0.042582, lr: 0.000190, time: 7.993394\n","iter: 15680, loss: 0.043753, lr: 0.000187, time: 8.813982\n","iter: 15700, loss: 0.039216, lr: 0.000185, time: 8.002096\n","iter: 15720, loss: 0.039699, lr: 0.000182, time: 8.781866\n","iter: 15740, loss: 0.050224, lr: 0.000180, time: 8.013974\n","iter: 15760, loss: 0.031671, lr: 0.000177, time: 8.805123\n","iter: 15780, loss: 0.060469, lr: 0.000175, time: 8.013462\n","iter: 15800, loss: 0.065043, lr: 0.000172, time: 8.795453\n","iter: 15820, loss: 0.065780, lr: 0.000170, time: 8.025713\n","iter: 15840, loss: 0.039030, lr: 0.000168, time: 8.755580\n","iter: 15860, loss: 0.049799, lr: 0.000165, time: 8.021061\n","iter: 15880, loss: 0.046459, lr: 0.000163, time: 8.780099\n","iter: 15900, loss: 0.038507, lr: 0.000161, time: 8.048222\n","iter: 15920, loss: 0.044981, lr: 0.000159, time: 8.786469\n","iter: 15940, loss: 0.039884, lr: 0.000157, time: 8.041676\n","iter: 15960, loss: 0.035733, lr: 0.000154, time: 8.848656\n","iter: 15980, loss: 0.033860, lr: 0.000152, time: 8.053254\n","iter: 16000, loss: 0.039338, lr: 0.000150, time: 8.758299\n","iter: 16020, loss: 0.043689, lr: 0.000148, time: 8.058959\n","iter: 16040, loss: 0.024258, lr: 0.000146, time: 8.788551\n","iter: 16060, loss: 0.050700, lr: 0.000144, time: 8.050048\n","iter: 16080, loss: 0.043830, lr: 0.000142, time: 8.815999\n","iter: 16100, loss: 0.046496, lr: 0.000140, time: 8.051092\n","iter: 16120, loss: 0.039130, lr: 0.000138, time: 8.840342\n","iter: 16140, loss: 0.032259, lr: 0.000136, time: 8.050166\n","iter: 16160, loss: 0.040321, lr: 0.000134, time: 8.766728\n","iter: 16180, loss: 0.017748, lr: 0.000133, time: 8.059742\n","iter: 16200, loss: 0.032294, lr: 0.000131, time: 8.841208\n","iter: 16220, loss: 0.038042, lr: 0.000129, time: 8.056385\n","iter: 16240, loss: 0.037873, lr: 0.000127, time: 8.702174\n","iter: 16260, loss: 0.038242, lr: 0.000125, time: 8.055564\n","iter: 16280, loss: 0.028463, lr: 0.000124, time: 8.664060\n","iter: 16300, loss: 0.046369, lr: 0.000122, time: 8.038593\n","iter: 16320, loss: 0.030336, lr: 0.000120, time: 8.769600\n","iter: 16340, loss: 0.018770, lr: 0.000119, time: 8.049304\n","iter: 16360, loss: 0.065198, lr: 0.000117, time: 8.740695\n","iter: 16380, loss: 0.037088, lr: 0.000115, time: 8.052831\n","iter: 16400, loss: 0.036068, lr: 0.000114, time: 8.674876\n","iter: 16420, loss: 0.060074, lr: 0.000112, time: 8.061947\n","iter: 16440, loss: 0.053947, lr: 0.000111, time: 8.009246\n","iter: 16460, loss: 0.046936, lr: 0.000109, time: 8.750130\n","iter: 16480, loss: 0.043219, lr: 0.000108, time: 8.016834\n","iter: 16500, loss: 0.065356, lr: 0.000106, time: 8.737381\n","iter: 16520, loss: 0.034829, lr: 0.000105, time: 8.018999\n","iter: 16540, loss: 0.031520, lr: 0.000103, time: 8.855158\n","iter: 16560, loss: 0.030268, lr: 0.000102, time: 8.017359\n","iter: 16580, loss: 0.056544, lr: 0.000101, time: 8.676890\n","iter: 16600, loss: 0.042083, lr: 0.000099, time: 8.026210\n","iter: 16620, loss: 0.029470, lr: 0.000098, time: 8.801391\n","iter: 16640, loss: 0.037186, lr: 0.000097, time: 8.026945\n","iter: 16660, loss: 0.041275, lr: 0.000095, time: 8.758427\n","iter: 16680, loss: 0.035545, lr: 0.000094, time: 8.049504\n","iter: 16700, loss: 0.034632, lr: 0.000093, time: 8.751822\n","iter: 16720, loss: 0.031501, lr: 0.000091, time: 8.046395\n","iter: 16740, loss: 0.040118, lr: 0.000090, time: 8.775892\n","iter: 16760, loss: 0.044479, lr: 0.000089, time: 8.045393\n","iter: 16780, loss: 0.050050, lr: 0.000088, time: 8.734165\n","iter: 16800, loss: 0.031405, lr: 0.000086, time: 8.045324\n","iter: 16820, loss: 0.049429, lr: 0.000085, time: 8.765627\n","iter: 16840, loss: 0.030316, lr: 0.000084, time: 8.050491\n","iter: 16860, loss: 0.027216, lr: 0.000083, time: 8.711945\n","iter: 16880, loss: 0.036610, lr: 0.000082, time: 8.052577\n","iter: 16900, loss: 0.017554, lr: 0.000081, time: 8.726255\n","iter: 16920, loss: 0.014115, lr: 0.000080, time: 8.051414\n","iter: 16940, loss: 0.014142, lr: 0.000078, time: 8.831451\n","iter: 16960, loss: 0.024180, lr: 0.000077, time: 8.055009\n","iter: 16980, loss: 0.039900, lr: 0.000076, time: 8.796808\n","iter: 17000, loss: 0.032143, lr: 0.000075, time: 8.040305\n","iter: 17020, loss: 0.017923, lr: 0.000074, time: 8.798705\n","iter: 17040, loss: 0.025473, lr: 0.000073, time: 8.029418\n","iter: 17060, loss: 0.028444, lr: 0.000072, time: 8.697835\n","iter: 17080, loss: 0.047281, lr: 0.000071, time: 8.049905\n","iter: 17100, loss: 0.018295, lr: 0.000070, time: 8.810063\n","iter: 17120, loss: 0.051338, lr: 0.000069, time: 8.051426\n","iter: 17140, loss: 0.041798, lr: 0.000068, time: 8.772639\n","iter: 17160, loss: 0.054473, lr: 0.000067, time: 8.048445\n","iter: 17180, loss: 0.021827, lr: 0.000066, time: 8.792500\n","iter: 17200, loss: 0.028752, lr: 0.000066, time: 8.034846\n","iter: 17220, loss: 0.030880, lr: 0.000065, time: 8.774629\n","iter: 17240, loss: 0.015910, lr: 0.000064, time: 8.072323\n","iter: 17260, loss: 0.054988, lr: 0.000063, time: 8.000846\n","iter: 17280, loss: 0.037121, lr: 0.000062, time: 8.850365\n","iter: 17300, loss: 0.026612, lr: 0.000061, time: 8.014416\n","iter: 17320, loss: 0.034100, lr: 0.000060, time: 8.791770\n","iter: 17340, loss: 0.023665, lr: 0.000060, time: 8.017298\n","iter: 17360, loss: 0.025062, lr: 0.000059, time: 8.824327\n","iter: 17380, loss: 0.020608, lr: 0.000058, time: 8.024572\n","iter: 17400, loss: 0.029118, lr: 0.000057, time: 8.921165\n","iter: 17420, loss: 0.033558, lr: 0.000056, time: 8.031942\n","iter: 17440, loss: 0.028691, lr: 0.000056, time: 8.807210\n","iter: 17460, loss: 0.042892, lr: 0.000055, time: 8.020263\n","iter: 17480, loss: 0.030431, lr: 0.000054, time: 8.790918\n","iter: 17500, loss: 0.029308, lr: 0.000053, time: 8.039653\n","iter: 17520, loss: 0.025400, lr: 0.000053, time: 8.695714\n","iter: 17540, loss: 0.025357, lr: 0.000052, time: 8.055612\n","iter: 17560, loss: 0.042820, lr: 0.000051, time: 8.826594\n","iter: 17580, loss: 0.027534, lr: 0.000050, time: 8.050082\n","iter: 17600, loss: 0.034264, lr: 0.000050, time: 8.798837\n","iter: 17620, loss: 0.038256, lr: 0.000049, time: 8.043681\n","iter: 17640, loss: 0.035679, lr: 0.000048, time: 8.806713\n","iter: 17660, loss: 0.018142, lr: 0.000048, time: 8.049120\n","iter: 17680, loss: 0.021422, lr: 0.000047, time: 8.802500\n","iter: 17700, loss: 0.017501, lr: 0.000046, time: 8.050018\n","iter: 17720, loss: 0.026954, lr: 0.000046, time: 8.650374\n","iter: 17740, loss: 0.029695, lr: 0.000045, time: 8.042712\n","iter: 17760, loss: 0.032580, lr: 0.000045, time: 8.759099\n","iter: 17780, loss: 0.023627, lr: 0.000044, time: 8.041338\n","iter: 17800, loss: 0.030160, lr: 0.000043, time: 8.780427\n","iter: 17820, loss: 0.019433, lr: 0.000043, time: 8.052358\n","iter: 17840, loss: 0.022776, lr: 0.000042, time: 8.844302\n","iter: 17860, loss: 0.045617, lr: 0.000042, time: 8.049156\n","iter: 17880, loss: 0.030803, lr: 0.000041, time: 8.760622\n","iter: 17900, loss: 0.025074, lr: 0.000040, time: 8.046179\n","iter: 17920, loss: 0.025802, lr: 0.000040, time: 8.818416\n","iter: 17940, loss: 0.034808, lr: 0.000039, time: 8.061903\n","iter: 17960, loss: 0.015430, lr: 0.000039, time: 8.777882\n","iter: 17980, loss: 0.029576, lr: 0.000038, time: 8.050007\n","iter: 18000, loss: 0.033054, lr: 0.000038, time: 8.756315\n","iter: 18020, loss: 0.013403, lr: 0.000037, time: 8.061220\n","iter: 18040, loss: 0.032927, lr: 0.000037, time: 8.788992\n","iter: 18060, loss: 0.045652, lr: 0.000036, time: 8.072346\n","iter: 18080, loss: 0.028003, lr: 0.000036, time: 8.001735\n","iter: 18100, loss: 0.034491, lr: 0.000035, time: 8.800245\n","iter: 18120, loss: 0.049837, lr: 0.000035, time: 8.010438\n","iter: 18140, loss: 0.021760, lr: 0.000034, time: 8.802174\n","iter: 18160, loss: 0.050667, lr: 0.000034, time: 8.011413\n","iter: 18180, loss: 0.037165, lr: 0.000033, time: 8.825265\n","iter: 18200, loss: 0.035118, lr: 0.000033, time: 8.011497\n","iter: 18220, loss: 0.015072, lr: 0.000032, time: 8.857881\n","iter: 18240, loss: 0.023117, lr: 0.000032, time: 8.029512\n","iter: 18260, loss: 0.016615, lr: 0.000032, time: 8.832941\n","iter: 18280, loss: 0.017525, lr: 0.000031, time: 8.024590\n","iter: 18300, loss: 0.022480, lr: 0.000031, time: 8.779449\n","iter: 18320, loss: 0.012636, lr: 0.000030, time: 8.042134\n","iter: 18340, loss: 0.024611, lr: 0.000030, time: 8.793085\n","iter: 18360, loss: 0.013018, lr: 0.000029, time: 8.028293\n","iter: 18380, loss: 0.021847, lr: 0.000029, time: 8.752984\n","iter: 18400, loss: 0.037575, lr: 0.000029, time: 8.047355\n","iter: 18420, loss: 0.013094, lr: 0.000028, time: 8.828512\n","iter: 18440, loss: 0.025272, lr: 0.000028, time: 8.053747\n","iter: 18460, loss: 0.011168, lr: 0.000027, time: 8.776670\n","iter: 18480, loss: 0.016002, lr: 0.000027, time: 8.053813\n","iter: 18500, loss: 0.038751, lr: 0.000027, time: 8.795536\n","iter: 18520, loss: 0.017987, lr: 0.000026, time: 8.063181\n","iter: 18540, loss: 0.017751, lr: 0.000026, time: 8.744847\n","iter: 18560, loss: 0.027122, lr: 0.000026, time: 8.051353\n","iter: 18580, loss: 0.035043, lr: 0.000025, time: 8.791785\n","iter: 18600, loss: 0.021694, lr: 0.000025, time: 8.046626\n","iter: 18620, loss: 0.037660, lr: 0.000025, time: 8.798624\n","iter: 18640, loss: 0.031986, lr: 0.000024, time: 8.045788\n","iter: 18660, loss: 0.027504, lr: 0.000024, time: 8.872514\n","iter: 18680, loss: 0.012822, lr: 0.000024, time: 8.055621\n","iter: 18700, loss: 0.024370, lr: 0.000023, time: 8.845025\n","iter: 18720, loss: 0.025195, lr: 0.000023, time: 8.041762\n","iter: 18740, loss: 0.043235, lr: 0.000023, time: 8.779078\n","iter: 18760, loss: 0.008422, lr: 0.000022, time: 8.044219\n","iter: 18780, loss: 0.022480, lr: 0.000022, time: 8.788348\n","iter: 18800, loss: 0.023170, lr: 0.000022, time: 8.073364\n","iter: 18820, loss: 0.015636, lr: 0.000021, time: 8.725864\n","iter: 18840, loss: 0.031568, lr: 0.000021, time: 8.050615\n","iter: 18860, loss: 0.016603, lr: 0.000021, time: 8.726482\n","iter: 18880, loss: 0.031798, lr: 0.000021, time: 8.067896\n","iter: 18900, loss: 0.014286, lr: 0.000020, time: 7.997899\n","iter: 18920, loss: 0.019124, lr: 0.000020, time: 8.791432\n","iter: 18940, loss: 0.011372, lr: 0.000020, time: 8.012811\n","iter: 18960, loss: 0.030914, lr: 0.000019, time: 8.841841\n","iter: 18980, loss: 0.046346, lr: 0.000019, time: 8.027590\n","iter: 19000, loss: 0.025091, lr: 0.000019, time: 8.707047\n","iter: 19020, loss: 0.019203, lr: 0.000019, time: 8.010785\n","iter: 19040, loss: 0.039713, lr: 0.000018, time: 8.810565\n","iter: 19060, loss: 0.025347, lr: 0.000018, time: 8.036188\n","iter: 19080, loss: 0.018719, lr: 0.000018, time: 8.805197\n","iter: 19100, loss: 0.024083, lr: 0.000018, time: 8.034529\n","iter: 19120, loss: 0.009426, lr: 0.000017, time: 8.836822\n","iter: 19140, loss: 0.022727, lr: 0.000017, time: 8.039631\n","iter: 19160, loss: 0.024482, lr: 0.000017, time: 8.624918\n","iter: 19180, loss: 0.027531, lr: 0.000017, time: 8.048339\n","iter: 19200, loss: 0.033508, lr: 0.000016, time: 8.826753\n","iter: 19220, loss: 0.019018, lr: 0.000016, time: 8.042049\n","iter: 19240, loss: 0.041929, lr: 0.000016, time: 8.816575\n","iter: 19260, loss: 0.029601, lr: 0.000016, time: 8.064582\n","iter: 19280, loss: 0.017498, lr: 0.000016, time: 8.776278\n","iter: 19300, loss: 0.039894, lr: 0.000015, time: 8.049845\n","iter: 19320, loss: 0.013979, lr: 0.000015, time: 8.737818\n","iter: 19340, loss: 0.008884, lr: 0.000015, time: 8.047509\n","iter: 19360, loss: 0.015894, lr: 0.000015, time: 8.805759\n","iter: 19380, loss: 0.029393, lr: 0.000015, time: 8.037392\n","iter: 19400, loss: 0.022081, lr: 0.000014, time: 8.816962\n","iter: 19420, loss: 0.030484, lr: 0.000014, time: 8.052850\n","iter: 19440, loss: 0.018344, lr: 0.000014, time: 8.741652\n","iter: 19460, loss: 0.024251, lr: 0.000014, time: 8.062236\n","iter: 19480, loss: 0.011735, lr: 0.000014, time: 8.770154\n","iter: 19500, loss: 0.012539, lr: 0.000013, time: 8.053926\n","iter: 19520, loss: 0.024141, lr: 0.000013, time: 8.795854\n","iter: 19540, loss: 0.018881, lr: 0.000013, time: 8.051624\n","iter: 19560, loss: 0.016490, lr: 0.000013, time: 8.766262\n","iter: 19580, loss: 0.021437, lr: 0.000013, time: 8.049059\n","iter: 19600, loss: 0.017316, lr: 0.000012, time: 8.813109\n","iter: 19620, loss: 0.011961, lr: 0.000012, time: 8.038259\n","iter: 19640, loss: 0.009524, lr: 0.000012, time: 8.822671\n","iter: 19660, loss: 0.016481, lr: 0.000012, time: 8.051794\n","iter: 19680, loss: 0.024676, lr: 0.000012, time: 8.787594\n","iter: 19700, loss: 0.011489, lr: 0.000012, time: 8.070800\n","iter: 19720, loss: 0.028552, lr: 0.000011, time: 8.007952\n","iter: 19740, loss: 0.018199, lr: 0.000011, time: 8.880432\n","iter: 19760, loss: 0.016470, lr: 0.000011, time: 8.018734\n","iter: 19780, loss: 0.020029, lr: 0.000011, time: 8.843588\n","iter: 19800, loss: 0.039623, lr: 0.000011, time: 8.029784\n","iter: 19820, loss: 0.018947, lr: 0.000011, time: 8.808664\n","iter: 19840, loss: 0.010178, lr: 0.000011, time: 8.022301\n","iter: 19860, loss: 0.038343, lr: 0.000010, time: 8.769855\n","iter: 19880, loss: 0.027237, lr: 0.000010, time: 8.025152\n","iter: 19900, loss: 0.016779, lr: 0.000010, time: 8.820789\n","iter: 19920, loss: 0.009557, lr: 0.000010, time: 8.023116\n","iter: 19940, loss: 0.015124, lr: 0.000010, time: 8.848658\n","iter: 19960, loss: 0.020261, lr: 0.000010, time: 8.010751\n","iter: 19980, loss: 0.014961, lr: 0.000010, time: 8.813349\n","iter: 20000, loss: 0.009745, lr: 0.000009, time: 8.008834\n","iter: 20020, loss: 0.027399, lr: 0.000009, time: 8.718270\n","iter: 20040, loss: 0.021858, lr: 0.000009, time: 8.055099\n","iter: 20060, loss: 0.027424, lr: 0.000009, time: 8.807273\n","iter: 20080, loss: 0.029337, lr: 0.000009, time: 8.058326\n","iter: 20100, loss: 0.031922, lr: 0.000009, time: 8.807276\n","iter: 20120, loss: 0.023443, lr: 0.000009, time: 8.042000\n","iter: 20140, loss: 0.024884, lr: 0.000009, time: 8.816979\n","iter: 20160, loss: 0.015027, lr: 0.000008, time: 8.047537\n","iter: 20180, loss: 0.014124, lr: 0.000008, time: 8.821768\n","iter: 20200, loss: 0.025359, lr: 0.000008, time: 8.054550\n","iter: 20220, loss: 0.014993, lr: 0.000008, time: 8.727826\n","iter: 20240, loss: 0.024180, lr: 0.000008, time: 8.060553\n","iter: 20260, loss: 0.035873, lr: 0.000008, time: 8.810691\n","iter: 20280, loss: 0.011482, lr: 0.000008, time: 8.048254\n","iter: 20300, loss: 0.010187, lr: 0.000008, time: 8.811352\n","iter: 20320, loss: 0.010994, lr: 0.000008, time: 8.048498\n","iter: 20340, loss: 0.014994, lr: 0.000007, time: 8.784959\n","iter: 20360, loss: 0.017945, lr: 0.000007, time: 8.058211\n","iter: 20380, loss: 0.025485, lr: 0.000007, time: 8.743472\n","iter: 20400, loss: 0.026484, lr: 0.000007, time: 8.046490\n","iter: 20420, loss: 0.012387, lr: 0.000007, time: 8.782273\n","iter: 20440, loss: 0.017962, lr: 0.000007, time: 8.047736\n","iter: 20460, loss: 0.013793, lr: 0.000007, time: 8.787668\n","iter: 20480, loss: 0.016174, lr: 0.000007, time: 8.064523\n","iter: 20500, loss: 0.014738, lr: 0.000007, time: 8.798511\n","iter: 20520, loss: 0.016295, lr: 0.000007, time: 8.089705\n","iter: 20540, loss: 0.026477, lr: 0.000007, time: 8.015822\n","iter: 20560, loss: 0.021531, lr: 0.000006, time: 8.856678\n","iter: 20580, loss: 0.012905, lr: 0.000006, time: 8.016647\n","iter: 20600, loss: 0.025999, lr: 0.000006, time: 8.798724\n","iter: 20620, loss: 0.009808, lr: 0.000006, time: 8.015119\n","iter: 20640, loss: 0.013126, lr: 0.000006, time: 8.842649\n","iter: 20660, loss: 0.007597, lr: 0.000006, time: 8.022130\n","iter: 20680, loss: 0.019770, lr: 0.000006, time: 8.841740\n","iter: 20700, loss: 0.021606, lr: 0.000006, time: 8.022729\n","iter: 20720, loss: 0.009446, lr: 0.000006, time: 8.722304\n","iter: 20740, loss: 0.012708, lr: 0.000006, time: 8.042827\n","iter: 20760, loss: 0.023332, lr: 0.000006, time: 8.816474\n","iter: 20780, loss: 0.019948, lr: 0.000006, time: 8.036155\n","iter: 20800, loss: 0.016803, lr: 0.000005, time: 8.714731\n","iter: 20820, loss: 0.024914, lr: 0.000005, time: 8.050270\n","iter: 20840, loss: 0.014629, lr: 0.000005, time: 8.813923\n","iter: 20860, loss: 0.025129, lr: 0.000005, time: 8.055764\n","iter: 20880, loss: 0.018053, lr: 0.000005, time: 8.780703\n","iter: 20900, loss: 0.012196, lr: 0.000005, time: 8.051725\n","iter: 20920, loss: 0.037727, lr: 0.000005, time: 8.773743\n","iter: 20940, loss: 0.016658, lr: 0.000005, time: 8.046110\n","iter: 20960, loss: 0.014955, lr: 0.000005, time: 8.747336\n","iter: 20980, loss: 0.015944, lr: 0.000005, time: 8.044976\n","iter: 21000, loss: 0.012063, lr: 0.000005, time: 8.815529\n","iter: 21020, loss: 0.018976, lr: 0.000005, time: 8.050914\n","iter: 21040, loss: 0.019949, lr: 0.000005, time: 8.758898\n","iter: 21060, loss: 0.015664, lr: 0.000005, time: 8.040432\n","iter: 21080, loss: 0.013584, lr: 0.000004, time: 8.804085\n","iter: 21100, loss: 0.017764, lr: 0.000004, time: 8.066213\n","iter: 21120, loss: 0.011746, lr: 0.000004, time: 8.783369\n","iter: 21140, loss: 0.024773, lr: 0.000004, time: 8.044409\n","iter: 21160, loss: 0.013072, lr: 0.000004, time: 8.804294\n","iter: 21180, loss: 0.013458, lr: 0.000004, time: 8.045300\n","iter: 21200, loss: 0.025242, lr: 0.000004, time: 8.777179\n","iter: 21220, loss: 0.010588, lr: 0.000004, time: 8.054613\n","iter: 21240, loss: 0.020424, lr: 0.000004, time: 8.628519\n","iter: 21260, loss: 0.026192, lr: 0.000004, time: 8.050479\n","iter: 21280, loss: 0.015388, lr: 0.000004, time: 8.783864\n","iter: 21300, loss: 0.014241, lr: 0.000004, time: 8.044840\n","iter: 21320, loss: 0.017284, lr: 0.000004, time: 8.772779\n","iter: 21340, loss: 0.013646, lr: 0.000004, time: 8.061610\n","iter: 21360, loss: 0.011648, lr: 0.000004, time: 7.998105\n","iter: 21380, loss: 0.034021, lr: 0.000004, time: 8.742751\n","iter: 21400, loss: 0.034758, lr: 0.000004, time: 8.001831\n","iter: 21420, loss: 0.014040, lr: 0.000004, time: 8.821986\n","iter: 21440, loss: 0.012123, lr: 0.000004, time: 8.010999\n","iter: 21460, loss: 0.014774, lr: 0.000003, time: 8.849749\n","iter: 21480, loss: 0.013656, lr: 0.000003, time: 8.036519\n","iter: 21500, loss: 0.015172, lr: 0.000003, time: 8.893380\n","iter: 21520, loss: 0.018266, lr: 0.000003, time: 8.052393\n","iter: 21540, loss: 0.018770, lr: 0.000003, time: 8.851982\n","iter: 21560, loss: 0.010535, lr: 0.000003, time: 8.040111\n","iter: 21580, loss: 0.008018, lr: 0.000003, time: 8.834513\n","iter: 21600, loss: 0.011729, lr: 0.000003, time: 8.036706\n","iter: 21620, loss: 0.017746, lr: 0.000003, time: 8.837884\n","iter: 21640, loss: 0.027294, lr: 0.000003, time: 8.040774\n","iter: 21660, loss: 0.005988, lr: 0.000003, time: 8.823173\n","iter: 21680, loss: 0.021324, lr: 0.000003, time: 8.053818\n","iter: 21700, loss: 0.028273, lr: 0.000003, time: 8.829098\n","iter: 21720, loss: 0.008194, lr: 0.000003, time: 8.057841\n","iter: 21740, loss: 0.011935, lr: 0.000003, time: 8.800666\n","iter: 21760, loss: 0.019308, lr: 0.000003, time: 8.057669\n","iter: 21780, loss: 0.014895, lr: 0.000003, time: 8.791113\n","iter: 21800, loss: 0.032116, lr: 0.000003, time: 8.044222\n","iter: 21820, loss: 0.012779, lr: 0.000003, time: 8.738660\n","iter: 21840, loss: 0.019633, lr: 0.000003, time: 8.044446\n","iter: 21860, loss: 0.024975, lr: 0.000003, time: 8.725700\n","iter: 21880, loss: 0.018767, lr: 0.000003, time: 8.039708\n","iter: 21900, loss: 0.023040, lr: 0.000003, time: 8.812750\n","iter: 21920, loss: 0.017857, lr: 0.000003, time: 8.049985\n","iter: 21940, loss: 0.016255, lr: 0.000002, time: 8.773866\n","iter: 21960, loss: 0.023623, lr: 0.000002, time: 8.043046\n","iter: 21980, loss: 0.037336, lr: 0.000002, time: 8.768336\n","iter: 22000, loss: 0.011388, lr: 0.000002, time: 8.033579\n","iter: 22020, loss: 0.032481, lr: 0.000002, time: 8.720385\n","iter: 22040, loss: 0.034922, lr: 0.000002, time: 8.051105\n","iter: 22060, loss: 0.027825, lr: 0.000002, time: 8.724723\n","iter: 22080, loss: 0.017122, lr: 0.000002, time: 8.037813\n","iter: 22100, loss: 0.016793, lr: 0.000002, time: 8.727969\n","iter: 22120, loss: 0.023293, lr: 0.000002, time: 8.037977\n","iter: 22140, loss: 0.016679, lr: 0.000002, time: 8.747829\n","iter: 22160, loss: 0.016617, lr: 0.000002, time: 8.066276\n","iter: 22180, loss: 0.018498, lr: 0.000002, time: 8.014747\n","iter: 22200, loss: 0.016416, lr: 0.000002, time: 8.785624\n","iter: 22220, loss: 0.015952, lr: 0.000002, time: 8.005757\n","iter: 22240, loss: 0.017955, lr: 0.000002, time: 8.848732\n","iter: 22260, loss: 0.016894, lr: 0.000002, time: 8.024085\n","iter: 22280, loss: 0.026035, lr: 0.000002, time: 8.830798\n","iter: 22300, loss: 0.020471, lr: 0.000002, time: 8.010223\n","iter: 22320, loss: 0.015522, lr: 0.000002, time: 8.682339\n","iter: 22340, loss: 0.023063, lr: 0.000002, time: 8.040486\n","iter: 22360, loss: 0.013367, lr: 0.000002, time: 8.851586\n","iter: 22380, loss: 0.014467, lr: 0.000002, time: 8.048502\n","iter: 22400, loss: 0.018775, lr: 0.000002, time: 8.882232\n","iter: 22420, loss: 0.012531, lr: 0.000002, time: 8.035454\n","iter: 22440, loss: 0.015681, lr: 0.000002, time: 8.756979\n","iter: 22460, loss: 0.008593, lr: 0.000002, time: 8.038299\n","iter: 22480, loss: 0.011779, lr: 0.000002, time: 8.765118\n","iter: 22500, loss: 0.015240, lr: 0.000002, time: 8.051752\n","iter: 22520, loss: 0.008661, lr: 0.000002, time: 8.794800\n","iter: 22540, loss: 0.017983, lr: 0.000002, time: 8.022587\n","iter: 22560, loss: 0.017289, lr: 0.000002, time: 8.748464\n","iter: 22580, loss: 0.013254, lr: 0.000002, time: 8.034054\n","iter: 22600, loss: 0.022732, lr: 0.000002, time: 8.772478\n","iter: 22620, loss: 0.012400, lr: 0.000002, time: 8.048282\n","iter: 22640, loss: 0.015018, lr: 0.000002, time: 8.747462\n","iter: 22660, loss: 0.015082, lr: 0.000002, time: 8.033646\n","iter: 22680, loss: 0.012532, lr: 0.000001, time: 8.721654\n","iter: 22700, loss: 0.012042, lr: 0.000001, time: 8.032123\n","iter: 22720, loss: 0.011645, lr: 0.000001, time: 8.742230\n","iter: 22740, loss: 0.027174, lr: 0.000001, time: 8.045583\n","iter: 22760, loss: 0.013832, lr: 0.000001, time: 8.801979\n","iter: 22780, loss: 0.030411, lr: 0.000001, time: 8.060354\n","iter: 22800, loss: 0.014685, lr: 0.000001, time: 8.778938\n","iter: 22820, loss: 0.022765, lr: 0.000001, time: 8.053556\n","iter: 22840, loss: 0.018081, lr: 0.000001, time: 8.746589\n","iter: 22860, loss: 0.012511, lr: 0.000001, time: 8.056068\n","iter: 22880, loss: 0.016225, lr: 0.000001, time: 8.762123\n","iter: 22900, loss: 0.019825, lr: 0.000001, time: 8.068395\n","iter: 22920, loss: 0.024185, lr: 0.000001, time: 8.771928\n","iter: 22940, loss: 0.018543, lr: 0.000001, time: 8.044356\n","iter: 22960, loss: 0.013008, lr: 0.000001, time: 8.746356\n","iter: 22980, loss: 0.020785, lr: 0.000001, time: 8.071937\n","iter: 23000, loss: 0.018611, lr: 0.000001, time: 7.992559\n","iter: 23020, loss: 0.018887, lr: 0.000001, time: 8.811104\n","iter: 23040, loss: 0.011140, lr: 0.000001, time: 7.999305\n","iter: 23060, loss: 0.012625, lr: 0.000001, time: 8.793314\n","iter: 23080, loss: 0.025230, lr: 0.000001, time: 8.002121\n","iter: 23100, loss: 0.017975, lr: 0.000001, time: 8.781368\n","iter: 23120, loss: 0.016782, lr: 0.000001, time: 8.024008\n","iter: 23140, loss: 0.013892, lr: 0.000001, time: 8.774123\n","iter: 23160, loss: 0.016818, lr: 0.000001, time: 8.018579\n","iter: 23180, loss: 0.021137, lr: 0.000001, time: 8.818670\n","iter: 23200, loss: 0.025485, lr: 0.000001, time: 8.030575\n","iter: 23220, loss: 0.024095, lr: 0.000001, time: 8.880765\n","iter: 23240, loss: 0.009313, lr: 0.000001, time: 8.033124\n","iter: 23260, loss: 0.027043, lr: 0.000001, time: 8.682898\n","iter: 23280, loss: 0.034421, lr: 0.000001, time: 8.026803\n","iter: 23300, loss: 0.023120, lr: 0.000001, time: 8.782661\n","iter: 23320, loss: 0.036496, lr: 0.000001, time: 8.043689\n","iter: 23340, loss: 0.010008, lr: 0.000001, time: 8.637588\n","iter: 23360, loss: 0.018056, lr: 0.000001, time: 8.035645\n","iter: 23380, loss: 0.017927, lr: 0.000001, time: 8.732736\n","iter: 23400, loss: 0.025720, lr: 0.000001, time: 8.046908\n","iter: 23420, loss: 0.018643, lr: 0.000001, time: 8.794806\n","iter: 23440, loss: 0.023431, lr: 0.000001, time: 8.039098\n","iter: 23460, loss: 0.034566, lr: 0.000001, time: 8.788726\n","iter: 23480, loss: 0.012078, lr: 0.000001, time: 8.044496\n","iter: 23500, loss: 0.012125, lr: 0.000001, time: 8.787639\n","iter: 23520, loss: 0.013282, lr: 0.000001, time: 8.058267\n","iter: 23540, loss: 0.017731, lr: 0.000001, time: 8.778630\n","iter: 23560, loss: 0.008969, lr: 0.000001, time: 8.048796\n","iter: 23580, loss: 0.014371, lr: 0.000001, time: 8.771527\n","iter: 23600, loss: 0.029744, lr: 0.000001, time: 8.046646\n","iter: 23620, loss: 0.016586, lr: 0.000001, time: 8.779585\n","iter: 23640, loss: 0.017272, lr: 0.000001, time: 8.048717\n","iter: 23660, loss: 0.010770, lr: 0.000001, time: 8.791608\n","iter: 23680, loss: 0.012677, lr: 0.000001, time: 8.055112\n","iter: 23700, loss: 0.009968, lr: 0.000001, time: 8.794514\n","iter: 23720, loss: 0.023916, lr: 0.000001, time: 8.036732\n","iter: 23740, loss: 0.013492, lr: 0.000001, time: 8.757454\n","iter: 23760, loss: 0.018929, lr: 0.000001, time: 8.055916\n","iter: 23780, loss: 0.011081, lr: 0.000001, time: 8.751309\n","iter: 23800, loss: 0.016646, lr: 0.000001, time: 8.059618\n","iter: 23820, loss: 0.010127, lr: 0.000001, time: 8.011644\n","iter: 23840, loss: 0.021899, lr: 0.000001, time: 8.859190\n","iter: 23860, loss: 0.010905, lr: 0.000001, time: 8.020802\n","iter: 23880, loss: 0.013639, lr: 0.000001, time: 8.821795\n","iter: 23900, loss: 0.026067, lr: 0.000001, time: 8.009490\n","iter: 23920, loss: 0.019337, lr: 0.000001, time: 8.851119\n","iter: 23940, loss: 0.027109, lr: 0.000001, time: 8.025872\n","iter: 23960, loss: 0.011942, lr: 0.000001, time: 8.784994\n","iter: 23980, loss: 0.007336, lr: 0.000001, time: 8.026002\n","iter: 24000, loss: 0.010313, lr: 0.000001, time: 8.823379\n","iter: 24020, loss: 0.008884, lr: 0.000001, time: 8.026557\n","iter: 24040, loss: 0.006605, lr: 0.000001, time: 8.787606\n","iter: 24060, loss: 0.014101, lr: 0.000001, time: 8.041676\n","iter: 24080, loss: 0.007891, lr: 0.000001, time: 8.747078\n","iter: 24100, loss: 0.020215, lr: 0.000001, time: 8.045776\n","iter: 24120, loss: 0.021205, lr: 0.000001, time: 8.792042\n","iter: 24140, loss: 0.021868, lr: 0.000001, time: 8.045815\n","iter: 24160, loss: 0.026298, lr: 0.000001, time: 8.748888\n","iter: 24180, loss: 0.014802, lr: 0.000001, time: 8.043820\n","iter: 24200, loss: 0.009049, lr: 0.000001, time: 8.759632\n","iter: 24220, loss: 0.023934, lr: 0.000001, time: 8.039318\n","iter: 24240, loss: 0.030434, lr: 0.000001, time: 8.755922\n","iter: 24260, loss: 0.019123, lr: 0.000000, time: 8.049391\n","iter: 24280, loss: 0.030830, lr: 0.000000, time: 8.738115\n","iter: 24300, loss: 0.023407, lr: 0.000000, time: 8.064216\n","iter: 24320, loss: 0.011578, lr: 0.000000, time: 8.676150\n","iter: 24340, loss: 0.020629, lr: 0.000000, time: 8.035318\n","iter: 24360, loss: 0.030451, lr: 0.000000, time: 8.728520\n","iter: 24380, loss: 0.045032, lr: 0.000000, time: 8.033807\n","iter: 24400, loss: 0.013646, lr: 0.000000, time: 8.658259\n","iter: 24420, loss: 0.023367, lr: 0.000000, time: 8.010567\n","iter: 24440, loss: 0.019156, lr: 0.000000, time: 8.742183\n","iter: 24460, loss: 0.020016, lr: 0.000000, time: 8.051441\n","iter: 24480, loss: 0.013958, lr: 0.000000, time: 8.745420\n","iter: 24500, loss: 0.012629, lr: 0.000000, time: 8.042582\n","iter: 24520, loss: 0.013805, lr: 0.000000, time: 8.669934\n","iter: 24540, loss: 0.029081, lr: 0.000000, time: 8.042648\n","iter: 24560, loss: 0.015215, lr: 0.000000, time: 8.794309\n","iter: 24580, loss: 0.016842, lr: 0.000000, time: 8.044431\n","iter: 24600, loss: 0.015212, lr: 0.000000, time: 8.754104\n","iter: 24620, loss: 0.012292, lr: 0.000000, time: 8.068243\n","iter: 24640, loss: 0.017467, lr: 0.000000, time: 8.016461\n","iter: 24660, loss: 0.012877, lr: 0.000000, time: 8.776151\n","iter: 24680, loss: 0.042516, lr: 0.000000, time: 8.017212\n","iter: 24700, loss: 0.022468, lr: 0.000000, time: 8.868453\n","iter: 24720, loss: 0.012197, lr: 0.000000, time: 8.008856\n","iter: 24740, loss: 0.018639, lr: 0.000000, time: 8.678642\n","iter: 24760, loss: 0.035380, lr: 0.000000, time: 8.019093\n","iter: 24780, loss: 0.032616, lr: 0.000000, time: 8.789821\n","iter: 24800, loss: 0.011816, lr: 0.000000, time: 8.026786\n","iter: 24820, loss: 0.012525, lr: 0.000000, time: 8.820329\n","iter: 24840, loss: 0.023274, lr: 0.000000, time: 8.032913\n","iter: 24860, loss: 0.012057, lr: 0.000000, time: 8.767165\n","iter: 24880, loss: 0.016325, lr: 0.000000, time: 8.046846\n","iter: 24900, loss: 0.035064, lr: 0.000000, time: 8.741488\n","iter: 24920, loss: 0.015733, lr: 0.000000, time: 8.034456\n","iter: 24940, loss: 0.028525, lr: 0.000000, time: 8.803446\n","iter: 24960, loss: 0.013751, lr: 0.000000, time: 8.051247\n","iter: 24980, loss: 0.024553, lr: 0.000000, time: 8.756736\n","saving trained model\n","everything finished\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dnRVXN3uC_vQ","outputId":"0427794e-0bfb-4f4b-f9e6-5517468c2a3f","executionInfo":{"status":"ok","timestamp":1574292005994,"user_tz":-60,"elapsed":383064,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n","create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 105 / 106  ...   completed\n","=======>  processing iter 616 / 617  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3YPjE6kaC_vT","outputId":"d0d39bdc-e6fa-4d5b-dd8c-81052c8b8345","executionInfo":{"status":"ok","timestamp":1574292141878,"user_tz":-60,"elapsed":135856,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:09<00:00, 26.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.7507753063839253, cmc is: [0.88182896 0.9162708  0.9334917  0.9435867  0.95190024]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zXo-c3UDGWfS"},"source":["### Model 7\n","AA model without GAP filter size = 2"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VMgtefm2GWfU","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oKkQrFZOGWfX","colab":{}},"source":["model_num = 7"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9BBOnNiDGWfY","colab":{}},"source":["filter_size = 2\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HvK4kfhaGWfd","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1_bT7AhzGWfg","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embedding = embedding\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embedding(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"0db46bf0-1850-4b59-c2ed-cbf95f6733d5","executionInfo":{"status":"ok","timestamp":1574423686456,"user_tz":-60,"elapsed":1536,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"sWgPgwUYGWfi","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net = nn.DataParallel(model)\n","summary(net, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 64, 32]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 33, 17]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 33, 17]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91           [-1, 256, 17, 9]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95           [-1, 512, 17, 9]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157            [-1, 512, 9, 5]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161           [-1, 1024, 9, 5]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","          Linear-187                 [-1, 1024]       2,098,176\n","     BatchNorm1d-188                 [-1, 1024]           2,048\n","            ReLU-189                 [-1, 1024]               0\n","   DenseNormReLU-190                 [-1, 1024]               0\n","          Linear-191                  [-1, 128]         131,200\n","           Model-192                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 54.46\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 152.74\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KfYai-9UGWfk","outputId":"ee72b197-898b-40b9-ab1e-3266e5ac6407","executionInfo":{"status":"error","timestamp":1574423830836,"user_tz":-60,"elapsed":132586,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["train(net = net, model_num = model_num)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["creating optimizer\n","start training ...\n","iter: 20, loss: 1.448830, lr: 0.000300, time: 24.569105\n","iter: 40, loss: 1.275735, lr: 0.000300, time: 22.240314\n","iter: 60, loss: 0.997035, lr: 0.000300, time: 22.836311\n","iter: 80, loss: 0.826610, lr: 0.000300, time: 22.186513\n","iter: 100, loss: 0.759229, lr: 0.000300, time: 22.824560\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-9268b36008b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-c610138c4bb9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, model_num)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Sa1FJErpGWfm","colab":{}},"source":["create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n","create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V36PhvHpGWfo","colab":{}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O7ekpTe6I9mf"},"source":["### Model 8\n","AA model without GAP, filter size = 5"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UI_G-hdhI9mw","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xkeG_6swI9m3","colab":{}},"source":["model_num = 8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y__Q93TBI9m8","colab":{}},"source":["filter_size = 5\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6PUIOTIoI9nA","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embedding = nn.Linear(in_features = 1024, out_features = 128).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IPVLRocVI9nE","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embedding = embedding\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embedding(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zJ62EAMhI9nH","outputId":"6245067e-a3db-45e1-9258-cb9ad1bb20b4","executionInfo":{"status":"ok","timestamp":1574670327207,"user_tz":-60,"elapsed":1292,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net = nn.DataParallel(model)\n","summary(net, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 67, 35]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 36, 20]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 36, 20]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91          [-1, 256, 20, 12]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95          [-1, 512, 20, 12]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157           [-1, 512, 12, 8]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161          [-1, 1024, 12, 8]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","          Linear-187                 [-1, 1024]       2,098,176\n","     BatchNorm1d-188                 [-1, 1024]           2,048\n","            ReLU-189                 [-1, 1024]               0\n","   DenseNormReLU-190                 [-1, 1024]               0\n","          Linear-191                  [-1, 128]         131,200\n","           Model-192                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 56.18\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 154.46\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NmaAoqCBI9nM","outputId":"333bc68a-c285-4cef-c92f-ab02985c8ccd","executionInfo":{"status":"ok","timestamp":1574440762641,"user_tz":-60,"elapsed":11281790,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(net = net, model_num = model_num)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["creating optimizer\n","start training ...\n","iter: 20, loss: 1.457304, lr: 0.000300, time: 10.250914\n","iter: 40, loss: 1.288778, lr: 0.000300, time: 8.640670\n","iter: 60, loss: 1.116820, lr: 0.000300, time: 9.426859\n","iter: 80, loss: 1.031139, lr: 0.000300, time: 8.653302\n","iter: 100, loss: 0.908385, lr: 0.000300, time: 9.467355\n","iter: 120, loss: 0.893467, lr: 0.000300, time: 8.653279\n","iter: 140, loss: 0.822807, lr: 0.000300, time: 9.537549\n","iter: 160, loss: 0.800341, lr: 0.000300, time: 8.662960\n","iter: 180, loss: 0.729820, lr: 0.000300, time: 9.518260\n","iter: 200, loss: 0.761619, lr: 0.000300, time: 8.670058\n","iter: 220, loss: 0.744410, lr: 0.000300, time: 9.372985\n","iter: 240, loss: 0.712039, lr: 0.000300, time: 8.671847\n","iter: 260, loss: 0.734840, lr: 0.000300, time: 9.410328\n","iter: 280, loss: 0.708731, lr: 0.000300, time: 8.674650\n","iter: 300, loss: 0.695145, lr: 0.000300, time: 9.444432\n","iter: 320, loss: 0.713895, lr: 0.000300, time: 8.702021\n","iter: 340, loss: 0.725194, lr: 0.000300, time: 9.498778\n","iter: 360, loss: 0.708405, lr: 0.000300, time: 8.725347\n","iter: 380, loss: 0.685210, lr: 0.000300, time: 9.455861\n","iter: 400, loss: 0.682252, lr: 0.000300, time: 8.713680\n","iter: 420, loss: 0.654787, lr: 0.000300, time: 9.463487\n","iter: 440, loss: 0.722928, lr: 0.000300, time: 8.707155\n","iter: 460, loss: 0.668517, lr: 0.000300, time: 9.490659\n","iter: 480, loss: 0.669568, lr: 0.000300, time: 8.704484\n","iter: 500, loss: 0.696025, lr: 0.000300, time: 9.437963\n","iter: 520, loss: 0.666883, lr: 0.000300, time: 8.715390\n","iter: 540, loss: 0.612968, lr: 0.000300, time: 9.450712\n","iter: 560, loss: 0.642538, lr: 0.000300, time: 8.709203\n","iter: 580, loss: 0.619231, lr: 0.000300, time: 9.426395\n","iter: 600, loss: 0.612322, lr: 0.000300, time: 8.689829\n","iter: 620, loss: 0.635001, lr: 0.000300, time: 9.443130\n","iter: 640, loss: 0.576028, lr: 0.000300, time: 8.705351\n","iter: 660, loss: 0.565455, lr: 0.000300, time: 9.532761\n","iter: 680, loss: 0.522570, lr: 0.000300, time: 8.734484\n","iter: 700, loss: 0.668978, lr: 0.000300, time: 9.468286\n","iter: 720, loss: 0.604918, lr: 0.000300, time: 8.712317\n","iter: 740, loss: 0.614504, lr: 0.000300, time: 9.450302\n","iter: 760, loss: 0.531841, lr: 0.000300, time: 8.723672\n","iter: 780, loss: 0.562069, lr: 0.000300, time: 9.458257\n","iter: 800, loss: 0.550060, lr: 0.000300, time: 8.717782\n","iter: 820, loss: 0.658632, lr: 0.000300, time: 9.442241\n","iter: 840, loss: 0.576581, lr: 0.000300, time: 8.733377\n","iter: 860, loss: 0.567351, lr: 0.000300, time: 8.651799\n","iter: 880, loss: 0.571923, lr: 0.000300, time: 9.539540\n","iter: 900, loss: 0.542166, lr: 0.000300, time: 8.678239\n","iter: 920, loss: 0.456823, lr: 0.000300, time: 9.528427\n","iter: 940, loss: 0.501822, lr: 0.000300, time: 8.659983\n","iter: 960, loss: 0.481787, lr: 0.000300, time: 9.523883\n","iter: 980, loss: 0.593111, lr: 0.000300, time: 8.696386\n","iter: 1000, loss: 0.509881, lr: 0.000300, time: 9.497870\n","iter: 1020, loss: 0.496722, lr: 0.000300, time: 8.692229\n","iter: 1040, loss: 0.453336, lr: 0.000300, time: 9.531277\n","iter: 1060, loss: 0.488103, lr: 0.000300, time: 8.668949\n","iter: 1080, loss: 0.459319, lr: 0.000300, time: 9.379504\n","iter: 1100, loss: 0.480926, lr: 0.000300, time: 8.672091\n","iter: 1120, loss: 0.480036, lr: 0.000300, time: 9.508111\n","iter: 1140, loss: 0.524870, lr: 0.000300, time: 8.718796\n","iter: 1160, loss: 0.428916, lr: 0.000300, time: 9.500068\n","iter: 1180, loss: 0.476735, lr: 0.000300, time: 8.725481\n","iter: 1200, loss: 0.473496, lr: 0.000300, time: 9.458133\n","iter: 1220, loss: 0.429607, lr: 0.000300, time: 8.709910\n","iter: 1240, loss: 0.440553, lr: 0.000300, time: 9.493709\n","iter: 1260, loss: 0.466066, lr: 0.000300, time: 8.705757\n","iter: 1280, loss: 0.416509, lr: 0.000300, time: 9.495073\n","iter: 1300, loss: 0.454392, lr: 0.000300, time: 8.718796\n","iter: 1320, loss: 0.460816, lr: 0.000300, time: 9.393163\n","iter: 1340, loss: 0.487721, lr: 0.000300, time: 8.701086\n","iter: 1360, loss: 0.471900, lr: 0.000300, time: 9.484286\n","iter: 1380, loss: 0.429853, lr: 0.000300, time: 8.710510\n","iter: 1400, loss: 0.419674, lr: 0.000300, time: 9.396496\n","iter: 1420, loss: 0.438712, lr: 0.000300, time: 8.706708\n","iter: 1440, loss: 0.418039, lr: 0.000300, time: 9.469341\n","iter: 1460, loss: 0.457049, lr: 0.000300, time: 8.703443\n","iter: 1480, loss: 0.354818, lr: 0.000300, time: 9.496246\n","iter: 1500, loss: 0.428590, lr: 0.000300, time: 8.714679\n","iter: 1520, loss: 0.345679, lr: 0.000300, time: 9.411864\n","iter: 1540, loss: 0.373461, lr: 0.000300, time: 8.717192\n","iter: 1560, loss: 0.372392, lr: 0.000300, time: 9.415436\n","iter: 1580, loss: 0.440355, lr: 0.000300, time: 8.750448\n","iter: 1600, loss: 0.481940, lr: 0.000300, time: 9.460352\n","iter: 1620, loss: 0.462560, lr: 0.000300, time: 8.713754\n","iter: 1640, loss: 0.369462, lr: 0.000300, time: 9.464464\n","iter: 1660, loss: 0.343828, lr: 0.000300, time: 8.727687\n","iter: 1680, loss: 0.351794, lr: 0.000300, time: 8.658469\n","iter: 1700, loss: 0.356004, lr: 0.000300, time: 9.454478\n","iter: 1720, loss: 0.367628, lr: 0.000300, time: 8.690352\n","iter: 1740, loss: 0.348991, lr: 0.000300, time: 9.423920\n","iter: 1760, loss: 0.369631, lr: 0.000300, time: 8.687109\n","iter: 1780, loss: 0.400962, lr: 0.000300, time: 9.452136\n","iter: 1800, loss: 0.301653, lr: 0.000300, time: 8.683408\n","iter: 1820, loss: 0.283414, lr: 0.000300, time: 9.535485\n","iter: 1840, loss: 0.347855, lr: 0.000300, time: 8.702960\n","iter: 1860, loss: 0.326169, lr: 0.000300, time: 9.473454\n","iter: 1880, loss: 0.341705, lr: 0.000300, time: 8.697820\n","iter: 1900, loss: 0.300634, lr: 0.000300, time: 9.537451\n","iter: 1920, loss: 0.383000, lr: 0.000300, time: 8.723102\n","iter: 1940, loss: 0.321767, lr: 0.000300, time: 9.524750\n","iter: 1960, loss: 0.311730, lr: 0.000300, time: 8.716287\n","iter: 1980, loss: 0.295468, lr: 0.000300, time: 9.484961\n","iter: 2000, loss: 0.381138, lr: 0.000300, time: 8.718717\n","iter: 2020, loss: 0.320217, lr: 0.000300, time: 9.410866\n","iter: 2040, loss: 0.365416, lr: 0.000300, time: 8.727185\n","iter: 2060, loss: 0.340179, lr: 0.000300, time: 9.367877\n","iter: 2080, loss: 0.321907, lr: 0.000300, time: 8.740164\n","iter: 2100, loss: 0.376023, lr: 0.000300, time: 9.441037\n","iter: 2120, loss: 0.329219, lr: 0.000300, time: 8.732495\n","iter: 2140, loss: 0.325906, lr: 0.000300, time: 9.453409\n","iter: 2160, loss: 0.292936, lr: 0.000300, time: 8.784389\n","iter: 2180, loss: 0.323564, lr: 0.000300, time: 9.458985\n","iter: 2200, loss: 0.323901, lr: 0.000300, time: 8.723015\n","iter: 2220, loss: 0.358907, lr: 0.000300, time: 9.441000\n","iter: 2240, loss: 0.302622, lr: 0.000300, time: 8.728371\n","iter: 2260, loss: 0.308369, lr: 0.000300, time: 9.534115\n","iter: 2280, loss: 0.330392, lr: 0.000300, time: 8.710065\n","iter: 2300, loss: 0.292404, lr: 0.000300, time: 9.460507\n","iter: 2320, loss: 0.291871, lr: 0.000300, time: 8.718557\n","iter: 2340, loss: 0.268282, lr: 0.000300, time: 9.515548\n","iter: 2360, loss: 0.264656, lr: 0.000300, time: 8.718383\n","iter: 2380, loss: 0.324886, lr: 0.000300, time: 9.479893\n","iter: 2400, loss: 0.320347, lr: 0.000300, time: 8.735792\n","iter: 2420, loss: 0.366609, lr: 0.000300, time: 9.466487\n","iter: 2440, loss: 0.340918, lr: 0.000300, time: 8.717231\n","iter: 2460, loss: 0.328533, lr: 0.000300, time: 9.366771\n","iter: 2480, loss: 0.317891, lr: 0.000300, time: 8.750134\n","iter: 2500, loss: 0.362450, lr: 0.000300, time: 8.652626\n","iter: 2520, loss: 0.328001, lr: 0.000300, time: 9.505653\n","iter: 2540, loss: 0.334602, lr: 0.000300, time: 8.674855\n","iter: 2560, loss: 0.334743, lr: 0.000300, time: 9.443255\n","iter: 2580, loss: 0.366770, lr: 0.000300, time: 8.712120\n","iter: 2600, loss: 0.331640, lr: 0.000300, time: 9.471332\n","iter: 2620, loss: 0.288961, lr: 0.000300, time: 8.680583\n","iter: 2640, loss: 0.272531, lr: 0.000300, time: 9.530929\n","iter: 2660, loss: 0.267659, lr: 0.000300, time: 8.709469\n","iter: 2680, loss: 0.309447, lr: 0.000300, time: 9.426516\n","iter: 2700, loss: 0.275882, lr: 0.000300, time: 8.706390\n","iter: 2720, loss: 0.289700, lr: 0.000300, time: 9.474282\n","iter: 2740, loss: 0.311858, lr: 0.000300, time: 8.694081\n","iter: 2760, loss: 0.266731, lr: 0.000300, time: 9.509969\n","iter: 2780, loss: 0.255285, lr: 0.000300, time: 8.713020\n","iter: 2800, loss: 0.286823, lr: 0.000300, time: 9.433111\n","iter: 2820, loss: 0.294893, lr: 0.000300, time: 8.701205\n","iter: 2840, loss: 0.256752, lr: 0.000300, time: 9.458654\n","iter: 2860, loss: 0.320208, lr: 0.000300, time: 8.706079\n","iter: 2880, loss: 0.262976, lr: 0.000300, time: 9.418920\n","iter: 2900, loss: 0.255295, lr: 0.000300, time: 8.729304\n","iter: 2920, loss: 0.305653, lr: 0.000300, time: 9.540189\n","iter: 2940, loss: 0.236929, lr: 0.000300, time: 8.707904\n","iter: 2960, loss: 0.248518, lr: 0.000300, time: 9.506727\n","iter: 2980, loss: 0.250587, lr: 0.000300, time: 8.721389\n","iter: 3000, loss: 0.258501, lr: 0.000300, time: 9.507388\n","iter: 3020, loss: 0.241650, lr: 0.000300, time: 8.741893\n","iter: 3040, loss: 0.273845, lr: 0.000300, time: 9.493652\n","iter: 3060, loss: 0.227021, lr: 0.000300, time: 8.732619\n","iter: 3080, loss: 0.288593, lr: 0.000300, time: 9.534448\n","iter: 3100, loss: 0.341058, lr: 0.000300, time: 8.730404\n","iter: 3120, loss: 0.239006, lr: 0.000300, time: 9.563721\n","iter: 3140, loss: 0.295103, lr: 0.000300, time: 8.733112\n","iter: 3160, loss: 0.289454, lr: 0.000300, time: 9.464252\n","iter: 3180, loss: 0.284471, lr: 0.000300, time: 8.730340\n","iter: 3200, loss: 0.259871, lr: 0.000300, time: 9.501905\n","iter: 3220, loss: 0.228095, lr: 0.000300, time: 8.696221\n","iter: 3240, loss: 0.264979, lr: 0.000300, time: 9.376595\n","iter: 3260, loss: 0.273269, lr: 0.000300, time: 8.719443\n","iter: 3280, loss: 0.255531, lr: 0.000300, time: 9.419507\n","iter: 3300, loss: 0.279092, lr: 0.000300, time: 8.728288\n","iter: 3320, loss: 0.271220, lr: 0.000300, time: 8.642524\n","iter: 3340, loss: 0.193034, lr: 0.000300, time: 9.463520\n","iter: 3360, loss: 0.249612, lr: 0.000300, time: 8.653758\n","iter: 3380, loss: 0.252083, lr: 0.000300, time: 9.360008\n","iter: 3400, loss: 0.203107, lr: 0.000300, time: 8.645463\n","iter: 3420, loss: 0.241574, lr: 0.000300, time: 9.500456\n","iter: 3440, loss: 0.243358, lr: 0.000300, time: 8.673369\n","iter: 3460, loss: 0.236423, lr: 0.000300, time: 9.521665\n","iter: 3480, loss: 0.278961, lr: 0.000300, time: 8.695058\n","iter: 3500, loss: 0.215815, lr: 0.000300, time: 9.504017\n","iter: 3520, loss: 0.219524, lr: 0.000300, time: 8.705995\n","iter: 3540, loss: 0.195223, lr: 0.000300, time: 9.458796\n","iter: 3560, loss: 0.232230, lr: 0.000300, time: 8.704681\n","iter: 3580, loss: 0.227864, lr: 0.000300, time: 9.520774\n","iter: 3600, loss: 0.278053, lr: 0.000300, time: 8.702326\n","iter: 3620, loss: 0.275463, lr: 0.000300, time: 9.488893\n","iter: 3640, loss: 0.231865, lr: 0.000300, time: 8.717385\n","iter: 3660, loss: 0.182172, lr: 0.000300, time: 9.485810\n","iter: 3680, loss: 0.213138, lr: 0.000300, time: 8.725006\n","iter: 3700, loss: 0.224502, lr: 0.000300, time: 9.448813\n","iter: 3720, loss: 0.291251, lr: 0.000300, time: 8.739287\n","iter: 3740, loss: 0.266241, lr: 0.000300, time: 9.592726\n","iter: 3760, loss: 0.252867, lr: 0.000300, time: 8.736848\n","iter: 3780, loss: 0.272547, lr: 0.000300, time: 9.458122\n","iter: 3800, loss: 0.305718, lr: 0.000300, time: 8.728351\n","iter: 3820, loss: 0.195405, lr: 0.000300, time: 9.500247\n","iter: 3840, loss: 0.207022, lr: 0.000300, time: 8.724299\n","iter: 3860, loss: 0.197050, lr: 0.000300, time: 9.515454\n","iter: 3880, loss: 0.227137, lr: 0.000300, time: 8.723201\n","iter: 3900, loss: 0.187678, lr: 0.000300, time: 9.383708\n","iter: 3920, loss: 0.210500, lr: 0.000300, time: 8.733989\n","iter: 3940, loss: 0.184888, lr: 0.000300, time: 9.439236\n","iter: 3960, loss: 0.220917, lr: 0.000300, time: 8.727185\n","iter: 3980, loss: 0.166254, lr: 0.000300, time: 9.484576\n","iter: 4000, loss: 0.180592, lr: 0.000300, time: 8.711972\n","iter: 4020, loss: 0.239468, lr: 0.000300, time: 9.520965\n","iter: 4040, loss: 0.206149, lr: 0.000300, time: 8.758658\n","iter: 4060, loss: 0.243855, lr: 0.000300, time: 9.523309\n","iter: 4080, loss: 0.260526, lr: 0.000300, time: 8.737701\n","iter: 4100, loss: 0.255743, lr: 0.000300, time: 9.432276\n","iter: 4120, loss: 0.210573, lr: 0.000300, time: 8.747871\n","iter: 4140, loss: 0.232496, lr: 0.000300, time: 8.655272\n","iter: 4160, loss: 0.212177, lr: 0.000300, time: 9.567263\n","iter: 4180, loss: 0.198404, lr: 0.000300, time: 8.659994\n","iter: 4200, loss: 0.202925, lr: 0.000300, time: 9.475297\n","iter: 4220, loss: 0.255644, lr: 0.000300, time: 8.684951\n","iter: 4240, loss: 0.210858, lr: 0.000300, time: 9.472641\n","iter: 4260, loss: 0.203079, lr: 0.000300, time: 8.671050\n","iter: 4280, loss: 0.191671, lr: 0.000300, time: 9.578800\n","iter: 4300, loss: 0.267184, lr: 0.000300, time: 8.686892\n","iter: 4320, loss: 0.228900, lr: 0.000300, time: 9.468995\n","iter: 4340, loss: 0.257728, lr: 0.000300, time: 8.707117\n","iter: 4360, loss: 0.260174, lr: 0.000300, time: 9.562570\n","iter: 4380, loss: 0.183862, lr: 0.000300, time: 8.683472\n","iter: 4400, loss: 0.189429, lr: 0.000300, time: 9.491399\n","iter: 4420, loss: 0.232907, lr: 0.000300, time: 8.721562\n","iter: 4440, loss: 0.216159, lr: 0.000300, time: 9.376474\n","iter: 4460, loss: 0.206277, lr: 0.000300, time: 8.749780\n","iter: 4480, loss: 0.236279, lr: 0.000300, time: 9.451859\n","iter: 4500, loss: 0.211368, lr: 0.000300, time: 8.710779\n","iter: 4520, loss: 0.191206, lr: 0.000300, time: 9.415441\n","iter: 4540, loss: 0.254805, lr: 0.000300, time: 8.709430\n","iter: 4560, loss: 0.219567, lr: 0.000300, time: 9.390542\n","iter: 4580, loss: 0.229920, lr: 0.000300, time: 8.714111\n","iter: 4600, loss: 0.208384, lr: 0.000300, time: 9.543335\n","iter: 4620, loss: 0.199370, lr: 0.000300, time: 8.722553\n","iter: 4640, loss: 0.224720, lr: 0.000300, time: 9.444250\n","iter: 4660, loss: 0.154069, lr: 0.000300, time: 8.714026\n","iter: 4680, loss: 0.193380, lr: 0.000300, time: 9.397494\n","iter: 4700, loss: 0.188626, lr: 0.000300, time: 8.716583\n","iter: 4720, loss: 0.205004, lr: 0.000300, time: 9.439652\n","iter: 4740, loss: 0.242285, lr: 0.000300, time: 8.705716\n","iter: 4760, loss: 0.165503, lr: 0.000300, time: 9.494032\n","iter: 4780, loss: 0.191970, lr: 0.000300, time: 8.711074\n","iter: 4800, loss: 0.190890, lr: 0.000300, time: 9.387758\n","iter: 4820, loss: 0.164930, lr: 0.000300, time: 8.702613\n","iter: 4840, loss: 0.215945, lr: 0.000300, time: 9.465758\n","iter: 4860, loss: 0.192852, lr: 0.000300, time: 8.694999\n","iter: 4880, loss: 0.188160, lr: 0.000300, time: 9.410038\n","iter: 4900, loss: 0.214720, lr: 0.000300, time: 8.714341\n","iter: 4920, loss: 0.164132, lr: 0.000300, time: 9.448064\n","iter: 4940, loss: 0.159728, lr: 0.000300, time: 8.755625\n","iter: 4960, loss: 0.175885, lr: 0.000300, time: 8.674895\n","iter: 4980, loss: 0.155897, lr: 0.000300, time: 9.521756\n","iter: 5000, loss: 0.163348, lr: 0.000300, time: 8.667663\n","iter: 5020, loss: 0.152336, lr: 0.000300, time: 9.427392\n","iter: 5040, loss: 0.159214, lr: 0.000300, time: 8.685360\n","iter: 5060, loss: 0.207010, lr: 0.000300, time: 9.505517\n","iter: 5080, loss: 0.203461, lr: 0.000300, time: 8.686294\n","iter: 5100, loss: 0.205268, lr: 0.000300, time: 9.429102\n","iter: 5120, loss: 0.187370, lr: 0.000300, time: 8.693391\n","iter: 5140, loss: 0.204522, lr: 0.000300, time: 9.574737\n","iter: 5160, loss: 0.193201, lr: 0.000300, time: 8.711178\n","iter: 5180, loss: 0.207643, lr: 0.000300, time: 9.524272\n","iter: 5200, loss: 0.149683, lr: 0.000300, time: 8.714986\n","iter: 5220, loss: 0.178455, lr: 0.000300, time: 9.481364\n","iter: 5240, loss: 0.273924, lr: 0.000300, time: 8.714015\n","iter: 5260, loss: 0.231034, lr: 0.000300, time: 9.405027\n","iter: 5280, loss: 0.200152, lr: 0.000300, time: 8.726768\n","iter: 5300, loss: 0.164953, lr: 0.000300, time: 9.399411\n","iter: 5320, loss: 0.244606, lr: 0.000300, time: 8.722049\n","iter: 5340, loss: 0.194536, lr: 0.000300, time: 9.472814\n","iter: 5360, loss: 0.177807, lr: 0.000300, time: 8.712116\n","iter: 5380, loss: 0.165759, lr: 0.000300, time: 9.488196\n","iter: 5400, loss: 0.190522, lr: 0.000300, time: 8.763515\n","iter: 5420, loss: 0.156003, lr: 0.000300, time: 9.463013\n","iter: 5440, loss: 0.184586, lr: 0.000300, time: 8.700428\n","iter: 5460, loss: 0.180428, lr: 0.000300, time: 9.512098\n","iter: 5480, loss: 0.156734, lr: 0.000300, time: 8.735267\n","iter: 5500, loss: 0.145112, lr: 0.000300, time: 9.344620\n","iter: 5520, loss: 0.161144, lr: 0.000300, time: 8.721237\n","iter: 5540, loss: 0.204386, lr: 0.000300, time: 9.412287\n","iter: 5560, loss: 0.203486, lr: 0.000300, time: 8.741587\n","iter: 5580, loss: 0.149693, lr: 0.000300, time: 9.453308\n","iter: 5600, loss: 0.182222, lr: 0.000300, time: 8.757679\n","iter: 5620, loss: 0.144040, lr: 0.000300, time: 9.487837\n","iter: 5640, loss: 0.160325, lr: 0.000300, time: 8.758008\n","iter: 5660, loss: 0.176766, lr: 0.000300, time: 9.339679\n","iter: 5680, loss: 0.196313, lr: 0.000300, time: 8.720441\n","iter: 5700, loss: 0.236801, lr: 0.000300, time: 9.475959\n","iter: 5720, loss: 0.201134, lr: 0.000300, time: 8.732810\n","iter: 5740, loss: 0.159355, lr: 0.000300, time: 9.483334\n","iter: 5760, loss: 0.114211, lr: 0.000300, time: 8.758764\n","iter: 5780, loss: 0.181740, lr: 0.000300, time: 8.674033\n","iter: 5800, loss: 0.116476, lr: 0.000300, time: 9.539883\n","iter: 5820, loss: 0.172155, lr: 0.000300, time: 8.676311\n","iter: 5840, loss: 0.203871, lr: 0.000300, time: 9.492081\n","iter: 5860, loss: 0.156032, lr: 0.000300, time: 8.663802\n","iter: 5880, loss: 0.163300, lr: 0.000300, time: 9.502763\n","iter: 5900, loss: 0.145444, lr: 0.000300, time: 8.693684\n","iter: 5920, loss: 0.132901, lr: 0.000300, time: 9.577935\n","iter: 5940, loss: 0.210388, lr: 0.000300, time: 8.701280\n","iter: 5960, loss: 0.169855, lr: 0.000300, time: 9.563732\n","iter: 5980, loss: 0.164787, lr: 0.000300, time: 8.684755\n","iter: 6000, loss: 0.172741, lr: 0.000300, time: 9.443803\n","iter: 6020, loss: 0.138968, lr: 0.000300, time: 8.696030\n","iter: 6040, loss: 0.139430, lr: 0.000300, time: 9.441733\n","iter: 6060, loss: 0.139660, lr: 0.000300, time: 8.701005\n","iter: 6080, loss: 0.152041, lr: 0.000300, time: 9.533712\n","iter: 6100, loss: 0.207267, lr: 0.000300, time: 8.715117\n","iter: 6120, loss: 0.196016, lr: 0.000300, time: 9.394979\n","iter: 6140, loss: 0.170557, lr: 0.000300, time: 8.735040\n","iter: 6160, loss: 0.169385, lr: 0.000300, time: 9.377400\n","iter: 6180, loss: 0.117023, lr: 0.000300, time: 8.738966\n","iter: 6200, loss: 0.163363, lr: 0.000300, time: 9.513297\n","iter: 6220, loss: 0.156408, lr: 0.000300, time: 8.733322\n","iter: 6240, loss: 0.148383, lr: 0.000300, time: 9.387822\n","iter: 6260, loss: 0.219270, lr: 0.000300, time: 8.737958\n","iter: 6280, loss: 0.130569, lr: 0.000300, time: 9.532188\n","iter: 6300, loss: 0.202050, lr: 0.000300, time: 8.718085\n","iter: 6320, loss: 0.166537, lr: 0.000300, time: 9.404946\n","iter: 6340, loss: 0.170857, lr: 0.000300, time: 8.723999\n","iter: 6360, loss: 0.122602, lr: 0.000300, time: 9.448282\n","iter: 6380, loss: 0.222543, lr: 0.000300, time: 8.721150\n","iter: 6400, loss: 0.153090, lr: 0.000300, time: 9.489878\n","iter: 6420, loss: 0.162686, lr: 0.000300, time: 8.716179\n","iter: 6440, loss: 0.187248, lr: 0.000300, time: 9.454955\n","iter: 6460, loss: 0.174276, lr: 0.000300, time: 8.725481\n","iter: 6480, loss: 0.150965, lr: 0.000300, time: 9.341096\n","iter: 6500, loss: 0.129057, lr: 0.000300, time: 8.739784\n","iter: 6520, loss: 0.202186, lr: 0.000300, time: 9.366457\n","iter: 6540, loss: 0.127313, lr: 0.000300, time: 8.722936\n","iter: 6560, loss: 0.173959, lr: 0.000300, time: 9.481510\n","iter: 6580, loss: 0.154055, lr: 0.000300, time: 8.741204\n","iter: 6600, loss: 0.144842, lr: 0.000300, time: 8.666043\n","iter: 6620, loss: 0.145146, lr: 0.000300, time: 9.584257\n","iter: 6640, loss: 0.202134, lr: 0.000300, time: 8.673295\n","iter: 6660, loss: 0.136828, lr: 0.000300, time: 9.487335\n","iter: 6680, loss: 0.193609, lr: 0.000300, time: 8.665303\n","iter: 6700, loss: 0.131074, lr: 0.000300, time: 9.495427\n","iter: 6720, loss: 0.162768, lr: 0.000300, time: 8.669843\n","iter: 6740, loss: 0.155581, lr: 0.000300, time: 9.477965\n","iter: 6760, loss: 0.172617, lr: 0.000300, time: 8.714684\n","iter: 6780, loss: 0.156537, lr: 0.000300, time: 9.519470\n","iter: 6800, loss: 0.133900, lr: 0.000300, time: 8.686693\n","iter: 6820, loss: 0.208215, lr: 0.000300, time: 9.512390\n","iter: 6840, loss: 0.174975, lr: 0.000300, time: 8.700010\n","iter: 6860, loss: 0.158909, lr: 0.000300, time: 9.548904\n","iter: 6880, loss: 0.169320, lr: 0.000300, time: 8.714855\n","iter: 6900, loss: 0.141516, lr: 0.000300, time: 9.445730\n","iter: 6920, loss: 0.160728, lr: 0.000300, time: 8.709021\n","iter: 6940, loss: 0.161975, lr: 0.000300, time: 9.479855\n","iter: 6960, loss: 0.133125, lr: 0.000300, time: 8.727633\n","iter: 6980, loss: 0.159291, lr: 0.000300, time: 9.450950\n","iter: 7000, loss: 0.127771, lr: 0.000300, time: 8.738276\n","iter: 7020, loss: 0.164510, lr: 0.000300, time: 9.523880\n","iter: 7040, loss: 0.174518, lr: 0.000300, time: 8.740040\n","iter: 7060, loss: 0.168758, lr: 0.000300, time: 9.497057\n","iter: 7080, loss: 0.115510, lr: 0.000300, time: 8.709908\n","iter: 7100, loss: 0.113598, lr: 0.000300, time: 9.444168\n","iter: 7120, loss: 0.157818, lr: 0.000300, time: 8.732472\n","iter: 7140, loss: 0.143617, lr: 0.000300, time: 9.495052\n","iter: 7160, loss: 0.166295, lr: 0.000300, time: 8.695651\n","iter: 7180, loss: 0.155041, lr: 0.000300, time: 9.533711\n","iter: 7200, loss: 0.110684, lr: 0.000300, time: 8.741749\n","iter: 7220, loss: 0.102070, lr: 0.000300, time: 9.511365\n","iter: 7240, loss: 0.108519, lr: 0.000300, time: 8.717120\n","iter: 7260, loss: 0.123153, lr: 0.000300, time: 9.516353\n","iter: 7280, loss: 0.099850, lr: 0.000300, time: 8.740485\n","iter: 7300, loss: 0.128221, lr: 0.000300, time: 9.451064\n","iter: 7320, loss: 0.138964, lr: 0.000300, time: 8.730545\n","iter: 7340, loss: 0.129090, lr: 0.000300, time: 9.569006\n","iter: 7360, loss: 0.163879, lr: 0.000300, time: 8.724329\n","iter: 7380, loss: 0.159552, lr: 0.000300, time: 9.392723\n","iter: 7400, loss: 0.099752, lr: 0.000300, time: 8.780858\n","iter: 7420, loss: 0.188302, lr: 0.000300, time: 8.649507\n","iter: 7440, loss: 0.148425, lr: 0.000300, time: 9.566393\n","iter: 7460, loss: 0.129367, lr: 0.000300, time: 8.686123\n","iter: 7480, loss: 0.163695, lr: 0.000300, time: 9.504194\n","iter: 7500, loss: 0.116368, lr: 0.000300, time: 8.690115\n","iter: 7520, loss: 0.160960, lr: 0.000300, time: 9.557795\n","iter: 7540, loss: 0.134553, lr: 0.000300, time: 8.697210\n","iter: 7560, loss: 0.141823, lr: 0.000300, time: 9.551188\n","iter: 7580, loss: 0.123711, lr: 0.000300, time: 8.681910\n","iter: 7600, loss: 0.165968, lr: 0.000300, time: 9.566923\n","iter: 7620, loss: 0.170215, lr: 0.000300, time: 8.690613\n","iter: 7640, loss: 0.142516, lr: 0.000300, time: 9.476579\n","iter: 7660, loss: 0.160291, lr: 0.000300, time: 8.704513\n","iter: 7680, loss: 0.123650, lr: 0.000300, time: 9.453620\n","iter: 7700, loss: 0.107797, lr: 0.000300, time: 8.716447\n","iter: 7720, loss: 0.145689, lr: 0.000300, time: 9.564639\n","iter: 7740, loss: 0.156746, lr: 0.000300, time: 8.745898\n","iter: 7760, loss: 0.139440, lr: 0.000300, time: 9.422111\n","iter: 7780, loss: 0.166056, lr: 0.000300, time: 8.747263\n","iter: 7800, loss: 0.156651, lr: 0.000300, time: 9.510120\n","iter: 7820, loss: 0.148532, lr: 0.000300, time: 8.716301\n","iter: 7840, loss: 0.100459, lr: 0.000300, time: 9.418732\n","iter: 7860, loss: 0.125715, lr: 0.000300, time: 8.726859\n","iter: 7880, loss: 0.169908, lr: 0.000300, time: 9.486444\n","iter: 7900, loss: 0.201868, lr: 0.000300, time: 8.702661\n","iter: 7920, loss: 0.185293, lr: 0.000300, time: 9.562227\n","iter: 7940, loss: 0.172377, lr: 0.000300, time: 8.720508\n","iter: 7960, loss: 0.114340, lr: 0.000300, time: 9.364922\n","iter: 7980, loss: 0.150699, lr: 0.000300, time: 8.716990\n","iter: 8000, loss: 0.128632, lr: 0.000300, time: 9.489878\n","iter: 8020, loss: 0.138164, lr: 0.000300, time: 8.721096\n","iter: 8040, loss: 0.103734, lr: 0.000300, time: 9.315399\n","iter: 8060, loss: 0.118419, lr: 0.000300, time: 8.705002\n","iter: 8080, loss: 0.145627, lr: 0.000300, time: 9.411503\n","iter: 8100, loss: 0.178784, lr: 0.000300, time: 8.706311\n","iter: 8120, loss: 0.110422, lr: 0.000300, time: 9.391544\n","iter: 8140, loss: 0.128483, lr: 0.000300, time: 8.706148\n","iter: 8160, loss: 0.185373, lr: 0.000300, time: 9.418680\n","iter: 8180, loss: 0.132255, lr: 0.000300, time: 8.739957\n","iter: 8200, loss: 0.144398, lr: 0.000300, time: 9.427954\n","iter: 8220, loss: 0.164829, lr: 0.000300, time: 8.743734\n","iter: 8240, loss: 0.135565, lr: 0.000300, time: 8.641365\n","iter: 8260, loss: 0.129421, lr: 0.000300, time: 9.580435\n","iter: 8280, loss: 0.118595, lr: 0.000300, time: 8.660240\n","iter: 8300, loss: 0.130364, lr: 0.000300, time: 9.492717\n","iter: 8320, loss: 0.173568, lr: 0.000300, time: 8.723766\n","iter: 8340, loss: 0.140057, lr: 0.000300, time: 9.533304\n","iter: 8360, loss: 0.128681, lr: 0.000300, time: 8.670449\n","iter: 8380, loss: 0.129486, lr: 0.000300, time: 9.421554\n","iter: 8400, loss: 0.168389, lr: 0.000300, time: 8.672869\n","iter: 8420, loss: 0.103544, lr: 0.000300, time: 9.505352\n","iter: 8440, loss: 0.161589, lr: 0.000300, time: 8.700296\n","iter: 8460, loss: 0.156440, lr: 0.000300, time: 9.421491\n","iter: 8480, loss: 0.142096, lr: 0.000300, time: 8.690002\n","iter: 8500, loss: 0.160139, lr: 0.000300, time: 9.473439\n","iter: 8520, loss: 0.122327, lr: 0.000300, time: 8.706966\n","iter: 8540, loss: 0.116737, lr: 0.000300, time: 9.426992\n","iter: 8560, loss: 0.094180, lr: 0.000300, time: 8.713238\n","iter: 8580, loss: 0.105158, lr: 0.000300, time: 9.404275\n","iter: 8600, loss: 0.146641, lr: 0.000300, time: 8.709758\n","iter: 8620, loss: 0.096081, lr: 0.000300, time: 9.359048\n","iter: 8640, loss: 0.157677, lr: 0.000300, time: 8.741819\n","iter: 8660, loss: 0.173703, lr: 0.000300, time: 9.379963\n","iter: 8680, loss: 0.147125, lr: 0.000300, time: 8.707026\n","iter: 8700, loss: 0.171887, lr: 0.000300, time: 9.439374\n","iter: 8720, loss: 0.124849, lr: 0.000300, time: 8.714316\n","iter: 8740, loss: 0.138099, lr: 0.000300, time: 9.413566\n","iter: 8760, loss: 0.119330, lr: 0.000300, time: 8.690848\n","iter: 8780, loss: 0.095563, lr: 0.000300, time: 9.416075\n","iter: 8800, loss: 0.110828, lr: 0.000300, time: 8.708274\n","iter: 8820, loss: 0.134472, lr: 0.000300, time: 9.408639\n","iter: 8840, loss: 0.135640, lr: 0.000300, time: 8.691453\n","iter: 8860, loss: 0.141786, lr: 0.000300, time: 9.316398\n","iter: 8880, loss: 0.101660, lr: 0.000300, time: 8.700011\n","iter: 8900, loss: 0.111911, lr: 0.000300, time: 9.402284\n","iter: 8920, loss: 0.131566, lr: 0.000300, time: 8.707968\n","iter: 8940, loss: 0.142808, lr: 0.000300, time: 9.410095\n","iter: 8960, loss: 0.109491, lr: 0.000300, time: 8.715517\n","iter: 8980, loss: 0.130793, lr: 0.000300, time: 9.395766\n","iter: 9000, loss: 0.134577, lr: 0.000300, time: 8.736449\n","iter: 9020, loss: 0.112695, lr: 0.000300, time: 9.451719\n","iter: 9040, loss: 0.132963, lr: 0.000300, time: 8.686035\n","iter: 9060, loss: 0.074402, lr: 0.000300, time: 8.649223\n","iter: 9080, loss: 0.152011, lr: 0.000300, time: 9.494120\n","iter: 9100, loss: 0.113824, lr: 0.000300, time: 8.660329\n","iter: 9120, loss: 0.142899, lr: 0.000300, time: 9.463269\n","iter: 9140, loss: 0.136943, lr: 0.000300, time: 8.665255\n","iter: 9160, loss: 0.155004, lr: 0.000300, time: 9.457105\n","iter: 9180, loss: 0.104423, lr: 0.000300, time: 8.677264\n","iter: 9200, loss: 0.098458, lr: 0.000300, time: 9.374564\n","iter: 9220, loss: 0.116635, lr: 0.000300, time: 8.669142\n","iter: 9240, loss: 0.098560, lr: 0.000300, time: 9.437979\n","iter: 9260, loss: 0.128731, lr: 0.000300, time: 8.682482\n","iter: 9280, loss: 0.121493, lr: 0.000300, time: 9.398617\n","iter: 9300, loss: 0.154249, lr: 0.000300, time: 8.683311\n","iter: 9320, loss: 0.095409, lr: 0.000300, time: 9.488601\n","iter: 9340, loss: 0.150792, lr: 0.000300, time: 8.678398\n","iter: 9360, loss: 0.125261, lr: 0.000300, time: 9.346339\n","iter: 9380, loss: 0.156014, lr: 0.000300, time: 8.689603\n","iter: 9400, loss: 0.105203, lr: 0.000300, time: 9.441544\n","iter: 9420, loss: 0.125794, lr: 0.000300, time: 8.692558\n","iter: 9440, loss: 0.105950, lr: 0.000300, time: 9.412829\n","iter: 9460, loss: 0.127110, lr: 0.000300, time: 8.709268\n","iter: 9480, loss: 0.141915, lr: 0.000300, time: 9.317989\n","iter: 9500, loss: 0.126673, lr: 0.000300, time: 8.694809\n","iter: 9520, loss: 0.138462, lr: 0.000300, time: 9.309794\n","iter: 9540, loss: 0.116020, lr: 0.000300, time: 8.673207\n","iter: 9560, loss: 0.143042, lr: 0.000300, time: 9.383388\n","iter: 9580, loss: 0.126420, lr: 0.000300, time: 8.692487\n","iter: 9600, loss: 0.100659, lr: 0.000300, time: 9.246628\n","iter: 9620, loss: 0.171516, lr: 0.000300, time: 8.700535\n","iter: 9640, loss: 0.155073, lr: 0.000300, time: 9.403713\n","iter: 9660, loss: 0.130526, lr: 0.000300, time: 8.666821\n","iter: 9680, loss: 0.110204, lr: 0.000300, time: 9.284549\n","iter: 9700, loss: 0.100566, lr: 0.000300, time: 8.677250\n","iter: 9720, loss: 0.094130, lr: 0.000300, time: 9.301496\n","iter: 9740, loss: 0.151653, lr: 0.000300, time: 8.693812\n","iter: 9760, loss: 0.067371, lr: 0.000300, time: 9.369011\n","iter: 9780, loss: 0.104434, lr: 0.000300, time: 8.670123\n","iter: 9800, loss: 0.093939, lr: 0.000300, time: 9.376983\n","iter: 9820, loss: 0.078073, lr: 0.000300, time: 8.701717\n","iter: 9840, loss: 0.124168, lr: 0.000300, time: 9.272735\n","iter: 9860, loss: 0.132926, lr: 0.000300, time: 8.725106\n","iter: 9880, loss: 0.099406, lr: 0.000300, time: 8.646945\n","iter: 9900, loss: 0.123849, lr: 0.000300, time: 9.374385\n","iter: 9920, loss: 0.095364, lr: 0.000300, time: 8.621969\n","iter: 9940, loss: 0.135308, lr: 0.000300, time: 9.447818\n","iter: 9960, loss: 0.145394, lr: 0.000300, time: 8.654624\n","iter: 9980, loss: 0.090155, lr: 0.000300, time: 9.309249\n","iter: 10000, loss: 0.120269, lr: 0.000300, time: 8.666893\n","iter: 10020, loss: 0.084109, lr: 0.000300, time: 9.331285\n","iter: 10040, loss: 0.096281, lr: 0.000300, time: 8.646315\n","iter: 10060, loss: 0.128875, lr: 0.000300, time: 9.391363\n","iter: 10080, loss: 0.070047, lr: 0.000300, time: 8.657911\n","iter: 10100, loss: 0.093199, lr: 0.000300, time: 9.376519\n","iter: 10120, loss: 0.087351, lr: 0.000300, time: 8.665658\n","iter: 10140, loss: 0.105169, lr: 0.000300, time: 9.440859\n","iter: 10160, loss: 0.113857, lr: 0.000300, time: 8.688660\n","iter: 10180, loss: 0.134556, lr: 0.000300, time: 9.426093\n","iter: 10200, loss: 0.151008, lr: 0.000300, time: 8.664189\n","iter: 10220, loss: 0.161325, lr: 0.000300, time: 9.432955\n","iter: 10240, loss: 0.116666, lr: 0.000300, time: 8.668009\n","iter: 10260, loss: 0.164615, lr: 0.000300, time: 9.304591\n","iter: 10280, loss: 0.107334, lr: 0.000300, time: 8.694242\n","iter: 10300, loss: 0.122144, lr: 0.000300, time: 9.416543\n","iter: 10320, loss: 0.095710, lr: 0.000300, time: 8.701451\n","iter: 10340, loss: 0.108991, lr: 0.000300, time: 9.443423\n","iter: 10360, loss: 0.113343, lr: 0.000300, time: 8.681068\n","iter: 10380, loss: 0.113454, lr: 0.000300, time: 9.408586\n","iter: 10400, loss: 0.102387, lr: 0.000300, time: 8.692485\n","iter: 10420, loss: 0.108030, lr: 0.000300, time: 9.460537\n","iter: 10440, loss: 0.111519, lr: 0.000300, time: 8.682171\n","iter: 10460, loss: 0.094407, lr: 0.000300, time: 9.330619\n","iter: 10480, loss: 0.146690, lr: 0.000300, time: 8.696041\n","iter: 10500, loss: 0.109794, lr: 0.000300, time: 9.331353\n","iter: 10520, loss: 0.098900, lr: 0.000300, time: 8.684449\n","iter: 10540, loss: 0.153729, lr: 0.000300, time: 9.323086\n","iter: 10560, loss: 0.119406, lr: 0.000300, time: 8.677376\n","iter: 10580, loss: 0.128043, lr: 0.000300, time: 9.292395\n","iter: 10600, loss: 0.137153, lr: 0.000300, time: 8.696985\n","iter: 10620, loss: 0.077647, lr: 0.000300, time: 9.427592\n","iter: 10640, loss: 0.109344, lr: 0.000300, time: 8.682029\n","iter: 10660, loss: 0.151688, lr: 0.000300, time: 9.391423\n","iter: 10680, loss: 0.124895, lr: 0.000300, time: 8.704943\n","iter: 10700, loss: 0.139482, lr: 0.000300, time: 8.646240\n","iter: 10720, loss: 0.163239, lr: 0.000300, time: 9.406928\n","iter: 10740, loss: 0.113558, lr: 0.000300, time: 8.646966\n","iter: 10760, loss: 0.103004, lr: 0.000300, time: 9.427188\n","iter: 10780, loss: 0.130414, lr: 0.000300, time: 8.658207\n","iter: 10800, loss: 0.113576, lr: 0.000300, time: 9.444127\n","iter: 10820, loss: 0.158200, lr: 0.000300, time: 8.660855\n","iter: 10840, loss: 0.116430, lr: 0.000300, time: 9.280321\n","iter: 10860, loss: 0.079389, lr: 0.000300, time: 8.671206\n","iter: 10880, loss: 0.107252, lr: 0.000300, time: 9.461548\n","iter: 10900, loss: 0.089595, lr: 0.000300, time: 8.659828\n","iter: 10920, loss: 0.086343, lr: 0.000300, time: 9.309860\n","iter: 10940, loss: 0.103806, lr: 0.000300, time: 8.669585\n","iter: 10960, loss: 0.097064, lr: 0.000300, time: 9.408286\n","iter: 10980, loss: 0.097822, lr: 0.000300, time: 8.677500\n","iter: 11000, loss: 0.120250, lr: 0.000300, time: 9.429136\n","iter: 11020, loss: 0.078311, lr: 0.000300, time: 8.683854\n","iter: 11040, loss: 0.108199, lr: 0.000300, time: 9.380174\n","iter: 11060, loss: 0.092596, lr: 0.000300, time: 8.711563\n","iter: 11080, loss: 0.113332, lr: 0.000300, time: 9.363244\n","iter: 11100, loss: 0.098934, lr: 0.000300, time: 8.695600\n","iter: 11120, loss: 0.091360, lr: 0.000300, time: 9.341892\n","iter: 11140, loss: 0.100015, lr: 0.000300, time: 8.695816\n","iter: 11160, loss: 0.108514, lr: 0.000300, time: 9.333456\n","iter: 11180, loss: 0.096547, lr: 0.000300, time: 8.700633\n","iter: 11200, loss: 0.084201, lr: 0.000300, time: 9.225928\n","iter: 11220, loss: 0.085688, lr: 0.000300, time: 8.683181\n","iter: 11240, loss: 0.128848, lr: 0.000300, time: 9.405983\n","iter: 11260, loss: 0.108332, lr: 0.000300, time: 8.687402\n","iter: 11280, loss: 0.094940, lr: 0.000300, time: 9.414645\n","iter: 11300, loss: 0.105150, lr: 0.000300, time: 8.682751\n","iter: 11320, loss: 0.139622, lr: 0.000300, time: 9.365003\n","iter: 11340, loss: 0.098998, lr: 0.000300, time: 8.702927\n","iter: 11360, loss: 0.111788, lr: 0.000300, time: 9.207229\n","iter: 11380, loss: 0.127626, lr: 0.000300, time: 8.685277\n","iter: 11400, loss: 0.144525, lr: 0.000300, time: 9.282373\n","iter: 11420, loss: 0.104797, lr: 0.000300, time: 8.691864\n","iter: 11440, loss: 0.129132, lr: 0.000300, time: 9.409313\n","iter: 11460, loss: 0.086840, lr: 0.000300, time: 8.690213\n","iter: 11480, loss: 0.107762, lr: 0.000300, time: 9.310237\n","iter: 11500, loss: 0.103268, lr: 0.000300, time: 8.750097\n","iter: 11520, loss: 0.087161, lr: 0.000300, time: 8.622374\n","iter: 11540, loss: 0.078493, lr: 0.000300, time: 9.445911\n","iter: 11560, loss: 0.099483, lr: 0.000300, time: 8.642179\n","iter: 11580, loss: 0.115257, lr: 0.000300, time: 9.414475\n","iter: 11600, loss: 0.117990, lr: 0.000300, time: 8.641171\n","iter: 11620, loss: 0.125065, lr: 0.000300, time: 9.371203\n","iter: 11640, loss: 0.108868, lr: 0.000300, time: 8.657635\n","iter: 11660, loss: 0.081500, lr: 0.000300, time: 9.403630\n","iter: 11680, loss: 0.109688, lr: 0.000300, time: 8.661000\n","iter: 11700, loss: 0.119798, lr: 0.000300, time: 9.383235\n","iter: 11720, loss: 0.121794, lr: 0.000300, time: 8.677252\n","iter: 11740, loss: 0.104578, lr: 0.000300, time: 9.373259\n","iter: 11760, loss: 0.080790, lr: 0.000300, time: 8.670431\n","iter: 11780, loss: 0.136970, lr: 0.000300, time: 9.377651\n","iter: 11800, loss: 0.138545, lr: 0.000300, time: 8.683663\n","iter: 11820, loss: 0.108405, lr: 0.000300, time: 9.304094\n","iter: 11840, loss: 0.150233, lr: 0.000300, time: 8.673228\n","iter: 11860, loss: 0.110643, lr: 0.000300, time: 9.347472\n","iter: 11880, loss: 0.105050, lr: 0.000300, time: 8.689220\n","iter: 11900, loss: 0.116576, lr: 0.000300, time: 9.260811\n","iter: 11920, loss: 0.102340, lr: 0.000300, time: 8.688968\n","iter: 11940, loss: 0.100183, lr: 0.000300, time: 9.279102\n","iter: 11960, loss: 0.164969, lr: 0.000300, time: 8.680902\n","iter: 11980, loss: 0.133789, lr: 0.000300, time: 9.427256\n","iter: 12000, loss: 0.124985, lr: 0.000300, time: 8.733089\n","iter: 12020, loss: 0.091410, lr: 0.000300, time: 9.511809\n","iter: 12040, loss: 0.096655, lr: 0.000300, time: 8.704444\n","iter: 12060, loss: 0.123549, lr: 0.000300, time: 9.296085\n","iter: 12080, loss: 0.100265, lr: 0.000300, time: 8.678442\n","iter: 12100, loss: 0.074779, lr: 0.000300, time: 9.341540\n","iter: 12120, loss: 0.112581, lr: 0.000300, time: 8.675625\n","iter: 12140, loss: 0.117015, lr: 0.000300, time: 9.364926\n","iter: 12160, loss: 0.096612, lr: 0.000300, time: 8.686587\n","iter: 12180, loss: 0.077985, lr: 0.000300, time: 9.334312\n","iter: 12200, loss: 0.113665, lr: 0.000300, time: 8.684211\n","iter: 12220, loss: 0.092126, lr: 0.000300, time: 9.310369\n","iter: 12240, loss: 0.074538, lr: 0.000300, time: 8.678147\n","iter: 12260, loss: 0.166497, lr: 0.000300, time: 9.395127\n","iter: 12280, loss: 0.073538, lr: 0.000300, time: 8.667037\n","iter: 12300, loss: 0.121229, lr: 0.000300, time: 9.427324\n","iter: 12320, loss: 0.087088, lr: 0.000300, time: 8.714281\n","iter: 12340, loss: 0.106872, lr: 0.000300, time: 8.633421\n","iter: 12360, loss: 0.081584, lr: 0.000300, time: 9.400680\n","iter: 12380, loss: 0.154514, lr: 0.000300, time: 8.647050\n","iter: 12400, loss: 0.125205, lr: 0.000300, time: 9.470793\n","iter: 12420, loss: 0.156200, lr: 0.000300, time: 8.615283\n","iter: 12440, loss: 0.062486, lr: 0.000300, time: 9.473709\n","iter: 12460, loss: 0.086844, lr: 0.000300, time: 8.661387\n","iter: 12480, loss: 0.118531, lr: 0.000300, time: 9.469372\n","iter: 12500, loss: 0.103544, lr: 0.000300, time: 8.650940\n","iter: 12520, loss: 0.105704, lr: 0.000300, time: 9.340600\n","iter: 12540, loss: 0.102836, lr: 0.000300, time: 8.643687\n","iter: 12560, loss: 0.085306, lr: 0.000300, time: 9.263803\n","iter: 12580, loss: 0.063774, lr: 0.000300, time: 8.653383\n","iter: 12600, loss: 0.095485, lr: 0.000300, time: 9.378997\n","iter: 12620, loss: 0.120191, lr: 0.000300, time: 8.665593\n","iter: 12640, loss: 0.128630, lr: 0.000300, time: 9.329306\n","iter: 12660, loss: 0.105135, lr: 0.000300, time: 8.657894\n","iter: 12680, loss: 0.088968, lr: 0.000300, time: 9.377657\n","iter: 12700, loss: 0.118971, lr: 0.000300, time: 8.682311\n","iter: 12720, loss: 0.105976, lr: 0.000300, time: 9.351417\n","iter: 12740, loss: 0.094935, lr: 0.000300, time: 8.672902\n","iter: 12760, loss: 0.113259, lr: 0.000300, time: 9.367513\n","iter: 12780, loss: 0.091213, lr: 0.000300, time: 8.667966\n","iter: 12800, loss: 0.111768, lr: 0.000300, time: 9.222856\n","iter: 12820, loss: 0.063828, lr: 0.000300, time: 8.654935\n","iter: 12840, loss: 0.078874, lr: 0.000300, time: 9.403611\n","iter: 12860, loss: 0.124911, lr: 0.000300, time: 8.683053\n","iter: 12880, loss: 0.096718, lr: 0.000300, time: 9.335867\n","iter: 12900, loss: 0.096884, lr: 0.000300, time: 8.684073\n","iter: 12920, loss: 0.122982, lr: 0.000300, time: 9.378447\n","iter: 12940, loss: 0.126899, lr: 0.000300, time: 8.670265\n","iter: 12960, loss: 0.092817, lr: 0.000300, time: 9.344401\n","iter: 12980, loss: 0.059232, lr: 0.000300, time: 8.669021\n","iter: 13000, loss: 0.104676, lr: 0.000300, time: 9.224801\n","iter: 13020, loss: 0.111716, lr: 0.000300, time: 8.703007\n","iter: 13040, loss: 0.104808, lr: 0.000300, time: 9.373230\n","iter: 13060, loss: 0.146440, lr: 0.000300, time: 8.665416\n","iter: 13080, loss: 0.106754, lr: 0.000300, time: 9.225160\n","iter: 13100, loss: 0.114581, lr: 0.000300, time: 8.668790\n","iter: 13120, loss: 0.103839, lr: 0.000300, time: 9.278366\n","iter: 13140, loss: 0.098707, lr: 0.000300, time: 8.687890\n","iter: 13160, loss: 0.105427, lr: 0.000300, time: 8.614270\n","iter: 13180, loss: 0.099577, lr: 0.000300, time: 9.262615\n","iter: 13200, loss: 0.070538, lr: 0.000300, time: 8.633243\n","iter: 13220, loss: 0.096596, lr: 0.000300, time: 9.403209\n","iter: 13240, loss: 0.065376, lr: 0.000300, time: 8.621125\n","iter: 13260, loss: 0.088252, lr: 0.000300, time: 9.391326\n","iter: 13280, loss: 0.101371, lr: 0.000300, time: 8.631483\n","iter: 13300, loss: 0.100314, lr: 0.000300, time: 9.175222\n","iter: 13320, loss: 0.065099, lr: 0.000300, time: 8.641425\n","iter: 13340, loss: 0.081554, lr: 0.000300, time: 9.271224\n","iter: 13360, loss: 0.091535, lr: 0.000300, time: 8.646450\n","iter: 13380, loss: 0.082056, lr: 0.000300, time: 9.370268\n","iter: 13400, loss: 0.146308, lr: 0.000300, time: 8.675364\n","iter: 13420, loss: 0.072406, lr: 0.000300, time: 9.349886\n","iter: 13440, loss: 0.096676, lr: 0.000300, time: 8.671674\n","iter: 13460, loss: 0.059931, lr: 0.000300, time: 9.361979\n","iter: 13480, loss: 0.127918, lr: 0.000300, time: 8.685963\n","iter: 13500, loss: 0.116523, lr: 0.000300, time: 9.397607\n","iter: 13520, loss: 0.087083, lr: 0.000300, time: 8.699177\n","iter: 13540, loss: 0.093593, lr: 0.000300, time: 9.409638\n","iter: 13560, loss: 0.076642, lr: 0.000300, time: 8.665675\n","iter: 13580, loss: 0.076837, lr: 0.000300, time: 9.380013\n","iter: 13600, loss: 0.122188, lr: 0.000300, time: 8.680305\n","iter: 13620, loss: 0.128250, lr: 0.000300, time: 9.360856\n","iter: 13640, loss: 0.083305, lr: 0.000300, time: 8.681664\n","iter: 13660, loss: 0.097283, lr: 0.000300, time: 9.282404\n","iter: 13680, loss: 0.098924, lr: 0.000300, time: 8.676170\n","iter: 13700, loss: 0.152130, lr: 0.000300, time: 9.377001\n","iter: 13720, loss: 0.101852, lr: 0.000300, time: 8.671956\n","iter: 13740, loss: 0.167620, lr: 0.000300, time: 9.360022\n","iter: 13760, loss: 0.123904, lr: 0.000300, time: 8.658667\n","iter: 13780, loss: 0.088453, lr: 0.000300, time: 9.365260\n","iter: 13800, loss: 0.133015, lr: 0.000300, time: 8.656126\n","iter: 13820, loss: 0.100028, lr: 0.000300, time: 9.425224\n","iter: 13840, loss: 0.121332, lr: 0.000300, time: 8.669868\n","iter: 13860, loss: 0.119215, lr: 0.000300, time: 9.285670\n","iter: 13880, loss: 0.096239, lr: 0.000300, time: 8.672114\n","iter: 13900, loss: 0.063977, lr: 0.000300, time: 9.289210\n","iter: 13920, loss: 0.069858, lr: 0.000300, time: 8.666325\n","iter: 13940, loss: 0.074287, lr: 0.000300, time: 9.324833\n","iter: 13960, loss: 0.082776, lr: 0.000300, time: 8.664623\n","iter: 13980, loss: 0.088265, lr: 0.000300, time: 8.606679\n","iter: 14000, loss: 0.056059, lr: 0.000300, time: 9.396141\n","iter: 14020, loss: 0.069008, lr: 0.000300, time: 8.639476\n","iter: 14040, loss: 0.099200, lr: 0.000300, time: 9.332171\n","iter: 14060, loss: 0.084230, lr: 0.000300, time: 8.631854\n","iter: 14080, loss: 0.106907, lr: 0.000300, time: 9.481570\n","iter: 14100, loss: 0.104755, lr: 0.000300, time: 8.642938\n","iter: 14120, loss: 0.078473, lr: 0.000300, time: 9.328572\n","iter: 14140, loss: 0.074422, lr: 0.000300, time: 8.639621\n","iter: 14160, loss: 0.095779, lr: 0.000300, time: 9.345774\n","iter: 14180, loss: 0.098012, lr: 0.000300, time: 8.634582\n","iter: 14200, loss: 0.082400, lr: 0.000300, time: 9.277817\n","iter: 14220, loss: 0.134949, lr: 0.000300, time: 8.651261\n","iter: 14240, loss: 0.085156, lr: 0.000300, time: 9.363045\n","iter: 14260, loss: 0.105435, lr: 0.000300, time: 8.632936\n","iter: 14280, loss: 0.115559, lr: 0.000300, time: 9.350863\n","iter: 14300, loss: 0.096649, lr: 0.000300, time: 8.661644\n","iter: 14320, loss: 0.082469, lr: 0.000300, time: 9.346572\n","iter: 14340, loss: 0.100591, lr: 0.000300, time: 8.665524\n","iter: 14360, loss: 0.066229, lr: 0.000300, time: 9.331779\n","iter: 14380, loss: 0.083348, lr: 0.000300, time: 8.657259\n","iter: 14400, loss: 0.062319, lr: 0.000300, time: 9.306840\n","iter: 14420, loss: 0.067071, lr: 0.000300, time: 8.700055\n","iter: 14440, loss: 0.075044, lr: 0.000300, time: 9.245335\n","iter: 14460, loss: 0.086826, lr: 0.000300, time: 8.675049\n","iter: 14480, loss: 0.068425, lr: 0.000300, time: 9.339101\n","iter: 14500, loss: 0.088207, lr: 0.000300, time: 8.663859\n","iter: 14520, loss: 0.078405, lr: 0.000300, time: 9.395276\n","iter: 14540, loss: 0.127869, lr: 0.000300, time: 8.665695\n","iter: 14560, loss: 0.092726, lr: 0.000300, time: 9.328895\n","iter: 14580, loss: 0.106134, lr: 0.000300, time: 8.655032\n","iter: 14600, loss: 0.083826, lr: 0.000300, time: 9.435843\n","iter: 14620, loss: 0.103498, lr: 0.000300, time: 8.677063\n","iter: 14640, loss: 0.062351, lr: 0.000300, time: 9.338285\n","iter: 14660, loss: 0.052514, lr: 0.000300, time: 8.665134\n","iter: 14680, loss: 0.064456, lr: 0.000300, time: 9.263669\n","iter: 14700, loss: 0.096630, lr: 0.000300, time: 8.654825\n","iter: 14720, loss: 0.055480, lr: 0.000300, time: 9.146497\n","iter: 14740, loss: 0.068550, lr: 0.000300, time: 8.659937\n","iter: 14760, loss: 0.089836, lr: 0.000300, time: 9.317277\n","iter: 14780, loss: 0.088070, lr: 0.000300, time: 8.680304\n","iter: 14800, loss: 0.077659, lr: 0.000300, time: 8.620296\n","iter: 14820, loss: 0.102750, lr: 0.000300, time: 9.449084\n","iter: 14840, loss: 0.100627, lr: 0.000300, time: 8.619138\n","iter: 14860, loss: 0.068460, lr: 0.000300, time: 9.415033\n","iter: 14880, loss: 0.060342, lr: 0.000300, time: 8.631469\n","iter: 14900, loss: 0.095969, lr: 0.000300, time: 9.241552\n","iter: 14920, loss: 0.075147, lr: 0.000300, time: 8.619040\n","iter: 14940, loss: 0.091100, lr: 0.000300, time: 9.383540\n","iter: 14960, loss: 0.080287, lr: 0.000300, time: 8.663806\n","iter: 14980, loss: 0.059474, lr: 0.000300, time: 9.397768\n","==> changing adam betas from (0.9, 0.999) to (0.5, 0.999)\n","==> start droping lr exponentially\n","iter: 15000, loss: 0.106308, lr: 0.000300, time: 8.641487\n","iter: 15020, loss: 0.060561, lr: 0.000295, time: 9.221263\n","iter: 15040, loss: 0.064998, lr: 0.000291, time: 8.657432\n","iter: 15060, loss: 0.111189, lr: 0.000287, time: 9.347905\n","iter: 15080, loss: 0.062750, lr: 0.000283, time: 8.654071\n","iter: 15100, loss: 0.077641, lr: 0.000280, time: 9.348302\n","iter: 15120, loss: 0.098980, lr: 0.000276, time: 8.665338\n","iter: 15140, loss: 0.053399, lr: 0.000272, time: 9.187683\n","iter: 15160, loss: 0.064716, lr: 0.000268, time: 8.675303\n","iter: 15180, loss: 0.078769, lr: 0.000265, time: 9.316586\n","iter: 15200, loss: 0.086171, lr: 0.000261, time: 8.664853\n","iter: 15220, loss: 0.062922, lr: 0.000257, time: 9.391297\n","iter: 15240, loss: 0.057988, lr: 0.000254, time: 8.667336\n","iter: 15260, loss: 0.080096, lr: 0.000250, time: 9.333867\n","iter: 15280, loss: 0.061192, lr: 0.000247, time: 8.686784\n","iter: 15300, loss: 0.067827, lr: 0.000244, time: 9.347295\n","iter: 15320, loss: 0.049888, lr: 0.000240, time: 8.649455\n","iter: 15340, loss: 0.071940, lr: 0.000237, time: 9.261077\n","iter: 15360, loss: 0.049831, lr: 0.000234, time: 8.678418\n","iter: 15380, loss: 0.062103, lr: 0.000230, time: 9.315373\n","iter: 15400, loss: 0.031514, lr: 0.000227, time: 8.677097\n","iter: 15420, loss: 0.041494, lr: 0.000224, time: 9.372803\n","iter: 15440, loss: 0.078790, lr: 0.000221, time: 8.648009\n","iter: 15460, loss: 0.075145, lr: 0.000218, time: 9.348582\n","iter: 15480, loss: 0.079069, lr: 0.000215, time: 8.665210\n","iter: 15500, loss: 0.074578, lr: 0.000212, time: 9.368260\n","iter: 15520, loss: 0.032123, lr: 0.000209, time: 8.684686\n","iter: 15540, loss: 0.066966, lr: 0.000206, time: 9.373712\n","iter: 15560, loss: 0.021721, lr: 0.000203, time: 8.664659\n","iter: 15580, loss: 0.039248, lr: 0.000201, time: 9.355556\n","iter: 15600, loss: 0.071843, lr: 0.000198, time: 8.648696\n","iter: 15620, loss: 0.062253, lr: 0.000195, time: 8.608586\n","iter: 15640, loss: 0.054195, lr: 0.000193, time: 9.366505\n","iter: 15660, loss: 0.051347, lr: 0.000190, time: 8.637961\n","iter: 15680, loss: 0.058494, lr: 0.000187, time: 9.430680\n","iter: 15700, loss: 0.030402, lr: 0.000185, time: 8.623423\n","iter: 15720, loss: 0.060311, lr: 0.000182, time: 9.356342\n","iter: 15740, loss: 0.025392, lr: 0.000180, time: 8.625452\n","iter: 15760, loss: 0.036837, lr: 0.000177, time: 9.192998\n","iter: 15780, loss: 0.061678, lr: 0.000175, time: 8.655689\n","iter: 15800, loss: 0.037076, lr: 0.000172, time: 9.412595\n","iter: 15820, loss: 0.042224, lr: 0.000170, time: 8.652002\n","iter: 15840, loss: 0.036003, lr: 0.000168, time: 9.329695\n","iter: 15860, loss: 0.048056, lr: 0.000165, time: 8.668860\n","iter: 15880, loss: 0.037568, lr: 0.000163, time: 9.281876\n","iter: 15900, loss: 0.047682, lr: 0.000161, time: 8.662434\n","iter: 15920, loss: 0.026933, lr: 0.000159, time: 9.303049\n","iter: 15940, loss: 0.057097, lr: 0.000157, time: 8.667262\n","iter: 15960, loss: 0.022483, lr: 0.000154, time: 9.281007\n","iter: 15980, loss: 0.038837, lr: 0.000152, time: 8.691750\n","iter: 16000, loss: 0.050932, lr: 0.000150, time: 9.391584\n","iter: 16020, loss: 0.027344, lr: 0.000148, time: 8.674185\n","iter: 16040, loss: 0.065200, lr: 0.000146, time: 9.325243\n","iter: 16060, loss: 0.026012, lr: 0.000144, time: 8.687792\n","iter: 16080, loss: 0.024354, lr: 0.000142, time: 9.336243\n","iter: 16100, loss: 0.013521, lr: 0.000140, time: 8.686908\n","iter: 16120, loss: 0.026803, lr: 0.000138, time: 9.321632\n","iter: 16140, loss: 0.036717, lr: 0.000136, time: 8.705030\n","iter: 16160, loss: 0.028194, lr: 0.000134, time: 9.370011\n","iter: 16180, loss: 0.053520, lr: 0.000133, time: 8.669549\n","iter: 16200, loss: 0.035937, lr: 0.000131, time: 9.374534\n","iter: 16220, loss: 0.032743, lr: 0.000129, time: 8.695434\n","iter: 16240, loss: 0.034822, lr: 0.000127, time: 9.335598\n","iter: 16260, loss: 0.013879, lr: 0.000125, time: 8.659514\n","iter: 16280, loss: 0.025285, lr: 0.000124, time: 9.254677\n","iter: 16300, loss: 0.035992, lr: 0.000122, time: 8.671654\n","iter: 16320, loss: 0.026712, lr: 0.000120, time: 9.265358\n","iter: 16340, loss: 0.023304, lr: 0.000119, time: 8.677624\n","iter: 16360, loss: 0.047743, lr: 0.000117, time: 9.378198\n","iter: 16380, loss: 0.021643, lr: 0.000115, time: 8.671460\n","iter: 16400, loss: 0.038775, lr: 0.000114, time: 9.229705\n","iter: 16420, loss: 0.041100, lr: 0.000112, time: 8.695064\n","iter: 16440, loss: 0.046602, lr: 0.000111, time: 8.607881\n","iter: 16460, loss: 0.023628, lr: 0.000109, time: 9.269840\n","iter: 16480, loss: 0.025458, lr: 0.000108, time: 8.616159\n","iter: 16500, loss: 0.028535, lr: 0.000106, time: 9.366814\n","iter: 16520, loss: 0.034376, lr: 0.000105, time: 8.619163\n","iter: 16540, loss: 0.035449, lr: 0.000103, time: 9.385015\n","iter: 16560, loss: 0.023299, lr: 0.000102, time: 8.635933\n","iter: 16580, loss: 0.034406, lr: 0.000101, time: 9.323214\n","iter: 16600, loss: 0.044306, lr: 0.000099, time: 8.638540\n","iter: 16620, loss: 0.025863, lr: 0.000098, time: 9.258774\n","iter: 16640, loss: 0.027755, lr: 0.000097, time: 8.620647\n","iter: 16660, loss: 0.038400, lr: 0.000095, time: 9.247618\n","iter: 16680, loss: 0.038617, lr: 0.000094, time: 8.653965\n","iter: 16700, loss: 0.026994, lr: 0.000093, time: 9.332884\n","iter: 16720, loss: 0.018625, lr: 0.000091, time: 8.686661\n","iter: 16740, loss: 0.010426, lr: 0.000090, time: 9.309334\n","iter: 16760, loss: 0.027176, lr: 0.000089, time: 8.688612\n","iter: 16780, loss: 0.022539, lr: 0.000088, time: 9.232629\n","iter: 16800, loss: 0.032040, lr: 0.000086, time: 8.657233\n","iter: 16820, loss: 0.023765, lr: 0.000085, time: 9.347924\n","iter: 16840, loss: 0.034510, lr: 0.000084, time: 8.669527\n","iter: 16860, loss: 0.045898, lr: 0.000083, time: 9.222670\n","iter: 16880, loss: 0.052290, lr: 0.000082, time: 8.664899\n","iter: 16900, loss: 0.041606, lr: 0.000081, time: 9.336045\n","iter: 16920, loss: 0.022660, lr: 0.000080, time: 8.676741\n","iter: 16940, loss: 0.041124, lr: 0.000078, time: 9.229541\n","iter: 16960, loss: 0.017892, lr: 0.000077, time: 8.654901\n","iter: 16980, loss: 0.020581, lr: 0.000076, time: 9.310301\n","iter: 17000, loss: 0.031441, lr: 0.000075, time: 8.684662\n","iter: 17020, loss: 0.015227, lr: 0.000074, time: 9.247787\n","iter: 17040, loss: 0.025274, lr: 0.000073, time: 8.654538\n","iter: 17060, loss: 0.025818, lr: 0.000072, time: 9.264925\n","iter: 17080, loss: 0.040830, lr: 0.000071, time: 8.685825\n","iter: 17100, loss: 0.049675, lr: 0.000070, time: 9.334693\n","iter: 17120, loss: 0.037457, lr: 0.000069, time: 8.670862\n","iter: 17140, loss: 0.031992, lr: 0.000068, time: 9.411119\n","iter: 17160, loss: 0.028804, lr: 0.000067, time: 8.646856\n","iter: 17180, loss: 0.020981, lr: 0.000066, time: 9.182442\n","iter: 17200, loss: 0.013207, lr: 0.000066, time: 8.658242\n","iter: 17220, loss: 0.011272, lr: 0.000065, time: 9.105435\n","iter: 17240, loss: 0.023611, lr: 0.000064, time: 8.705865\n","iter: 17260, loss: 0.019461, lr: 0.000063, time: 8.643612\n","iter: 17280, loss: 0.015489, lr: 0.000062, time: 9.384946\n","iter: 17300, loss: 0.028148, lr: 0.000061, time: 8.612815\n","iter: 17320, loss: 0.029656, lr: 0.000060, time: 9.343503\n","iter: 17340, loss: 0.018919, lr: 0.000060, time: 8.621844\n","iter: 17360, loss: 0.013057, lr: 0.000059, time: 9.213528\n","iter: 17380, loss: 0.021668, lr: 0.000058, time: 8.637526\n","iter: 17400, loss: 0.027254, lr: 0.000057, time: 9.382743\n","iter: 17420, loss: 0.022963, lr: 0.000056, time: 8.650514\n","iter: 17440, loss: 0.027073, lr: 0.000056, time: 9.343590\n","iter: 17460, loss: 0.039804, lr: 0.000055, time: 8.662765\n","iter: 17480, loss: 0.028293, lr: 0.000054, time: 9.319439\n","iter: 17500, loss: 0.014760, lr: 0.000053, time: 8.651834\n","iter: 17520, loss: 0.016337, lr: 0.000053, time: 9.345696\n","iter: 17540, loss: 0.035879, lr: 0.000052, time: 8.686274\n","iter: 17560, loss: 0.029610, lr: 0.000051, time: 9.383536\n","iter: 17580, loss: 0.030086, lr: 0.000050, time: 8.650569\n","iter: 17600, loss: 0.036660, lr: 0.000050, time: 9.245806\n","iter: 17620, loss: 0.059394, lr: 0.000049, time: 8.685376\n","iter: 17640, loss: 0.039160, lr: 0.000048, time: 9.240595\n","iter: 17660, loss: 0.033847, lr: 0.000048, time: 8.665212\n","iter: 17680, loss: 0.019100, lr: 0.000047, time: 9.294403\n","iter: 17700, loss: 0.018813, lr: 0.000046, time: 8.665063\n","iter: 17720, loss: 0.014455, lr: 0.000046, time: 9.307583\n","iter: 17740, loss: 0.017778, lr: 0.000045, time: 8.646464\n","iter: 17760, loss: 0.017888, lr: 0.000045, time: 9.282206\n","iter: 17780, loss: 0.019901, lr: 0.000044, time: 8.653677\n","iter: 17800, loss: 0.019350, lr: 0.000043, time: 9.336035\n","iter: 17820, loss: 0.022067, lr: 0.000043, time: 8.675804\n","iter: 17840, loss: 0.011025, lr: 0.000042, time: 9.261003\n","iter: 17860, loss: 0.012904, lr: 0.000042, time: 8.653954\n","iter: 17880, loss: 0.024941, lr: 0.000041, time: 9.349322\n","iter: 17900, loss: 0.029235, lr: 0.000040, time: 8.680707\n","iter: 17920, loss: 0.009008, lr: 0.000040, time: 9.343755\n","iter: 17940, loss: 0.016935, lr: 0.000039, time: 8.665577\n","iter: 17960, loss: 0.014297, lr: 0.000039, time: 9.240454\n","iter: 17980, loss: 0.023216, lr: 0.000038, time: 8.677388\n","iter: 18000, loss: 0.009754, lr: 0.000038, time: 9.393052\n","iter: 18020, loss: 0.025302, lr: 0.000037, time: 8.659398\n","iter: 18040, loss: 0.018270, lr: 0.000037, time: 9.232188\n","iter: 18060, loss: 0.015954, lr: 0.000036, time: 8.680622\n","iter: 18080, loss: 0.032357, lr: 0.000036, time: 8.615133\n","iter: 18100, loss: 0.026188, lr: 0.000035, time: 9.377026\n","iter: 18120, loss: 0.011342, lr: 0.000035, time: 8.618722\n","iter: 18140, loss: 0.012961, lr: 0.000034, time: 9.433593\n","iter: 18160, loss: 0.027473, lr: 0.000034, time: 8.631052\n","iter: 18180, loss: 0.012338, lr: 0.000033, time: 9.249370\n","iter: 18200, loss: 0.024110, lr: 0.000033, time: 8.635139\n","iter: 18220, loss: 0.014680, lr: 0.000032, time: 9.282978\n","iter: 18240, loss: 0.017339, lr: 0.000032, time: 8.637435\n","iter: 18260, loss: 0.006220, lr: 0.000032, time: 9.349917\n","iter: 18280, loss: 0.018325, lr: 0.000031, time: 8.642434\n","iter: 18300, loss: 0.018471, lr: 0.000031, time: 9.321248\n","iter: 18320, loss: 0.023280, lr: 0.000030, time: 8.639984\n","iter: 18340, loss: 0.012014, lr: 0.000030, time: 9.219520\n","iter: 18360, loss: 0.025287, lr: 0.000029, time: 8.664739\n","iter: 18380, loss: 0.017638, lr: 0.000029, time: 9.350460\n","iter: 18400, loss: 0.013179, lr: 0.000029, time: 8.660955\n","iter: 18420, loss: 0.013220, lr: 0.000028, time: 9.133640\n","iter: 18440, loss: 0.025544, lr: 0.000028, time: 8.661559\n","iter: 18460, loss: 0.028280, lr: 0.000027, time: 9.307670\n","iter: 18480, loss: 0.015263, lr: 0.000027, time: 8.657436\n","iter: 18500, loss: 0.028880, lr: 0.000027, time: 9.191123\n","iter: 18520, loss: 0.036322, lr: 0.000026, time: 8.654480\n","iter: 18540, loss: 0.027035, lr: 0.000026, time: 9.327941\n","iter: 18560, loss: 0.012190, lr: 0.000026, time: 8.660660\n","iter: 18580, loss: 0.035571, lr: 0.000025, time: 9.279735\n","iter: 18600, loss: 0.024698, lr: 0.000025, time: 8.655514\n","iter: 18620, loss: 0.015320, lr: 0.000025, time: 9.290198\n","iter: 18640, loss: 0.009866, lr: 0.000024, time: 8.660878\n","iter: 18660, loss: 0.024714, lr: 0.000024, time: 9.236059\n","iter: 18680, loss: 0.056381, lr: 0.000024, time: 8.645056\n","iter: 18700, loss: 0.038737, lr: 0.000023, time: 9.342957\n","iter: 18720, loss: 0.017105, lr: 0.000023, time: 8.653484\n","iter: 18740, loss: 0.015833, lr: 0.000023, time: 9.322694\n","iter: 18760, loss: 0.013876, lr: 0.000022, time: 8.635957\n","iter: 18780, loss: 0.011951, lr: 0.000022, time: 9.281174\n","iter: 18800, loss: 0.007429, lr: 0.000022, time: 8.654098\n","iter: 18820, loss: 0.028232, lr: 0.000021, time: 9.234153\n","iter: 18840, loss: 0.006695, lr: 0.000021, time: 8.655973\n","iter: 18860, loss: 0.019189, lr: 0.000021, time: 9.119086\n","iter: 18880, loss: 0.017442, lr: 0.000021, time: 8.689984\n","iter: 18900, loss: 0.030631, lr: 0.000020, time: 8.609612\n","iter: 18920, loss: 0.018810, lr: 0.000020, time: 9.394224\n","iter: 18940, loss: 0.016375, lr: 0.000020, time: 8.612761\n","iter: 18960, loss: 0.029877, lr: 0.000019, time: 9.369035\n","iter: 18980, loss: 0.009845, lr: 0.000019, time: 8.624143\n","iter: 19000, loss: 0.016348, lr: 0.000019, time: 9.371553\n","iter: 19020, loss: 0.032067, lr: 0.000019, time: 8.608052\n","iter: 19040, loss: 0.009180, lr: 0.000018, time: 9.324092\n","iter: 19060, loss: 0.038925, lr: 0.000018, time: 8.614671\n","iter: 19080, loss: 0.030593, lr: 0.000018, time: 9.315490\n","iter: 19100, loss: 0.016603, lr: 0.000018, time: 8.638184\n","iter: 19120, loss: 0.013044, lr: 0.000017, time: 9.307104\n","iter: 19140, loss: 0.016589, lr: 0.000017, time: 8.627358\n","iter: 19160, loss: 0.029383, lr: 0.000017, time: 9.361056\n","iter: 19180, loss: 0.018328, lr: 0.000017, time: 8.670340\n","iter: 19200, loss: 0.012079, lr: 0.000016, time: 9.335934\n","iter: 19220, loss: 0.017986, lr: 0.000016, time: 8.680750\n","iter: 19240, loss: 0.028490, lr: 0.000016, time: 9.339672\n","iter: 19260, loss: 0.022171, lr: 0.000016, time: 8.643914\n","iter: 19280, loss: 0.025460, lr: 0.000016, time: 9.324997\n","iter: 19300, loss: 0.021386, lr: 0.000015, time: 8.658846\n","iter: 19320, loss: 0.011711, lr: 0.000015, time: 9.340634\n","iter: 19340, loss: 0.022867, lr: 0.000015, time: 8.651570\n","iter: 19360, loss: 0.014008, lr: 0.000015, time: 9.295177\n","iter: 19380, loss: 0.012996, lr: 0.000015, time: 8.675211\n","iter: 19400, loss: 0.024641, lr: 0.000014, time: 9.395898\n","iter: 19420, loss: 0.013670, lr: 0.000014, time: 8.662396\n","iter: 19440, loss: 0.011009, lr: 0.000014, time: 9.264112\n","iter: 19460, loss: 0.012044, lr: 0.000014, time: 8.675574\n","iter: 19480, loss: 0.011200, lr: 0.000014, time: 9.407385\n","iter: 19500, loss: 0.015626, lr: 0.000013, time: 8.670969\n","iter: 19520, loss: 0.017128, lr: 0.000013, time: 9.389747\n","iter: 19540, loss: 0.013975, lr: 0.000013, time: 8.677370\n","iter: 19560, loss: 0.033672, lr: 0.000013, time: 9.364388\n","iter: 19580, loss: 0.018673, lr: 0.000013, time: 8.675350\n","iter: 19600, loss: 0.014553, lr: 0.000012, time: 9.296152\n","iter: 19620, loss: 0.007236, lr: 0.000012, time: 8.682344\n","iter: 19640, loss: 0.020498, lr: 0.000012, time: 9.300683\n","iter: 19660, loss: 0.019400, lr: 0.000012, time: 8.671096\n","iter: 19680, loss: 0.011051, lr: 0.000012, time: 9.406294\n","iter: 19700, loss: 0.034990, lr: 0.000012, time: 8.658987\n","iter: 19720, loss: 0.028018, lr: 0.000011, time: 8.620471\n","iter: 19740, loss: 0.008032, lr: 0.000011, time: 9.327631\n","iter: 19760, loss: 0.018330, lr: 0.000011, time: 8.627375\n","iter: 19780, loss: 0.020069, lr: 0.000011, time: 9.394400\n","iter: 19800, loss: 0.026236, lr: 0.000011, time: 8.633258\n","iter: 19820, loss: 0.018115, lr: 0.000011, time: 9.260101\n","iter: 19840, loss: 0.008200, lr: 0.000011, time: 8.621182\n","iter: 19860, loss: 0.016250, lr: 0.000010, time: 9.304354\n","iter: 19880, loss: 0.014574, lr: 0.000010, time: 8.644732\n","iter: 19900, loss: 0.019160, lr: 0.000010, time: 9.392288\n","iter: 19920, loss: 0.032696, lr: 0.000010, time: 8.648781\n","iter: 19940, loss: 0.011258, lr: 0.000010, time: 9.319348\n","iter: 19960, loss: 0.010009, lr: 0.000010, time: 8.635444\n","iter: 19980, loss: 0.009756, lr: 0.000010, time: 9.288238\n","iter: 20000, loss: 0.012380, lr: 0.000009, time: 8.655182\n","iter: 20020, loss: 0.011400, lr: 0.000009, time: 9.412042\n","iter: 20040, loss: 0.023011, lr: 0.000009, time: 8.662221\n","iter: 20060, loss: 0.012619, lr: 0.000009, time: 9.373724\n","iter: 20080, loss: 0.018855, lr: 0.000009, time: 8.667071\n","iter: 20100, loss: 0.016387, lr: 0.000009, time: 9.218827\n","iter: 20120, loss: 0.016419, lr: 0.000009, time: 8.670508\n","iter: 20140, loss: 0.006979, lr: 0.000009, time: 9.318630\n","iter: 20160, loss: 0.029157, lr: 0.000008, time: 8.687517\n","iter: 20180, loss: 0.016354, lr: 0.000008, time: 9.349241\n","iter: 20200, loss: 0.018337, lr: 0.000008, time: 8.664001\n","iter: 20220, loss: 0.028412, lr: 0.000008, time: 9.415377\n","iter: 20240, loss: 0.019109, lr: 0.000008, time: 8.709794\n","iter: 20260, loss: 0.009968, lr: 0.000008, time: 9.328027\n","iter: 20280, loss: 0.026286, lr: 0.000008, time: 8.679350\n","iter: 20300, loss: 0.008205, lr: 0.000008, time: 9.371473\n","iter: 20320, loss: 0.030153, lr: 0.000008, time: 8.683790\n","iter: 20340, loss: 0.006484, lr: 0.000007, time: 9.350605\n","iter: 20360, loss: 0.027919, lr: 0.000007, time: 8.667727\n","iter: 20380, loss: 0.014941, lr: 0.000007, time: 9.302800\n","iter: 20400, loss: 0.010952, lr: 0.000007, time: 8.652873\n","iter: 20420, loss: 0.021353, lr: 0.000007, time: 9.263854\n","iter: 20440, loss: 0.014715, lr: 0.000007, time: 8.694617\n","iter: 20460, loss: 0.015655, lr: 0.000007, time: 9.286870\n","iter: 20480, loss: 0.022181, lr: 0.000007, time: 8.648745\n","iter: 20500, loss: 0.019552, lr: 0.000007, time: 9.225055\n","iter: 20520, loss: 0.011073, lr: 0.000007, time: 8.667988\n","iter: 20540, loss: 0.017831, lr: 0.000007, time: 8.625464\n","iter: 20560, loss: 0.013202, lr: 0.000006, time: 9.329026\n","iter: 20580, loss: 0.020809, lr: 0.000006, time: 8.607679\n","iter: 20600, loss: 0.023267, lr: 0.000006, time: 9.402658\n","iter: 20620, loss: 0.015106, lr: 0.000006, time: 8.625365\n","iter: 20640, loss: 0.015128, lr: 0.000006, time: 9.390199\n","iter: 20660, loss: 0.036240, lr: 0.000006, time: 8.618222\n","iter: 20680, loss: 0.024310, lr: 0.000006, time: 9.333866\n","iter: 20700, loss: 0.021010, lr: 0.000006, time: 8.653181\n","iter: 20720, loss: 0.012585, lr: 0.000006, time: 9.381617\n","iter: 20740, loss: 0.027138, lr: 0.000006, time: 8.665969\n","iter: 20760, loss: 0.025795, lr: 0.000006, time: 9.242662\n","iter: 20780, loss: 0.017704, lr: 0.000006, time: 8.663074\n","iter: 20800, loss: 0.012204, lr: 0.000005, time: 9.324842\n","iter: 20820, loss: 0.009899, lr: 0.000005, time: 8.669054\n","iter: 20840, loss: 0.022913, lr: 0.000005, time: 9.234277\n","iter: 20860, loss: 0.022558, lr: 0.000005, time: 8.660949\n","iter: 20880, loss: 0.011412, lr: 0.000005, time: 9.326274\n","iter: 20900, loss: 0.009925, lr: 0.000005, time: 8.678434\n","iter: 20920, loss: 0.017906, lr: 0.000005, time: 9.226452\n","iter: 20940, loss: 0.021102, lr: 0.000005, time: 8.658847\n","iter: 20960, loss: 0.034699, lr: 0.000005, time: 9.270807\n","iter: 20980, loss: 0.019881, lr: 0.000005, time: 8.696464\n","iter: 21000, loss: 0.011752, lr: 0.000005, time: 9.217618\n","iter: 21020, loss: 0.019994, lr: 0.000005, time: 8.656648\n","iter: 21040, loss: 0.012890, lr: 0.000005, time: 9.342299\n","iter: 21060, loss: 0.022958, lr: 0.000005, time: 8.644079\n","iter: 21080, loss: 0.035314, lr: 0.000004, time: 9.338799\n","iter: 21100, loss: 0.012879, lr: 0.000004, time: 8.664831\n","iter: 21120, loss: 0.015086, lr: 0.000004, time: 9.349803\n","iter: 21140, loss: 0.018456, lr: 0.000004, time: 8.667693\n","iter: 21160, loss: 0.016047, lr: 0.000004, time: 9.230019\n","iter: 21180, loss: 0.015447, lr: 0.000004, time: 8.685888\n","iter: 21200, loss: 0.008973, lr: 0.000004, time: 9.368006\n","iter: 21220, loss: 0.031064, lr: 0.000004, time: 8.681073\n","iter: 21240, loss: 0.019160, lr: 0.000004, time: 9.359748\n","iter: 21260, loss: 0.011432, lr: 0.000004, time: 8.671294\n","iter: 21280, loss: 0.014696, lr: 0.000004, time: 9.328276\n","iter: 21300, loss: 0.018388, lr: 0.000004, time: 8.689621\n","iter: 21320, loss: 0.018015, lr: 0.000004, time: 9.340608\n","iter: 21340, loss: 0.010585, lr: 0.000004, time: 8.668630\n","iter: 21360, loss: 0.022079, lr: 0.000004, time: 8.633262\n","iter: 21380, loss: 0.014348, lr: 0.000004, time: 9.248158\n","iter: 21400, loss: 0.025108, lr: 0.000004, time: 8.637044\n","iter: 21420, loss: 0.014591, lr: 0.000004, time: 9.398967\n","iter: 21440, loss: 0.017836, lr: 0.000004, time: 8.620361\n","iter: 21460, loss: 0.021441, lr: 0.000003, time: 9.327702\n","iter: 21480, loss: 0.016658, lr: 0.000003, time: 8.642504\n","iter: 21500, loss: 0.019489, lr: 0.000003, time: 9.379534\n","iter: 21520, loss: 0.011065, lr: 0.000003, time: 8.637703\n","iter: 21540, loss: 0.008179, lr: 0.000003, time: 9.348648\n","iter: 21560, loss: 0.020960, lr: 0.000003, time: 8.661819\n","iter: 21580, loss: 0.010975, lr: 0.000003, time: 9.236146\n","iter: 21600, loss: 0.010821, lr: 0.000003, time: 8.650270\n","iter: 21620, loss: 0.016035, lr: 0.000003, time: 9.400481\n","iter: 21640, loss: 0.009116, lr: 0.000003, time: 8.680352\n","iter: 21660, loss: 0.009992, lr: 0.000003, time: 9.143469\n","iter: 21680, loss: 0.013381, lr: 0.000003, time: 8.669355\n","iter: 21700, loss: 0.007971, lr: 0.000003, time: 9.405520\n","iter: 21720, loss: 0.007722, lr: 0.000003, time: 8.650192\n","iter: 21740, loss: 0.011333, lr: 0.000003, time: 9.289922\n","iter: 21760, loss: 0.020014, lr: 0.000003, time: 8.697384\n","iter: 21780, loss: 0.009705, lr: 0.000003, time: 9.325554\n","iter: 21800, loss: 0.006384, lr: 0.000003, time: 8.693798\n","iter: 21820, loss: 0.015845, lr: 0.000003, time: 9.312220\n","iter: 21840, loss: 0.014125, lr: 0.000003, time: 8.668408\n","iter: 21860, loss: 0.024357, lr: 0.000003, time: 9.378235\n","iter: 21880, loss: 0.005878, lr: 0.000003, time: 8.666613\n","iter: 21900, loss: 0.022273, lr: 0.000003, time: 9.337725\n","iter: 21920, loss: 0.030597, lr: 0.000003, time: 8.663987\n","iter: 21940, loss: 0.028548, lr: 0.000002, time: 9.301607\n","iter: 21960, loss: 0.008411, lr: 0.000002, time: 8.677217\n","iter: 21980, loss: 0.012645, lr: 0.000002, time: 9.313501\n","iter: 22000, loss: 0.030854, lr: 0.000002, time: 8.676703\n","iter: 22020, loss: 0.008501, lr: 0.000002, time: 9.217291\n","iter: 22040, loss: 0.006380, lr: 0.000002, time: 8.659583\n","iter: 22060, loss: 0.015751, lr: 0.000002, time: 9.286641\n","iter: 22080, loss: 0.014947, lr: 0.000002, time: 8.690565\n","iter: 22100, loss: 0.003507, lr: 0.000002, time: 9.343040\n","iter: 22120, loss: 0.026350, lr: 0.000002, time: 8.654452\n","iter: 22140, loss: 0.019931, lr: 0.000002, time: 9.362241\n","iter: 22160, loss: 0.015248, lr: 0.000002, time: 8.687347\n","iter: 22180, loss: 0.019069, lr: 0.000002, time: 8.609301\n","iter: 22200, loss: 0.010081, lr: 0.000002, time: 9.319133\n","iter: 22220, loss: 0.012341, lr: 0.000002, time: 8.616621\n","iter: 22240, loss: 0.013802, lr: 0.000002, time: 9.410538\n","iter: 22260, loss: 0.035276, lr: 0.000002, time: 8.645565\n","iter: 22280, loss: 0.011847, lr: 0.000002, time: 9.372646\n","iter: 22300, loss: 0.016789, lr: 0.000002, time: 8.662260\n","iter: 22320, loss: 0.013603, lr: 0.000002, time: 9.285647\n","iter: 22340, loss: 0.007033, lr: 0.000002, time: 8.662841\n","iter: 22360, loss: 0.011839, lr: 0.000002, time: 9.413415\n","iter: 22380, loss: 0.015791, lr: 0.000002, time: 8.660286\n","iter: 22400, loss: 0.020209, lr: 0.000002, time: 9.320255\n","iter: 22420, loss: 0.011663, lr: 0.000002, time: 8.661105\n","iter: 22440, loss: 0.016202, lr: 0.000002, time: 9.397501\n","iter: 22460, loss: 0.017767, lr: 0.000002, time: 8.703000\n","iter: 22480, loss: 0.014398, lr: 0.000002, time: 9.408505\n","iter: 22500, loss: 0.008948, lr: 0.000002, time: 8.672696\n","iter: 22520, loss: 0.011096, lr: 0.000002, time: 9.231577\n","iter: 22540, loss: 0.030766, lr: 0.000002, time: 8.671513\n","iter: 22560, loss: 0.021894, lr: 0.000002, time: 9.329130\n","iter: 22580, loss: 0.018784, lr: 0.000002, time: 8.690146\n","iter: 22600, loss: 0.023253, lr: 0.000002, time: 9.366931\n","iter: 22620, loss: 0.021056, lr: 0.000002, time: 8.694649\n","iter: 22640, loss: 0.003097, lr: 0.000002, time: 9.260940\n","iter: 22660, loss: 0.016971, lr: 0.000002, time: 8.697035\n","iter: 22680, loss: 0.019154, lr: 0.000001, time: 9.359061\n","iter: 22700, loss: 0.017003, lr: 0.000001, time: 8.674697\n","iter: 22720, loss: 0.035969, lr: 0.000001, time: 9.357556\n","iter: 22740, loss: 0.017654, lr: 0.000001, time: 8.673510\n","iter: 22760, loss: 0.019254, lr: 0.000001, time: 9.391550\n","iter: 22780, loss: 0.012275, lr: 0.000001, time: 8.658347\n","iter: 22800, loss: 0.006975, lr: 0.000001, time: 9.274687\n","iter: 22820, loss: 0.032830, lr: 0.000001, time: 8.677752\n","iter: 22840, loss: 0.007359, lr: 0.000001, time: 9.405876\n","iter: 22860, loss: 0.011480, lr: 0.000001, time: 8.658031\n","iter: 22880, loss: 0.028074, lr: 0.000001, time: 9.310793\n","iter: 22900, loss: 0.009811, lr: 0.000001, time: 8.683141\n","iter: 22920, loss: 0.013687, lr: 0.000001, time: 9.364042\n","iter: 22940, loss: 0.008399, lr: 0.000001, time: 8.655166\n","iter: 22960, loss: 0.019853, lr: 0.000001, time: 9.239198\n","iter: 22980, loss: 0.011554, lr: 0.000001, time: 8.709148\n","iter: 23000, loss: 0.008031, lr: 0.000001, time: 8.624949\n","iter: 23020, loss: 0.010234, lr: 0.000001, time: 9.319039\n","iter: 23040, loss: 0.012696, lr: 0.000001, time: 8.631884\n","iter: 23060, loss: 0.018454, lr: 0.000001, time: 9.410251\n","iter: 23080, loss: 0.006433, lr: 0.000001, time: 8.619551\n","iter: 23100, loss: 0.008485, lr: 0.000001, time: 9.391062\n","iter: 23120, loss: 0.021936, lr: 0.000001, time: 8.645948\n","iter: 23140, loss: 0.013757, lr: 0.000001, time: 9.319387\n","iter: 23160, loss: 0.016667, lr: 0.000001, time: 8.658416\n","iter: 23180, loss: 0.007136, lr: 0.000001, time: 9.372673\n","iter: 23200, loss: 0.014000, lr: 0.000001, time: 8.643833\n","iter: 23220, loss: 0.013379, lr: 0.000001, time: 9.386866\n","iter: 23240, loss: 0.013668, lr: 0.000001, time: 8.657985\n","iter: 23260, loss: 0.010985, lr: 0.000001, time: 9.475425\n","iter: 23280, loss: 0.024581, lr: 0.000001, time: 8.668947\n","iter: 23300, loss: 0.008911, lr: 0.000001, time: 9.378365\n","iter: 23320, loss: 0.018838, lr: 0.000001, time: 8.661534\n","iter: 23340, loss: 0.016760, lr: 0.000001, time: 9.405000\n","iter: 23360, loss: 0.023924, lr: 0.000001, time: 8.700529\n","iter: 23380, loss: 0.014660, lr: 0.000001, time: 9.362383\n","iter: 23400, loss: 0.011288, lr: 0.000001, time: 8.673432\n","iter: 23420, loss: 0.009220, lr: 0.000001, time: 9.356877\n","iter: 23440, loss: 0.016887, lr: 0.000001, time: 8.684974\n","iter: 23460, loss: 0.010247, lr: 0.000001, time: 9.247848\n","iter: 23480, loss: 0.015690, lr: 0.000001, time: 8.692059\n","iter: 23500, loss: 0.018723, lr: 0.000001, time: 9.372367\n","iter: 23520, loss: 0.009333, lr: 0.000001, time: 8.667961\n","iter: 23540, loss: 0.015401, lr: 0.000001, time: 9.411129\n","iter: 23560, loss: 0.017751, lr: 0.000001, time: 8.678453\n","iter: 23580, loss: 0.046902, lr: 0.000001, time: 9.264267\n","iter: 23600, loss: 0.008520, lr: 0.000001, time: 8.655368\n","iter: 23620, loss: 0.020087, lr: 0.000001, time: 9.412284\n","iter: 23640, loss: 0.007285, lr: 0.000001, time: 8.685519\n","iter: 23660, loss: 0.022878, lr: 0.000001, time: 9.309985\n","iter: 23680, loss: 0.015972, lr: 0.000001, time: 8.678683\n","iter: 23700, loss: 0.012846, lr: 0.000001, time: 9.329726\n","iter: 23720, loss: 0.030045, lr: 0.000001, time: 8.684494\n","iter: 23740, loss: 0.015290, lr: 0.000001, time: 9.358560\n","iter: 23760, loss: 0.032708, lr: 0.000001, time: 8.693254\n","iter: 23780, loss: 0.028705, lr: 0.000001, time: 9.259552\n","iter: 23800, loss: 0.019310, lr: 0.000001, time: 8.738906\n","iter: 23820, loss: 0.015302, lr: 0.000001, time: 8.640118\n","iter: 23840, loss: 0.025943, lr: 0.000001, time: 9.337164\n","iter: 23860, loss: 0.007785, lr: 0.000001, time: 8.618232\n","iter: 23880, loss: 0.034221, lr: 0.000001, time: 9.390472\n","iter: 23900, loss: 0.019198, lr: 0.000001, time: 8.639084\n","iter: 23920, loss: 0.004075, lr: 0.000001, time: 9.402071\n","iter: 23940, loss: 0.022214, lr: 0.000001, time: 8.688890\n","iter: 23960, loss: 0.014530, lr: 0.000001, time: 9.258521\n","iter: 23980, loss: 0.015335, lr: 0.000001, time: 8.664069\n","iter: 24000, loss: 0.015740, lr: 0.000001, time: 9.301454\n","iter: 24020, loss: 0.017175, lr: 0.000001, time: 8.650437\n","iter: 24040, loss: 0.018081, lr: 0.000001, time: 9.416091\n","iter: 24060, loss: 0.021067, lr: 0.000001, time: 8.675693\n","iter: 24080, loss: 0.029864, lr: 0.000001, time: 9.250507\n","iter: 24100, loss: 0.016450, lr: 0.000001, time: 8.677905\n","iter: 24120, loss: 0.011340, lr: 0.000001, time: 9.355761\n","iter: 24140, loss: 0.025476, lr: 0.000001, time: 8.682196\n","iter: 24160, loss: 0.020609, lr: 0.000001, time: 9.245241\n","iter: 24180, loss: 0.018401, lr: 0.000001, time: 8.682253\n","iter: 24200, loss: 0.009115, lr: 0.000001, time: 9.263066\n","iter: 24220, loss: 0.012040, lr: 0.000001, time: 8.660676\n","iter: 24240, loss: 0.014200, lr: 0.000001, time: 9.397777\n","iter: 24260, loss: 0.025101, lr: 0.000000, time: 8.688157\n","iter: 24280, loss: 0.009519, lr: 0.000000, time: 9.389359\n","iter: 24300, loss: 0.008513, lr: 0.000000, time: 8.705005\n","iter: 24320, loss: 0.017970, lr: 0.000000, time: 9.326461\n","iter: 24340, loss: 0.018957, lr: 0.000000, time: 8.686829\n","iter: 24360, loss: 0.011935, lr: 0.000000, time: 9.122267\n","iter: 24380, loss: 0.017310, lr: 0.000000, time: 8.694487\n","iter: 24400, loss: 0.006051, lr: 0.000000, time: 9.316839\n","iter: 24420, loss: 0.012098, lr: 0.000000, time: 8.666162\n","iter: 24440, loss: 0.020304, lr: 0.000000, time: 9.450479\n","iter: 24460, loss: 0.017041, lr: 0.000000, time: 8.698907\n","iter: 24480, loss: 0.025947, lr: 0.000000, time: 9.354629\n","iter: 24500, loss: 0.015435, lr: 0.000000, time: 8.669479\n","iter: 24520, loss: 0.006198, lr: 0.000000, time: 9.300690\n","iter: 24540, loss: 0.020374, lr: 0.000000, time: 8.685516\n","iter: 24560, loss: 0.018870, lr: 0.000000, time: 9.305409\n","iter: 24580, loss: 0.010295, lr: 0.000000, time: 8.683812\n","iter: 24600, loss: 0.013510, lr: 0.000000, time: 9.349680\n","iter: 24620, loss: 0.019341, lr: 0.000000, time: 8.663451\n","iter: 24640, loss: 0.019474, lr: 0.000000, time: 8.617625\n","iter: 24660, loss: 0.008271, lr: 0.000000, time: 9.367859\n","iter: 24680, loss: 0.011399, lr: 0.000000, time: 8.611361\n","iter: 24700, loss: 0.033154, lr: 0.000000, time: 9.386818\n","iter: 24720, loss: 0.002854, lr: 0.000000, time: 8.642261\n","iter: 24740, loss: 0.017425, lr: 0.000000, time: 9.401299\n","iter: 24760, loss: 0.018259, lr: 0.000000, time: 8.659735\n","iter: 24780, loss: 0.013361, lr: 0.000000, time: 9.405261\n","iter: 24800, loss: 0.015073, lr: 0.000000, time: 8.629035\n","iter: 24820, loss: 0.005047, lr: 0.000000, time: 9.318731\n","iter: 24840, loss: 0.016294, lr: 0.000000, time: 8.639795\n","iter: 24860, loss: 0.009901, lr: 0.000000, time: 9.380274\n","iter: 24880, loss: 0.025606, lr: 0.000000, time: 8.655168\n","iter: 24900, loss: 0.006386, lr: 0.000000, time: 9.273645\n","iter: 24920, loss: 0.006209, lr: 0.000000, time: 8.666178\n","iter: 24940, loss: 0.009765, lr: 0.000000, time: 9.384945\n","iter: 24960, loss: 0.008425, lr: 0.000000, time: 8.649858\n","iter: 24980, loss: 0.032600, lr: 0.000000, time: 9.292446\n","saving trained model\n","everything finished\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ScMgSnCaI9nP","outputId":"d72c2da0-f005-442e-aed0-761ad62181d1","executionInfo":{"status":"ok","timestamp":1574672113460,"user_tz":-60,"elapsed":1317611,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n","create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 105 / 106  ...   completed\n","=======>  processing iter 616 / 617  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iKHhH5NZI9nS","outputId":"295eb752-26f1-4f6e-f83e-93a06548c911","executionInfo":{"status":"ok","timestamp":1574672285291,"user_tz":-60,"elapsed":170856,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:43<00:00, 21.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.7417072430419447, cmc is: [0.8738124  0.91270787 0.9376485  0.9483373  0.9578385 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EcW7wJMFsa2A","colab_type":"text"},"source":["## Extra work"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"34579b2f-4e7b-43bb-df33-7d5b11f2d3d7","executionInfo":{"status":"ok","timestamp":1574291465509,"user_tz":-60,"elapsed":793,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"hmbJo6ZXp0Yl","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["#checking GPU\n","print(torch.cuda.current_device())\n","print(torch.cuda.device(0))\n","print(torch.cuda.device_count())\n","print(torch.cuda.get_device_name(0))\n","print(torch.cuda.is_available())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","<torch.cuda.device object at 0x7f5d7975ff28>\n","1\n","Tesla P100-PCIE-16GB\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6LfoES3PIeEy","colab_type":"text"},"source":["Visualizing filters "]},{"cell_type":"code","metadata":{"id":"E--uAbaXIdkR","colab_type":"code","outputId":"8333d43d-9067-491e-8004-e455b31d5ccc","executionInfo":{"status":"ok","timestamp":1574678441321,"user_tz":-60,"elapsed":819,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["a = np.array([1., 1.])\n","filt_2 = a[:,None]*a[None,:]\n","b = np.array([1., 2., 1.])\n","filt_3 = b[:,None]*b[None,:]\n","c = np.array([1., 4., 6., 4., 1.])\n","filt_5 = c[:,None]*c[None,:]\n","\n","print(filt_2, \"\\n\",\n","      filt_3, \"\\n\", filt_5)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1. 1.]\n"," [1. 1.]] \n"," [[1. 2. 1.]\n"," [2. 4. 2.]\n"," [1. 2. 1.]] \n"," [[ 1.  4.  6.  4.  1.]\n"," [ 4. 16. 24. 16.  4.]\n"," [ 6. 24. 36. 24.  6.]\n"," [ 4. 16. 24. 16.  4.]\n"," [ 1.  4.  6.  4.  1.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GBxB9pQNJrPX","colab_type":"code","colab":{}},"source":["import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JITRNV0jKvKB","colab_type":"code","colab":{}},"source":["img = cv2.imread('images/Market1501/bounding_box_train/1099_c2s2_145927_04.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rf9qDBkeLQJJ","colab_type":"code","colab":{}},"source":["np.asarray(img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t4fxUxhJq0Pg","colab_type":"code","outputId":"eb584193-c582-4f70-da5f-59f730d0a7da","executionInfo":{"status":"error","timestamp":1574670750263,"user_tz":-60,"elapsed":13934,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":248}},"source":["store_path= \"./res/emb_query{}.pkl\".format(model_num)\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","## logging\n","FORMAT = '%(levelname)s %(filename)s:%(lineno)d: %(message)s'\n","logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n","logger = logging.getLogger(__name__)\n","## restore model\n","logger.info('restoring model')\n","model = net\n","#model = nn.DataParallel(model)\n","model = net.cuda()\n","model.module.load_state_dict(torch.load('./res/model{}.pkl'.format(model_num)))\n","model = nn.DataParallel(model)\n","model.eval()\n","\n","## load gallery dataset\n","batchsize = 32\n","ds = Market1501(pids_list=list(labels_query.fid), array=X_query, is_train = False)\n","dl = DataLoader(ds, batch_size = batchsize, drop_last = False, num_workers = 4)\n","\n","## embedding samples\n","logger.info('start embedding')\n","all_iter_nums = len(ds) // batchsize + 1\n","embeddings = []\n","label_ids = []\n","label_cams = []\n","for it, (img, lb_id, lb_cam) in enumerate(dl):\n","  print('\\r=======>  processing iter {} / {}'.format(it, all_iter_nums),\n","            end = '', flush = True)\n","  label_ids.append(lb_id)\n","  label_cams.append(lb_cam)\n","  embds = []\n","  for im in img:\n","      im = im.cuda()\n","      embd = model(im).detach().cpu().numpy()\n","      embds.append(embd)\n","  embed = sum(embds) / len(embds)\n","  embeddings.append(embed)\n","print('  ...   completed')\n","\n","embeddings = np.vstack(embeddings)\n","label_ids = np.hstack(label_ids)\n","label_cams = np.hstack(label_cams)\n","\n","## dump results\n","logger.info('dump embeddings')\n","embd_res = {'embeddings': embeddings, 'label_ids': label_ids, 'label_cams': label_cams}\n","with open(store_path, 'wb') as fw:\n","  pickle.dump(embd_res, fw)\n","\n","logger.info('embedding finished')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 4 / 106"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-70baa63de200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0membd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0membds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"5eS-FobdnHNv","colab_type":"text"},"source":["## Models not included in the paper\n","\n","Model 2 = baseline with stride = 1 in last layer\n","\n","Model 3 = Anti-aliased model, re-trained with stride = 2 in last layer\n","\n","Model 4 = AA model with extra connected layers and stride = 2 in last layer. Model uses wrong way of cutting of layers.\n","\n","Model 5 = AA model with extra connected layers, stride = 2 in last layer blurring before GAP. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jaAc9kiAm7hn"},"source":["### Model 2 \n","\n","Baseline model with last conv layer using stride = 1"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4v7nuUyum7hk","colab":{}},"source":["from own_code.backbone import EmbedNetwork"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_w6-jxWQm7hh","colab":{}},"source":["model_num = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PRTyj63Om7hc","colab":{}},"source":["## model and loss\n","logger.info('setting up backbone model and loss')\n","net = EmbedNetwork(pretrained_base=True).cuda()\n","net = nn.DataParallel(net)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sM4h2pYmm7ha","colab":{}},"source":["summary(net, (3,128,64))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D5HZxWJcm7hX","colab":{}},"source":["train(net = net, model_num = 100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0J3Oulrcm7hQ","colab":{}},"source":["embed(dataset = X_query, fids = labels_query.fid, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vDD04GDxm7hJ","colab":{}},"source":["embed(dataset = X_test, fids = labels_test.fid, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3k1Cj8E_m7g_","colab":{}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-EJgFosBl44B","colab_type":"text"},"source":["### Model 3\n","AA model with stride = 2 on last layer "]},{"cell_type":"code","metadata":{"id":"0-G1Cssk5wI9","colab_type":"code","colab":{}},"source":["model_num = 3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uy9ZwPHjXjyN","colab_type":"code","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    # shape [N, C]\n","    x = x.view(x.size(0), -1)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zHxA-6tqhAjr","colab":{}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net = nn.DataParallel(model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZvEN6Se2pIBL","colab_type":"text"},"source":["### Model 4\n","AA model with stride = 2 on last layer and extra layers added, similar to baseline"]},{"cell_type":"code","metadata":{"id":"6Pn5cUMQ5tpD","colab_type":"code","colab":{}},"source":["model_num = 4 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSzrsYZSaIER","colab_type":"code","colab":{}},"source":["filter_size = 3\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-1]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNTgXfzrru47","colab_type":"code","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embed = nn.Linear(in_features = 1024, out_features = 128).to(device)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyEO3jdBsZMd","colab_type":"code","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.fc_head = fc_head\n","    self.embed = embed\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048 )\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embed(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JegAvkNcX-VA","colab_type":"code","outputId":"2f173c91-b8af-464c-c1ac-8e92431711fe","executionInfo":{"status":"ok","timestamp":1574179549714,"user_tz":-60,"elapsed":1089,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net = nn.DataParallel(model)\n","summary(net, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157           [-1, 512, 10, 6]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n","AdaptiveAvgPool2d-187           [-1, 2048, 1, 1]               0\n","          Linear-188                 [-1, 1024]       2,098,176\n","     BatchNorm1d-189                 [-1, 1024]           2,048\n","            ReLU-190                 [-1, 1024]               0\n","   DenseNormReLU-191                 [-1, 1024]               0\n","          Linear-192                  [-1, 128]         131,200\n","           Model-193                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 55.01\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 153.29\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"f9d73c5a-e190-47b7-e59f-801f0697e4a4","executionInfo":{"status":"ok","timestamp":1574144344108,"user_tz":-60,"elapsed":27501463,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"BxzSMWb2fr0W","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(net = net, model_num = model_num)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["start training ...\n","iter: 20, loss: 1.458782, lr: 0.000300, time: 22.624931\n","iter: 40, loss: 1.247162, lr: 0.000300, time: 21.425703\n","iter: 60, loss: 1.076981, lr: 0.000300, time: 22.276768\n","iter: 80, loss: 0.953682, lr: 0.000300, time: 21.575980\n","iter: 100, loss: 0.844872, lr: 0.000300, time: 22.374335\n","iter: 120, loss: 0.801240, lr: 0.000300, time: 21.627635\n","iter: 140, loss: 0.774012, lr: 0.000300, time: 22.385483\n","iter: 160, loss: 0.743056, lr: 0.000300, time: 21.691047\n","iter: 180, loss: 0.734847, lr: 0.000300, time: 22.317383\n","iter: 200, loss: 0.726912, lr: 0.000300, time: 21.674103\n","iter: 220, loss: 0.745130, lr: 0.000300, time: 22.379704\n","iter: 240, loss: 0.735073, lr: 0.000300, time: 21.654823\n","iter: 260, loss: 0.711214, lr: 0.000300, time: 22.332100\n","iter: 280, loss: 0.742581, lr: 0.000300, time: 21.681024\n","iter: 300, loss: 0.727017, lr: 0.000300, time: 22.407910\n","iter: 320, loss: 0.707554, lr: 0.000300, time: 21.647517\n","iter: 340, loss: 0.731300, lr: 0.000300, time: 22.239858\n","iter: 360, loss: 0.695230, lr: 0.000300, time: 21.687057\n","iter: 380, loss: 0.761834, lr: 0.000300, time: 22.445347\n","iter: 400, loss: 0.731435, lr: 0.000300, time: 21.688585\n","iter: 420, loss: 0.706098, lr: 0.000300, time: 22.355298\n","iter: 440, loss: 0.713037, lr: 0.000300, time: 21.696029\n","iter: 460, loss: 0.691635, lr: 0.000300, time: 22.400776\n","iter: 480, loss: 0.697989, lr: 0.000300, time: 21.693631\n","iter: 500, loss: 0.662096, lr: 0.000300, time: 22.222048\n","iter: 520, loss: 0.674279, lr: 0.000300, time: 21.658110\n","iter: 540, loss: 0.677383, lr: 0.000300, time: 22.365610\n","iter: 560, loss: 0.699702, lr: 0.000300, time: 21.703902\n","iter: 580, loss: 0.657662, lr: 0.000300, time: 22.336874\n","iter: 600, loss: 0.682593, lr: 0.000300, time: 21.710696\n","iter: 620, loss: 0.606424, lr: 0.000300, time: 22.340300\n","iter: 640, loss: 0.640961, lr: 0.000300, time: 21.723759\n","iter: 660, loss: 0.642952, lr: 0.000300, time: 22.304933\n","iter: 680, loss: 0.637176, lr: 0.000300, time: 21.700060\n","iter: 700, loss: 0.612923, lr: 0.000300, time: 22.361518\n","iter: 720, loss: 0.655954, lr: 0.000300, time: 21.699818\n","iter: 740, loss: 0.596095, lr: 0.000300, time: 22.393596\n","iter: 760, loss: 0.585804, lr: 0.000300, time: 21.675782\n","iter: 780, loss: 0.589515, lr: 0.000300, time: 22.400054\n","iter: 800, loss: 0.579584, lr: 0.000300, time: 21.699478\n","iter: 820, loss: 0.614376, lr: 0.000300, time: 22.325330\n","iter: 840, loss: 0.591258, lr: 0.000300, time: 21.691352\n","iter: 860, loss: 0.613183, lr: 0.000300, time: 21.640598\n","iter: 880, loss: 0.564176, lr: 0.000300, time: 22.456211\n","iter: 900, loss: 0.602873, lr: 0.000300, time: 21.650562\n","iter: 920, loss: 0.574358, lr: 0.000300, time: 22.428013\n","iter: 940, loss: 0.596244, lr: 0.000300, time: 21.651014\n","iter: 960, loss: 0.585946, lr: 0.000300, time: 22.338268\n","iter: 980, loss: 0.491474, lr: 0.000300, time: 21.659675\n","iter: 1000, loss: 0.491758, lr: 0.000300, time: 22.403926\n","iter: 1020, loss: 0.492451, lr: 0.000300, time: 21.691466\n","iter: 1040, loss: 0.532958, lr: 0.000300, time: 22.425463\n","iter: 1060, loss: 0.582633, lr: 0.000300, time: 21.696128\n","iter: 1080, loss: 0.599858, lr: 0.000300, time: 22.350243\n","iter: 1100, loss: 0.556396, lr: 0.000300, time: 21.694091\n","iter: 1120, loss: 0.458656, lr: 0.000300, time: 22.346370\n","iter: 1140, loss: 0.590656, lr: 0.000300, time: 21.649427\n","iter: 1160, loss: 0.527878, lr: 0.000300, time: 22.342050\n","iter: 1180, loss: 0.524064, lr: 0.000300, time: 21.643296\n","iter: 1200, loss: 0.522856, lr: 0.000300, time: 22.283279\n","iter: 1220, loss: 0.504922, lr: 0.000300, time: 21.719514\n","iter: 1240, loss: 0.511333, lr: 0.000300, time: 22.374247\n","iter: 1260, loss: 0.478553, lr: 0.000300, time: 21.716166\n","iter: 1280, loss: 0.439962, lr: 0.000300, time: 22.432118\n","iter: 1300, loss: 0.438589, lr: 0.000300, time: 21.715716\n","iter: 1320, loss: 0.492897, lr: 0.000300, time: 22.453096\n","iter: 1340, loss: 0.456962, lr: 0.000300, time: 21.699008\n","iter: 1360, loss: 0.445814, lr: 0.000300, time: 22.340599\n","iter: 1380, loss: 0.413335, lr: 0.000300, time: 21.703730\n","iter: 1400, loss: 0.435456, lr: 0.000300, time: 22.338377\n","iter: 1420, loss: 0.500284, lr: 0.000300, time: 21.703660\n","iter: 1440, loss: 0.462762, lr: 0.000300, time: 22.312983\n","iter: 1460, loss: 0.440171, lr: 0.000300, time: 21.719348\n","iter: 1480, loss: 0.469362, lr: 0.000300, time: 22.355853\n","iter: 1500, loss: 0.457598, lr: 0.000300, time: 21.694358\n","iter: 1520, loss: 0.406987, lr: 0.000300, time: 22.211313\n","iter: 1540, loss: 0.482244, lr: 0.000300, time: 21.691762\n","iter: 1560, loss: 0.435974, lr: 0.000300, time: 22.416253\n","iter: 1580, loss: 0.418426, lr: 0.000300, time: 21.717090\n","iter: 1600, loss: 0.417336, lr: 0.000300, time: 22.399679\n","iter: 1620, loss: 0.409598, lr: 0.000300, time: 21.707004\n","iter: 1640, loss: 0.448345, lr: 0.000300, time: 22.310194\n","iter: 1660, loss: 0.359336, lr: 0.000300, time: 21.698961\n","iter: 1680, loss: 0.421028, lr: 0.000300, time: 21.632388\n","iter: 1700, loss: 0.420431, lr: 0.000300, time: 22.423254\n","iter: 1720, loss: 0.436242, lr: 0.000300, time: 21.637645\n","iter: 1740, loss: 0.419230, lr: 0.000300, time: 22.391879\n","iter: 1760, loss: 0.414510, lr: 0.000300, time: 21.670596\n","iter: 1780, loss: 0.406533, lr: 0.000300, time: 22.441046\n","iter: 1800, loss: 0.392479, lr: 0.000300, time: 21.667681\n","iter: 1820, loss: 0.400775, lr: 0.000300, time: 22.415021\n","iter: 1840, loss: 0.383815, lr: 0.000300, time: 21.673382\n","iter: 1860, loss: 0.293126, lr: 0.000300, time: 22.239902\n","iter: 1880, loss: 0.377728, lr: 0.000300, time: 21.661807\n","iter: 1900, loss: 0.394993, lr: 0.000300, time: 22.354807\n","iter: 1920, loss: 0.395701, lr: 0.000300, time: 21.714084\n","iter: 1940, loss: 0.374388, lr: 0.000300, time: 22.393728\n","iter: 1960, loss: 0.362164, lr: 0.000300, time: 21.701795\n","iter: 1980, loss: 0.352314, lr: 0.000300, time: 22.398694\n","iter: 2000, loss: 0.368700, lr: 0.000300, time: 21.704426\n","iter: 2020, loss: 0.287288, lr: 0.000300, time: 22.348824\n","iter: 2040, loss: 0.309045, lr: 0.000300, time: 21.710543\n","iter: 2060, loss: 0.364709, lr: 0.000300, time: 22.421044\n","iter: 2080, loss: 0.268605, lr: 0.000300, time: 21.693819\n","iter: 2100, loss: 0.262915, lr: 0.000300, time: 22.361473\n","iter: 2120, loss: 0.398514, lr: 0.000300, time: 21.690659\n","iter: 2140, loss: 0.265898, lr: 0.000300, time: 22.339252\n","iter: 2160, loss: 0.411081, lr: 0.000300, time: 21.682906\n","iter: 2180, loss: 0.396838, lr: 0.000300, time: 22.307585\n","iter: 2200, loss: 0.398644, lr: 0.000300, time: 21.697901\n","iter: 2220, loss: 0.386820, lr: 0.000300, time: 22.399096\n","iter: 2240, loss: 0.389277, lr: 0.000300, time: 21.688168\n","iter: 2260, loss: 0.405839, lr: 0.000300, time: 22.266311\n","iter: 2280, loss: 0.310736, lr: 0.000300, time: 21.694903\n","iter: 2300, loss: 0.358934, lr: 0.000300, time: 22.390385\n","iter: 2320, loss: 0.294572, lr: 0.000300, time: 21.661997\n","iter: 2340, loss: 0.342161, lr: 0.000300, time: 22.281316\n","iter: 2360, loss: 0.346580, lr: 0.000300, time: 21.667975\n","iter: 2380, loss: 0.353932, lr: 0.000300, time: 22.187598\n","iter: 2400, loss: 0.324444, lr: 0.000300, time: 21.656477\n","iter: 2420, loss: 0.378580, lr: 0.000300, time: 22.174258\n","iter: 2440, loss: 0.419529, lr: 0.000300, time: 21.641562\n","iter: 2460, loss: 0.327593, lr: 0.000300, time: 22.213727\n","iter: 2480, loss: 0.268480, lr: 0.000300, time: 21.703932\n","iter: 2500, loss: 0.292216, lr: 0.000300, time: 21.633023\n","iter: 2520, loss: 0.317555, lr: 0.000300, time: 22.420669\n","iter: 2540, loss: 0.334611, lr: 0.000300, time: 21.635879\n","iter: 2560, loss: 0.325956, lr: 0.000300, time: 22.303653\n","iter: 2580, loss: 0.397747, lr: 0.000300, time: 21.651465\n","iter: 2600, loss: 0.240914, lr: 0.000300, time: 22.360633\n","iter: 2620, loss: 0.299789, lr: 0.000300, time: 21.648439\n","iter: 2640, loss: 0.255998, lr: 0.000300, time: 22.428863\n","iter: 2660, loss: 0.299785, lr: 0.000300, time: 21.650722\n","iter: 2680, loss: 0.280890, lr: 0.000300, time: 22.438202\n","iter: 2700, loss: 0.310796, lr: 0.000300, time: 21.674142\n","iter: 2720, loss: 0.282449, lr: 0.000300, time: 22.303705\n","iter: 2740, loss: 0.297188, lr: 0.000300, time: 21.675323\n","iter: 2760, loss: 0.254972, lr: 0.000300, time: 22.338992\n","iter: 2780, loss: 0.276954, lr: 0.000300, time: 21.611768\n","iter: 2800, loss: 0.313556, lr: 0.000300, time: 22.355820\n","iter: 2820, loss: 0.287762, lr: 0.000300, time: 21.631346\n","iter: 2840, loss: 0.293891, lr: 0.000300, time: 22.318085\n","iter: 2860, loss: 0.299197, lr: 0.000300, time: 21.644455\n","iter: 2880, loss: 0.283228, lr: 0.000300, time: 22.354426\n","iter: 2900, loss: 0.288415, lr: 0.000300, time: 21.609668\n","iter: 2920, loss: 0.328046, lr: 0.000300, time: 22.256668\n","iter: 2940, loss: 0.243802, lr: 0.000300, time: 21.634368\n","iter: 2960, loss: 0.253876, lr: 0.000300, time: 22.303307\n","iter: 2980, loss: 0.264277, lr: 0.000300, time: 21.609494\n","iter: 3000, loss: 0.242595, lr: 0.000300, time: 22.358164\n","iter: 3020, loss: 0.229264, lr: 0.000300, time: 21.638338\n","iter: 3040, loss: 0.281972, lr: 0.000300, time: 22.277619\n","iter: 3060, loss: 0.255275, lr: 0.000300, time: 21.670157\n","iter: 3080, loss: 0.299050, lr: 0.000300, time: 22.361948\n","iter: 3100, loss: 0.272388, lr: 0.000300, time: 21.624637\n","iter: 3120, loss: 0.318324, lr: 0.000300, time: 22.346456\n","iter: 3140, loss: 0.278434, lr: 0.000300, time: 21.649515\n","iter: 3160, loss: 0.305074, lr: 0.000300, time: 22.327337\n","iter: 3180, loss: 0.252731, lr: 0.000300, time: 21.682454\n","iter: 3200, loss: 0.317870, lr: 0.000300, time: 22.311521\n","iter: 3220, loss: 0.253141, lr: 0.000300, time: 21.617796\n","iter: 3240, loss: 0.280182, lr: 0.000300, time: 22.361975\n","iter: 3260, loss: 0.255680, lr: 0.000300, time: 21.671173\n","iter: 3280, loss: 0.284264, lr: 0.000300, time: 22.360190\n","iter: 3300, loss: 0.244982, lr: 0.000300, time: 21.636287\n","iter: 3320, loss: 0.268742, lr: 0.000300, time: 21.570644\n","iter: 3340, loss: 0.289269, lr: 0.000300, time: 22.324078\n","iter: 3360, loss: 0.314532, lr: 0.000300, time: 21.562838\n","iter: 3380, loss: 0.270894, lr: 0.000300, time: 22.332157\n","iter: 3400, loss: 0.344174, lr: 0.000300, time: 21.568315\n","iter: 3420, loss: 0.243204, lr: 0.000300, time: 22.339808\n","iter: 3440, loss: 0.266288, lr: 0.000300, time: 21.611080\n","iter: 3460, loss: 0.278512, lr: 0.000300, time: 22.325198\n","iter: 3480, loss: 0.239066, lr: 0.000300, time: 21.662882\n","iter: 3500, loss: 0.227534, lr: 0.000300, time: 22.417188\n","iter: 3520, loss: 0.220129, lr: 0.000300, time: 21.615664\n","iter: 3540, loss: 0.214868, lr: 0.000300, time: 22.347424\n","iter: 3560, loss: 0.262473, lr: 0.000300, time: 21.601163\n","iter: 3580, loss: 0.221465, lr: 0.000300, time: 22.223130\n","iter: 3600, loss: 0.238110, lr: 0.000300, time: 21.614603\n","iter: 3620, loss: 0.220732, lr: 0.000300, time: 22.174244\n","iter: 3640, loss: 0.290873, lr: 0.000300, time: 21.587026\n","iter: 3660, loss: 0.238122, lr: 0.000300, time: 22.156895\n","iter: 3680, loss: 0.281749, lr: 0.000300, time: 21.596332\n","iter: 3700, loss: 0.271038, lr: 0.000300, time: 22.276889\n","iter: 3720, loss: 0.277194, lr: 0.000300, time: 21.597099\n","iter: 3740, loss: 0.310841, lr: 0.000300, time: 22.317595\n","iter: 3760, loss: 0.270732, lr: 0.000300, time: 21.603281\n","iter: 3780, loss: 0.180869, lr: 0.000300, time: 22.198755\n","iter: 3800, loss: 0.210531, lr: 0.000300, time: 21.616328\n","iter: 3820, loss: 0.200382, lr: 0.000300, time: 22.333541\n","iter: 3840, loss: 0.205584, lr: 0.000300, time: 21.613717\n","iter: 3860, loss: 0.304161, lr: 0.000300, time: 22.102341\n","iter: 3880, loss: 0.233652, lr: 0.000300, time: 21.578661\n","iter: 3900, loss: 0.283738, lr: 0.000300, time: 22.288143\n","iter: 3920, loss: 0.237266, lr: 0.000300, time: 21.599421\n","iter: 3940, loss: 0.208245, lr: 0.000300, time: 22.075189\n","iter: 3960, loss: 0.192140, lr: 0.000300, time: 21.634004\n","iter: 3980, loss: 0.217555, lr: 0.000300, time: 22.350116\n","iter: 4000, loss: 0.240695, lr: 0.000300, time: 21.608731\n","iter: 4020, loss: 0.321485, lr: 0.000300, time: 22.287385\n","iter: 4040, loss: 0.235185, lr: 0.000300, time: 21.659118\n","iter: 4060, loss: 0.255322, lr: 0.000300, time: 22.269827\n","iter: 4080, loss: 0.247351, lr: 0.000300, time: 21.617119\n","iter: 4100, loss: 0.246003, lr: 0.000300, time: 22.264585\n","iter: 4120, loss: 0.212911, lr: 0.000300, time: 21.578958\n","iter: 4140, loss: 0.260558, lr: 0.000300, time: 21.542433\n","iter: 4160, loss: 0.182825, lr: 0.000300, time: 22.307460\n","iter: 4180, loss: 0.239086, lr: 0.000300, time: 21.536674\n","iter: 4200, loss: 0.190525, lr: 0.000300, time: 22.338726\n","iter: 4220, loss: 0.211051, lr: 0.000300, time: 21.570923\n","iter: 4240, loss: 0.215572, lr: 0.000300, time: 22.373318\n","iter: 4260, loss: 0.239574, lr: 0.000300, time: 21.562964\n","iter: 4280, loss: 0.300927, lr: 0.000300, time: 22.252699\n","iter: 4300, loss: 0.189385, lr: 0.000300, time: 21.590891\n","iter: 4320, loss: 0.224217, lr: 0.000300, time: 22.276409\n","iter: 4340, loss: 0.195543, lr: 0.000300, time: 21.589928\n","iter: 4360, loss: 0.244926, lr: 0.000300, time: 22.333644\n","iter: 4380, loss: 0.257942, lr: 0.000300, time: 21.585897\n","iter: 4400, loss: 0.243787, lr: 0.000300, time: 22.367036\n","iter: 4420, loss: 0.203707, lr: 0.000300, time: 21.634341\n","iter: 4440, loss: 0.228766, lr: 0.000300, time: 22.326009\n","iter: 4460, loss: 0.170803, lr: 0.000300, time: 21.609877\n","iter: 4480, loss: 0.239016, lr: 0.000300, time: 22.329059\n","iter: 4500, loss: 0.238392, lr: 0.000300, time: 21.653290\n","iter: 4520, loss: 0.273538, lr: 0.000300, time: 22.303473\n","iter: 4540, loss: 0.206150, lr: 0.000300, time: 21.627418\n","iter: 4560, loss: 0.207912, lr: 0.000300, time: 22.266790\n","iter: 4580, loss: 0.249543, lr: 0.000300, time: 21.613141\n","iter: 4600, loss: 0.252200, lr: 0.000300, time: 22.323214\n","iter: 4620, loss: 0.194876, lr: 0.000300, time: 21.617616\n","iter: 4640, loss: 0.192443, lr: 0.000300, time: 22.225907\n","iter: 4660, loss: 0.221335, lr: 0.000300, time: 21.635536\n","iter: 4680, loss: 0.174679, lr: 0.000300, time: 22.350624\n","iter: 4700, loss: 0.193595, lr: 0.000300, time: 21.616715\n","iter: 4720, loss: 0.241708, lr: 0.000300, time: 22.171921\n","iter: 4740, loss: 0.181806, lr: 0.000300, time: 21.604541\n","iter: 4760, loss: 0.217986, lr: 0.000300, time: 22.240135\n","iter: 4780, loss: 0.172297, lr: 0.000300, time: 21.576155\n","iter: 4800, loss: 0.164111, lr: 0.000300, time: 22.138196\n","iter: 4820, loss: 0.183092, lr: 0.000300, time: 21.604321\n","iter: 4840, loss: 0.236018, lr: 0.000300, time: 22.371361\n","iter: 4860, loss: 0.219901, lr: 0.000300, time: 21.645559\n","iter: 4880, loss: 0.187545, lr: 0.000300, time: 22.062302\n","iter: 4900, loss: 0.203040, lr: 0.000300, time: 21.577196\n","iter: 4920, loss: 0.219705, lr: 0.000300, time: 22.281687\n","iter: 4940, loss: 0.234245, lr: 0.000300, time: 21.627128\n","iter: 4960, loss: 0.271017, lr: 0.000300, time: 21.565747\n","iter: 4980, loss: 0.236157, lr: 0.000300, time: 22.362433\n","iter: 5000, loss: 0.207071, lr: 0.000300, time: 21.604394\n","iter: 5020, loss: 0.200379, lr: 0.000300, time: 22.362609\n","iter: 5040, loss: 0.164080, lr: 0.000300, time: 21.580503\n","iter: 5060, loss: 0.215437, lr: 0.000300, time: 22.244899\n","iter: 5080, loss: 0.183965, lr: 0.000300, time: 21.631479\n","iter: 5100, loss: 0.205378, lr: 0.000300, time: 22.398568\n","iter: 5120, loss: 0.149130, lr: 0.000300, time: 21.649312\n","iter: 5140, loss: 0.159822, lr: 0.000300, time: 22.309951\n","iter: 5160, loss: 0.145789, lr: 0.000300, time: 21.619592\n","iter: 5180, loss: 0.166962, lr: 0.000300, time: 22.277375\n","iter: 5200, loss: 0.196821, lr: 0.000300, time: 21.674765\n","iter: 5220, loss: 0.181199, lr: 0.000300, time: 22.392863\n","iter: 5240, loss: 0.187642, lr: 0.000300, time: 21.675561\n","iter: 5260, loss: 0.140190, lr: 0.000300, time: 22.328918\n","iter: 5280, loss: 0.233661, lr: 0.000300, time: 21.688449\n","iter: 5300, loss: 0.203857, lr: 0.000300, time: 22.254520\n","iter: 5320, loss: 0.180119, lr: 0.000300, time: 21.685557\n","iter: 5340, loss: 0.154494, lr: 0.000300, time: 22.408796\n","iter: 5360, loss: 0.166550, lr: 0.000300, time: 21.672847\n","iter: 5380, loss: 0.220578, lr: 0.000300, time: 22.218339\n","iter: 5400, loss: 0.193862, lr: 0.000300, time: 21.676194\n","iter: 5420, loss: 0.178342, lr: 0.000300, time: 22.360854\n","iter: 5440, loss: 0.170176, lr: 0.000300, time: 21.701368\n","iter: 5460, loss: 0.165392, lr: 0.000300, time: 22.391408\n","iter: 5480, loss: 0.161115, lr: 0.000300, time: 21.655352\n","iter: 5500, loss: 0.187368, lr: 0.000300, time: 22.389186\n","iter: 5520, loss: 0.209078, lr: 0.000300, time: 21.728357\n","iter: 5540, loss: 0.180876, lr: 0.000300, time: 22.388722\n","iter: 5560, loss: 0.211217, lr: 0.000300, time: 21.718920\n","iter: 5580, loss: 0.213988, lr: 0.000300, time: 22.245170\n","iter: 5600, loss: 0.162995, lr: 0.000300, time: 21.736639\n","iter: 5620, loss: 0.185901, lr: 0.000300, time: 22.439802\n","iter: 5640, loss: 0.178506, lr: 0.000300, time: 21.720284\n","iter: 5660, loss: 0.209749, lr: 0.000300, time: 22.429260\n","iter: 5680, loss: 0.162866, lr: 0.000300, time: 21.764119\n","iter: 5700, loss: 0.157303, lr: 0.000300, time: 22.388981\n","iter: 5720, loss: 0.226028, lr: 0.000300, time: 21.715357\n","iter: 5740, loss: 0.206345, lr: 0.000300, time: 22.398415\n","iter: 5760, loss: 0.209223, lr: 0.000300, time: 21.607031\n","iter: 5780, loss: 0.162939, lr: 0.000300, time: 21.558828\n","iter: 5800, loss: 0.141214, lr: 0.000300, time: 22.412713\n","iter: 5820, loss: 0.171376, lr: 0.000300, time: 21.660893\n","iter: 5840, loss: 0.211078, lr: 0.000300, time: 22.229075\n","iter: 5860, loss: 0.154506, lr: 0.000300, time: 21.630657\n","iter: 5880, loss: 0.193717, lr: 0.000300, time: 22.442104\n","iter: 5900, loss: 0.132684, lr: 0.000300, time: 21.688133\n","iter: 5920, loss: 0.204720, lr: 0.000300, time: 22.362097\n","iter: 5940, loss: 0.224711, lr: 0.000300, time: 21.653275\n","iter: 5960, loss: 0.207228, lr: 0.000300, time: 22.365160\n","iter: 5980, loss: 0.157764, lr: 0.000300, time: 21.696742\n","iter: 6000, loss: 0.154574, lr: 0.000300, time: 22.404755\n","iter: 6020, loss: 0.217908, lr: 0.000300, time: 21.669752\n","iter: 6040, loss: 0.133740, lr: 0.000300, time: 22.288602\n","iter: 6060, loss: 0.157148, lr: 0.000300, time: 21.644850\n","iter: 6080, loss: 0.203000, lr: 0.000300, time: 22.391299\n","iter: 6100, loss: 0.151196, lr: 0.000300, time: 21.637589\n","iter: 6120, loss: 0.178702, lr: 0.000300, time: 22.345624\n","iter: 6140, loss: 0.211320, lr: 0.000300, time: 21.682785\n","iter: 6160, loss: 0.216797, lr: 0.000300, time: 22.184458\n","iter: 6180, loss: 0.161552, lr: 0.000300, time: 21.694960\n","iter: 6200, loss: 0.232727, lr: 0.000300, time: 22.331735\n","iter: 6220, loss: 0.207027, lr: 0.000300, time: 21.689855\n","iter: 6240, loss: 0.189133, lr: 0.000300, time: 22.397442\n","iter: 6260, loss: 0.179056, lr: 0.000300, time: 21.712481\n","iter: 6280, loss: 0.154164, lr: 0.000300, time: 22.266839\n","iter: 6300, loss: 0.175212, lr: 0.000300, time: 21.712467\n","iter: 6320, loss: 0.194510, lr: 0.000300, time: 22.402339\n","iter: 6340, loss: 0.144217, lr: 0.000300, time: 21.690791\n","iter: 6360, loss: 0.170574, lr: 0.000300, time: 22.466208\n","iter: 6380, loss: 0.139282, lr: 0.000300, time: 21.752959\n","iter: 6400, loss: 0.127104, lr: 0.000300, time: 22.257370\n","iter: 6420, loss: 0.181872, lr: 0.000300, time: 21.735818\n","iter: 6440, loss: 0.139544, lr: 0.000300, time: 22.458737\n","iter: 6460, loss: 0.135207, lr: 0.000300, time: 21.699087\n","iter: 6480, loss: 0.178279, lr: 0.000300, time: 22.250745\n","iter: 6500, loss: 0.174824, lr: 0.000300, time: 21.699476\n","iter: 6520, loss: 0.154787, lr: 0.000300, time: 22.444992\n","iter: 6540, loss: 0.150703, lr: 0.000300, time: 21.654993\n","iter: 6560, loss: 0.173272, lr: 0.000300, time: 22.250055\n","iter: 6580, loss: 0.184912, lr: 0.000300, time: 21.671135\n","iter: 6600, loss: 0.177427, lr: 0.000300, time: 21.617953\n","iter: 6620, loss: 0.188947, lr: 0.000300, time: 22.284744\n","iter: 6640, loss: 0.131476, lr: 0.000300, time: 21.625721\n","iter: 6660, loss: 0.161007, lr: 0.000300, time: 22.366951\n","iter: 6680, loss: 0.171019, lr: 0.000300, time: 21.677954\n","iter: 6700, loss: 0.150206, lr: 0.000300, time: 22.318086\n","iter: 6720, loss: 0.134082, lr: 0.000300, time: 21.643516\n","iter: 6740, loss: 0.136953, lr: 0.000300, time: 22.329149\n","iter: 6760, loss: 0.149959, lr: 0.000300, time: 21.649746\n","iter: 6780, loss: 0.138977, lr: 0.000300, time: 22.390898\n","iter: 6800, loss: 0.211153, lr: 0.000300, time: 21.636301\n","iter: 6820, loss: 0.130016, lr: 0.000300, time: 22.343720\n","iter: 6840, loss: 0.180931, lr: 0.000300, time: 21.699595\n","iter: 6860, loss: 0.154847, lr: 0.000300, time: 22.225165\n","iter: 6880, loss: 0.137161, lr: 0.000300, time: 21.652860\n","iter: 6900, loss: 0.165011, lr: 0.000300, time: 22.308490\n","iter: 6920, loss: 0.188605, lr: 0.000300, time: 21.668758\n","iter: 6940, loss: 0.206166, lr: 0.000300, time: 22.419069\n","iter: 6960, loss: 0.178759, lr: 0.000300, time: 21.652655\n","iter: 6980, loss: 0.133128, lr: 0.000300, time: 22.383152\n","iter: 7000, loss: 0.160149, lr: 0.000300, time: 21.670455\n","iter: 7020, loss: 0.153532, lr: 0.000300, time: 22.411375\n","iter: 7040, loss: 0.136637, lr: 0.000300, time: 21.616534\n","iter: 7060, loss: 0.206991, lr: 0.000300, time: 22.267911\n","iter: 7080, loss: 0.153157, lr: 0.000300, time: 21.676393\n","iter: 7100, loss: 0.133110, lr: 0.000300, time: 22.380333\n","iter: 7120, loss: 0.140132, lr: 0.000300, time: 21.716961\n","iter: 7140, loss: 0.137292, lr: 0.000300, time: 22.346545\n","iter: 7160, loss: 0.109664, lr: 0.000300, time: 21.687829\n","iter: 7180, loss: 0.142442, lr: 0.000300, time: 22.335813\n","iter: 7200, loss: 0.125540, lr: 0.000300, time: 21.655046\n","iter: 7220, loss: 0.157105, lr: 0.000300, time: 22.356836\n","iter: 7240, loss: 0.134609, lr: 0.000300, time: 21.653805\n","iter: 7260, loss: 0.144970, lr: 0.000300, time: 22.314407\n","iter: 7280, loss: 0.132098, lr: 0.000300, time: 21.671206\n","iter: 7300, loss: 0.121460, lr: 0.000300, time: 22.330559\n","iter: 7320, loss: 0.178869, lr: 0.000300, time: 21.620598\n","iter: 7340, loss: 0.138751, lr: 0.000300, time: 22.286065\n","iter: 7360, loss: 0.146820, lr: 0.000300, time: 21.647951\n","iter: 7380, loss: 0.151507, lr: 0.000300, time: 22.283241\n","iter: 7400, loss: 0.213744, lr: 0.000300, time: 21.639568\n","iter: 7420, loss: 0.213491, lr: 0.000300, time: 21.586744\n","iter: 7440, loss: 0.168494, lr: 0.000300, time: 22.418145\n","iter: 7460, loss: 0.172579, lr: 0.000300, time: 21.592839\n","iter: 7480, loss: 0.191178, lr: 0.000300, time: 22.281475\n","iter: 7500, loss: 0.198241, lr: 0.000300, time: 21.632074\n","iter: 7520, loss: 0.192108, lr: 0.000300, time: 22.200786\n","iter: 7540, loss: 0.174561, lr: 0.000300, time: 21.638393\n","iter: 7560, loss: 0.110030, lr: 0.000300, time: 22.324974\n","iter: 7580, loss: 0.164367, lr: 0.000300, time: 21.628319\n","iter: 7600, loss: 0.146576, lr: 0.000300, time: 22.260519\n","iter: 7620, loss: 0.150891, lr: 0.000300, time: 21.610787\n","iter: 7640, loss: 0.199626, lr: 0.000300, time: 22.249060\n","iter: 7660, loss: 0.161001, lr: 0.000300, time: 21.600124\n","iter: 7680, loss: 0.153211, lr: 0.000300, time: 22.354887\n","iter: 7700, loss: 0.227429, lr: 0.000300, time: 21.667281\n","iter: 7720, loss: 0.197318, lr: 0.000300, time: 22.209211\n","iter: 7740, loss: 0.175523, lr: 0.000300, time: 21.650720\n","iter: 7760, loss: 0.206340, lr: 0.000300, time: 22.167387\n","iter: 7780, loss: 0.148945, lr: 0.000300, time: 21.663105\n","iter: 7800, loss: 0.195309, lr: 0.000300, time: 22.165230\n","iter: 7820, loss: 0.138657, lr: 0.000300, time: 21.655884\n","iter: 7840, loss: 0.177152, lr: 0.000300, time: 22.362743\n","iter: 7860, loss: 0.164684, lr: 0.000300, time: 21.631446\n","iter: 7880, loss: 0.121821, lr: 0.000300, time: 22.193545\n","iter: 7900, loss: 0.200888, lr: 0.000300, time: 21.637763\n","iter: 7920, loss: 0.143763, lr: 0.000300, time: 22.353890\n","iter: 7940, loss: 0.110200, lr: 0.000300, time: 21.646734\n","iter: 7960, loss: 0.130070, lr: 0.000300, time: 22.338331\n","iter: 7980, loss: 0.103980, lr: 0.000300, time: 21.641532\n","iter: 8000, loss: 0.126301, lr: 0.000300, time: 22.304527\n","iter: 8020, loss: 0.094820, lr: 0.000300, time: 21.591786\n","iter: 8040, loss: 0.092930, lr: 0.000300, time: 22.355603\n","iter: 8060, loss: 0.141611, lr: 0.000300, time: 21.605099\n","iter: 8080, loss: 0.149181, lr: 0.000300, time: 22.337318\n","iter: 8100, loss: 0.138233, lr: 0.000300, time: 21.616881\n","iter: 8120, loss: 0.105825, lr: 0.000300, time: 22.292940\n","iter: 8140, loss: 0.127765, lr: 0.000300, time: 21.587515\n","iter: 8160, loss: 0.112077, lr: 0.000300, time: 22.318750\n","iter: 8180, loss: 0.157636, lr: 0.000300, time: 21.641657\n","iter: 8200, loss: 0.155072, lr: 0.000300, time: 22.268405\n","iter: 8220, loss: 0.190953, lr: 0.000300, time: 21.594905\n","iter: 8240, loss: 0.200476, lr: 0.000300, time: 21.581898\n","iter: 8260, loss: 0.161132, lr: 0.000300, time: 22.284293\n","iter: 8280, loss: 0.152334, lr: 0.000300, time: 21.606963\n","iter: 8300, loss: 0.093954, lr: 0.000300, time: 22.363046\n","iter: 8320, loss: 0.155323, lr: 0.000300, time: 21.594466\n","iter: 8340, loss: 0.169185, lr: 0.000300, time: 22.219202\n","iter: 8360, loss: 0.154093, lr: 0.000300, time: 21.621482\n","iter: 8380, loss: 0.127508, lr: 0.000300, time: 22.387326\n","iter: 8400, loss: 0.130572, lr: 0.000300, time: 21.647338\n","iter: 8420, loss: 0.155184, lr: 0.000300, time: 22.424058\n","iter: 8440, loss: 0.121090, lr: 0.000300, time: 21.630030\n","iter: 8460, loss: 0.151393, lr: 0.000300, time: 22.217610\n","iter: 8480, loss: 0.106349, lr: 0.000300, time: 21.655854\n","iter: 8500, loss: 0.155435, lr: 0.000300, time: 22.374568\n","iter: 8520, loss: 0.107730, lr: 0.000300, time: 21.660438\n","iter: 8540, loss: 0.134376, lr: 0.000300, time: 22.362693\n","iter: 8560, loss: 0.138014, lr: 0.000300, time: 21.631642\n","iter: 8580, loss: 0.107645, lr: 0.000300, time: 22.353074\n","iter: 8600, loss: 0.104893, lr: 0.000300, time: 21.630611\n","iter: 8620, loss: 0.136305, lr: 0.000300, time: 22.357003\n","iter: 8640, loss: 0.143548, lr: 0.000300, time: 21.679609\n","iter: 8660, loss: 0.174608, lr: 0.000300, time: 22.363250\n","iter: 8680, loss: 0.133701, lr: 0.000300, time: 21.658998\n","iter: 8700, loss: 0.145054, lr: 0.000300, time: 22.283917\n","iter: 8720, loss: 0.159353, lr: 0.000300, time: 21.653155\n","iter: 8740, loss: 0.166165, lr: 0.000300, time: 22.336946\n","iter: 8760, loss: 0.178272, lr: 0.000300, time: 21.697303\n","iter: 8780, loss: 0.117622, lr: 0.000300, time: 22.408060\n","iter: 8800, loss: 0.144451, lr: 0.000300, time: 21.677864\n","iter: 8820, loss: 0.103217, lr: 0.000300, time: 22.364187\n","iter: 8840, loss: 0.120124, lr: 0.000300, time: 21.678057\n","iter: 8860, loss: 0.159408, lr: 0.000300, time: 22.417857\n","iter: 8880, loss: 0.104695, lr: 0.000300, time: 21.651241\n","iter: 8900, loss: 0.161537, lr: 0.000300, time: 22.266471\n","iter: 8920, loss: 0.160304, lr: 0.000300, time: 21.633142\n","iter: 8940, loss: 0.119408, lr: 0.000300, time: 22.301769\n","iter: 8960, loss: 0.147963, lr: 0.000300, time: 21.613696\n","iter: 8980, loss: 0.202542, lr: 0.000300, time: 22.336618\n","iter: 9000, loss: 0.151965, lr: 0.000300, time: 21.677247\n","iter: 9020, loss: 0.136347, lr: 0.000300, time: 22.270744\n","iter: 9040, loss: 0.110374, lr: 0.000300, time: 21.676913\n","iter: 9060, loss: 0.114446, lr: 0.000300, time: 21.635990\n","iter: 9080, loss: 0.145330, lr: 0.000300, time: 22.459799\n","iter: 9100, loss: 0.133841, lr: 0.000300, time: 21.628433\n","iter: 9120, loss: 0.177950, lr: 0.000300, time: 22.422917\n","iter: 9140, loss: 0.135969, lr: 0.000300, time: 21.643256\n","iter: 9160, loss: 0.122126, lr: 0.000300, time: 22.419364\n","iter: 9180, loss: 0.122874, lr: 0.000300, time: 21.657833\n","iter: 9200, loss: 0.115981, lr: 0.000300, time: 22.258954\n","iter: 9220, loss: 0.112374, lr: 0.000300, time: 21.656956\n","iter: 9240, loss: 0.140541, lr: 0.000300, time: 22.294787\n","iter: 9260, loss: 0.124893, lr: 0.000300, time: 21.663012\n","iter: 9280, loss: 0.102107, lr: 0.000300, time: 22.418449\n","iter: 9300, loss: 0.131619, lr: 0.000300, time: 21.692482\n","iter: 9320, loss: 0.100037, lr: 0.000300, time: 22.372766\n","iter: 9340, loss: 0.158214, lr: 0.000300, time: 21.657950\n","iter: 9360, loss: 0.117769, lr: 0.000300, time: 22.334177\n","iter: 9380, loss: 0.095853, lr: 0.000300, time: 21.687708\n","iter: 9400, loss: 0.164138, lr: 0.000300, time: 22.365897\n","iter: 9420, loss: 0.097676, lr: 0.000300, time: 21.684859\n","iter: 9440, loss: 0.132128, lr: 0.000300, time: 22.232041\n","iter: 9460, loss: 0.120303, lr: 0.000300, time: 21.639116\n","iter: 9480, loss: 0.101665, lr: 0.000300, time: 22.300161\n","iter: 9500, loss: 0.120908, lr: 0.000300, time: 21.588971\n","iter: 9520, loss: 0.113853, lr: 0.000300, time: 22.374282\n","iter: 9540, loss: 0.102188, lr: 0.000300, time: 21.651798\n","iter: 9560, loss: 0.150654, lr: 0.000300, time: 22.372773\n","iter: 9580, loss: 0.174771, lr: 0.000300, time: 21.643109\n","iter: 9600, loss: 0.166523, lr: 0.000300, time: 22.346033\n","iter: 9620, loss: 0.106867, lr: 0.000300, time: 21.625094\n","iter: 9640, loss: 0.125003, lr: 0.000300, time: 22.383787\n","iter: 9660, loss: 0.109960, lr: 0.000300, time: 21.651042\n","iter: 9680, loss: 0.136634, lr: 0.000300, time: 22.320068\n","iter: 9700, loss: 0.120075, lr: 0.000300, time: 21.637772\n","iter: 9720, loss: 0.141581, lr: 0.000300, time: 22.363147\n","iter: 9740, loss: 0.149996, lr: 0.000300, time: 21.671436\n","iter: 9760, loss: 0.140998, lr: 0.000300, time: 22.217545\n","iter: 9780, loss: 0.154585, lr: 0.000300, time: 21.645890\n","iter: 9800, loss: 0.129373, lr: 0.000300, time: 22.344114\n","iter: 9820, loss: 0.105091, lr: 0.000300, time: 21.657351\n","iter: 9840, loss: 0.173424, lr: 0.000300, time: 22.391771\n","iter: 9860, loss: 0.178618, lr: 0.000300, time: 21.666250\n","iter: 9880, loss: 0.117714, lr: 0.000300, time: 21.641805\n","iter: 9900, loss: 0.118278, lr: 0.000300, time: 22.306224\n","iter: 9920, loss: 0.149823, lr: 0.000300, time: 21.582948\n","iter: 9940, loss: 0.114183, lr: 0.000300, time: 22.359277\n","iter: 9960, loss: 0.108377, lr: 0.000300, time: 21.592440\n","iter: 9980, loss: 0.114205, lr: 0.000300, time: 22.314084\n","iter: 10000, loss: 0.164738, lr: 0.000300, time: 21.644826\n","iter: 10020, loss: 0.138885, lr: 0.000300, time: 22.380909\n","iter: 10040, loss: 0.143505, lr: 0.000300, time: 21.644294\n","iter: 10060, loss: 0.143949, lr: 0.000300, time: 22.379197\n","iter: 10080, loss: 0.132933, lr: 0.000300, time: 21.640008\n","iter: 10100, loss: 0.122859, lr: 0.000300, time: 22.384741\n","iter: 10120, loss: 0.146718, lr: 0.000300, time: 21.659106\n","iter: 10140, loss: 0.098746, lr: 0.000300, time: 22.304005\n","iter: 10160, loss: 0.138448, lr: 0.000300, time: 21.653701\n","iter: 10180, loss: 0.162896, lr: 0.000300, time: 22.295596\n","iter: 10200, loss: 0.146910, lr: 0.000300, time: 21.665897\n","iter: 10220, loss: 0.206976, lr: 0.000300, time: 22.273094\n","iter: 10240, loss: 0.117081, lr: 0.000300, time: 21.685004\n","iter: 10260, loss: 0.100563, lr: 0.000300, time: 22.336901\n","iter: 10280, loss: 0.102831, lr: 0.000300, time: 21.668391\n","iter: 10300, loss: 0.110300, lr: 0.000300, time: 22.270972\n","iter: 10320, loss: 0.113969, lr: 0.000300, time: 21.644902\n","iter: 10340, loss: 0.150153, lr: 0.000300, time: 22.318557\n","iter: 10360, loss: 0.115337, lr: 0.000300, time: 21.671119\n","iter: 10380, loss: 0.161809, lr: 0.000300, time: 22.366039\n","iter: 10400, loss: 0.135220, lr: 0.000300, time: 21.659299\n","iter: 10420, loss: 0.150862, lr: 0.000300, time: 22.238537\n","iter: 10440, loss: 0.142286, lr: 0.000300, time: 21.647553\n","iter: 10460, loss: 0.130471, lr: 0.000300, time: 22.386311\n","iter: 10480, loss: 0.095412, lr: 0.000300, time: 21.654001\n","iter: 10500, loss: 0.120792, lr: 0.000300, time: 22.368100\n","iter: 10520, loss: 0.115251, lr: 0.000300, time: 21.705891\n","iter: 10540, loss: 0.128587, lr: 0.000300, time: 22.349667\n","iter: 10560, loss: 0.119289, lr: 0.000300, time: 21.659102\n","iter: 10580, loss: 0.124230, lr: 0.000300, time: 22.376569\n","iter: 10600, loss: 0.148901, lr: 0.000300, time: 21.684078\n","iter: 10620, loss: 0.104156, lr: 0.000300, time: 22.375716\n","iter: 10640, loss: 0.135682, lr: 0.000300, time: 21.673388\n","iter: 10660, loss: 0.144149, lr: 0.000300, time: 22.286997\n","iter: 10680, loss: 0.118089, lr: 0.000300, time: 21.680595\n","iter: 10700, loss: 0.108206, lr: 0.000300, time: 21.620514\n","iter: 10720, loss: 0.109206, lr: 0.000300, time: 22.396883\n","iter: 10740, loss: 0.117559, lr: 0.000300, time: 21.639818\n","iter: 10760, loss: 0.080237, lr: 0.000300, time: 22.410400\n","iter: 10780, loss: 0.149302, lr: 0.000300, time: 21.646769\n","iter: 10800, loss: 0.099144, lr: 0.000300, time: 22.471681\n","iter: 10820, loss: 0.116437, lr: 0.000300, time: 21.624665\n","iter: 10840, loss: 0.114442, lr: 0.000300, time: 22.422360\n","iter: 10860, loss: 0.112972, lr: 0.000300, time: 21.659523\n","iter: 10880, loss: 0.090246, lr: 0.000300, time: 22.395565\n","iter: 10900, loss: 0.094228, lr: 0.000300, time: 21.685637\n","iter: 10920, loss: 0.141833, lr: 0.000300, time: 22.324012\n","iter: 10940, loss: 0.155561, lr: 0.000300, time: 21.675429\n","iter: 10960, loss: 0.156167, lr: 0.000300, time: 22.311399\n","iter: 10980, loss: 0.150976, lr: 0.000300, time: 21.650002\n","iter: 11000, loss: 0.132870, lr: 0.000300, time: 22.156026\n","iter: 11020, loss: 0.151879, lr: 0.000300, time: 21.660228\n","iter: 11040, loss: 0.134574, lr: 0.000300, time: 22.188095\n","iter: 11060, loss: 0.144474, lr: 0.000300, time: 21.669152\n","iter: 11080, loss: 0.120942, lr: 0.000300, time: 22.184927\n","iter: 11100, loss: 0.108370, lr: 0.000300, time: 21.682254\n","iter: 11120, loss: 0.114105, lr: 0.000300, time: 22.379075\n","iter: 11140, loss: 0.131958, lr: 0.000300, time: 21.660694\n","iter: 11160, loss: 0.097211, lr: 0.000300, time: 22.419313\n","iter: 11180, loss: 0.106674, lr: 0.000300, time: 21.652535\n","iter: 11200, loss: 0.126880, lr: 0.000300, time: 22.340576\n","iter: 11220, loss: 0.085324, lr: 0.000300, time: 21.698849\n","iter: 11240, loss: 0.088278, lr: 0.000300, time: 22.395641\n","iter: 11260, loss: 0.088739, lr: 0.000300, time: 21.658002\n","iter: 11280, loss: 0.070755, lr: 0.000300, time: 22.382971\n","iter: 11300, loss: 0.090180, lr: 0.000300, time: 21.698960\n","iter: 11320, loss: 0.091777, lr: 0.000300, time: 22.426897\n","iter: 11340, loss: 0.116345, lr: 0.000300, time: 21.678466\n","iter: 11360, loss: 0.101817, lr: 0.000300, time: 22.419077\n","iter: 11380, loss: 0.108361, lr: 0.000300, time: 21.695406\n","iter: 11400, loss: 0.135478, lr: 0.000300, time: 22.420725\n","iter: 11420, loss: 0.101871, lr: 0.000300, time: 21.665700\n","iter: 11440, loss: 0.081688, lr: 0.000300, time: 22.232876\n","iter: 11460, loss: 0.121164, lr: 0.000300, time: 21.694306\n","iter: 11480, loss: 0.118201, lr: 0.000300, time: 22.345788\n","iter: 11500, loss: 0.103575, lr: 0.000300, time: 21.653404\n","iter: 11520, loss: 0.113548, lr: 0.000300, time: 21.629007\n","iter: 11540, loss: 0.113690, lr: 0.000300, time: 22.385913\n","iter: 11560, loss: 0.138372, lr: 0.000300, time: 21.617296\n","iter: 11580, loss: 0.090865, lr: 0.000300, time: 22.343297\n","iter: 11600, loss: 0.135480, lr: 0.000300, time: 21.647341\n","iter: 11620, loss: 0.087848, lr: 0.000300, time: 22.264600\n","iter: 11640, loss: 0.076269, lr: 0.000300, time: 21.663392\n","iter: 11680, loss: 0.108431, lr: 0.000300, time: 21.641718\n","iter: 11700, loss: 0.130697, lr: 0.000300, time: 22.382775\n","iter: 11720, loss: 0.085064, lr: 0.000300, time: 21.617770\n","iter: 11740, loss: 0.101244, lr: 0.000300, time: 22.424494\n","iter: 11760, loss: 0.115498, lr: 0.000300, time: 21.637030\n","iter: 11780, loss: 0.149414, lr: 0.000300, time: 22.219444\n","iter: 11800, loss: 0.108762, lr: 0.000300, time: 21.619728\n","iter: 11820, loss: 0.126308, lr: 0.000300, time: 22.380449\n","iter: 11840, loss: 0.131301, lr: 0.000300, time: 21.646539\n","iter: 11860, loss: 0.132509, lr: 0.000300, time: 22.330752\n","iter: 11880, loss: 0.080527, lr: 0.000300, time: 21.650374\n","iter: 11900, loss: 0.061562, lr: 0.000300, time: 22.340831\n","iter: 11920, loss: 0.098155, lr: 0.000300, time: 21.632994\n","iter: 11940, loss: 0.109616, lr: 0.000300, time: 22.341973\n","iter: 11960, loss: 0.148913, lr: 0.000300, time: 21.641299\n","iter: 11980, loss: 0.119075, lr: 0.000300, time: 22.359767\n","iter: 12000, loss: 0.075038, lr: 0.000300, time: 21.622118\n","iter: 12020, loss: 0.080726, lr: 0.000300, time: 22.342329\n","iter: 12040, loss: 0.098626, lr: 0.000300, time: 21.637713\n","iter: 12060, loss: 0.090558, lr: 0.000300, time: 22.354761\n","iter: 12080, loss: 0.124615, lr: 0.000300, time: 21.628417\n","iter: 12100, loss: 0.090068, lr: 0.000300, time: 22.370192\n","iter: 12120, loss: 0.103921, lr: 0.000300, time: 21.641362\n","iter: 12140, loss: 0.074786, lr: 0.000300, time: 22.327765\n","iter: 12160, loss: 0.118708, lr: 0.000300, time: 21.637380\n","iter: 12180, loss: 0.095041, lr: 0.000300, time: 22.353950\n","iter: 12200, loss: 0.081532, lr: 0.000300, time: 21.622892\n","iter: 12220, loss: 0.081538, lr: 0.000300, time: 22.310794\n","iter: 12240, loss: 0.103117, lr: 0.000300, time: 21.642976\n","iter: 12260, loss: 0.103402, lr: 0.000300, time: 22.360706\n","iter: 12280, loss: 0.082577, lr: 0.000300, time: 21.667266\n","iter: 12300, loss: 0.111256, lr: 0.000300, time: 22.234775\n","iter: 12320, loss: 0.145934, lr: 0.000300, time: 21.658517\n","iter: 12340, loss: 0.115069, lr: 0.000300, time: 21.639474\n","iter: 12360, loss: 0.120744, lr: 0.000300, time: 22.289928\n","iter: 12380, loss: 0.109827, lr: 0.000300, time: 21.640779\n","iter: 12400, loss: 0.111192, lr: 0.000300, time: 22.432397\n","iter: 12420, loss: 0.114206, lr: 0.000300, time: 21.630975\n","iter: 12440, loss: 0.149292, lr: 0.000300, time: 22.507562\n","iter: 12460, loss: 0.071024, lr: 0.000300, time: 21.634743\n","iter: 12480, loss: 0.136053, lr: 0.000300, time: 22.405018\n","iter: 12500, loss: 0.092464, lr: 0.000300, time: 21.627801\n","iter: 12520, loss: 0.139081, lr: 0.000300, time: 22.261190\n","iter: 12540, loss: 0.138269, lr: 0.000300, time: 21.667646\n","iter: 12560, loss: 0.153839, lr: 0.000300, time: 22.268511\n","iter: 12580, loss: 0.124314, lr: 0.000300, time: 21.674090\n","iter: 12600, loss: 0.115748, lr: 0.000300, time: 22.345502\n","iter: 12620, loss: 0.110982, lr: 0.000300, time: 21.659400\n","iter: 12640, loss: 0.115460, lr: 0.000300, time: 22.324976\n","iter: 12660, loss: 0.140015, lr: 0.000300, time: 21.651923\n","iter: 12680, loss: 0.066374, lr: 0.000300, time: 22.319256\n","iter: 12700, loss: 0.070666, lr: 0.000300, time: 21.661080\n","iter: 12720, loss: 0.094752, lr: 0.000300, time: 22.177830\n","iter: 12740, loss: 0.079884, lr: 0.000300, time: 21.645328\n","iter: 12760, loss: 0.064936, lr: 0.000300, time: 22.317451\n","iter: 12780, loss: 0.121452, lr: 0.000300, time: 21.652928\n","iter: 12800, loss: 0.068622, lr: 0.000300, time: 22.189836\n","iter: 12820, loss: 0.083716, lr: 0.000300, time: 21.643059\n","iter: 12840, loss: 0.098402, lr: 0.000300, time: 22.376745\n","iter: 12860, loss: 0.120994, lr: 0.000300, time: 21.646373\n","iter: 12880, loss: 0.084179, lr: 0.000300, time: 22.318223\n","iter: 12900, loss: 0.078662, lr: 0.000300, time: 21.642151\n","iter: 12920, loss: 0.122440, lr: 0.000300, time: 22.346424\n","iter: 12940, loss: 0.106958, lr: 0.000300, time: 21.632302\n","iter: 12960, loss: 0.109622, lr: 0.000300, time: 22.330818\n","iter: 12980, loss: 0.153288, lr: 0.000300, time: 21.657236\n","iter: 13000, loss: 0.111618, lr: 0.000300, time: 22.379622\n","iter: 13020, loss: 0.139694, lr: 0.000300, time: 21.652581\n","iter: 13040, loss: 0.131297, lr: 0.000300, time: 22.321569\n","iter: 13060, loss: 0.112033, lr: 0.000300, time: 21.635481\n","iter: 13080, loss: 0.128765, lr: 0.000300, time: 22.331044\n","iter: 13100, loss: 0.110748, lr: 0.000300, time: 21.655993\n","iter: 13120, loss: 0.098249, lr: 0.000300, time: 22.300196\n","iter: 13140, loss: 0.100816, lr: 0.000300, time: 21.612541\n","iter: 13160, loss: 0.119895, lr: 0.000300, time: 21.489700\n","iter: 13180, loss: 0.085691, lr: 0.000300, time: 22.278445\n","iter: 13200, loss: 0.119254, lr: 0.000300, time: 21.513535\n","iter: 13220, loss: 0.113536, lr: 0.000300, time: 22.192448\n","iter: 13240, loss: 0.094178, lr: 0.000300, time: 21.491099\n","iter: 13260, loss: 0.114225, lr: 0.000300, time: 22.183844\n","iter: 13280, loss: 0.122045, lr: 0.000300, time: 21.492933\n","iter: 13300, loss: 0.143236, lr: 0.000300, time: 22.289128\n","iter: 13320, loss: 0.107459, lr: 0.000300, time: 21.620620\n","iter: 13340, loss: 0.110885, lr: 0.000300, time: 22.408816\n","iter: 13360, loss: 0.096422, lr: 0.000300, time: 21.651714\n","iter: 13380, loss: 0.081889, lr: 0.000300, time: 22.411467\n","iter: 13400, loss: 0.072777, lr: 0.000300, time: 21.641527\n","iter: 13420, loss: 0.105407, lr: 0.000300, time: 22.382460\n","iter: 13440, loss: 0.114205, lr: 0.000300, time: 21.661957\n","iter: 13460, loss: 0.107442, lr: 0.000300, time: 22.364957\n","iter: 13480, loss: 0.114219, lr: 0.000300, time: 21.656845\n","iter: 13500, loss: 0.159120, lr: 0.000300, time: 22.344718\n","iter: 13520, loss: 0.119974, lr: 0.000300, time: 21.664693\n","iter: 13540, loss: 0.101719, lr: 0.000300, time: 22.338514\n","iter: 13560, loss: 0.085158, lr: 0.000300, time: 21.641519\n","iter: 13580, loss: 0.110168, lr: 0.000300, time: 22.365496\n","iter: 13600, loss: 0.102952, lr: 0.000300, time: 21.655101\n","iter: 13620, loss: 0.124256, lr: 0.000300, time: 22.304117\n","iter: 13640, loss: 0.084808, lr: 0.000300, time: 21.663505\n","iter: 13660, loss: 0.084439, lr: 0.000300, time: 22.243592\n","iter: 13680, loss: 0.098898, lr: 0.000300, time: 21.628608\n","iter: 13700, loss: 0.091551, lr: 0.000300, time: 22.386794\n","iter: 13720, loss: 0.105509, lr: 0.000300, time: 21.697968\n","iter: 13740, loss: 0.083136, lr: 0.000300, time: 22.388121\n","iter: 13760, loss: 0.101868, lr: 0.000300, time: 21.663585\n","iter: 13780, loss: 0.066357, lr: 0.000300, time: 22.325981\n","iter: 13800, loss: 0.079870, lr: 0.000300, time: 21.641476\n","iter: 13820, loss: 0.143474, lr: 0.000300, time: 22.338501\n","iter: 13840, loss: 0.076263, lr: 0.000300, time: 21.679326\n","iter: 13860, loss: 0.110509, lr: 0.000300, time: 22.344532\n","iter: 13880, loss: 0.107916, lr: 0.000300, time: 21.664043\n","iter: 13900, loss: 0.089185, lr: 0.000300, time: 22.333468\n","iter: 13920, loss: 0.099636, lr: 0.000300, time: 21.684637\n","iter: 13940, loss: 0.121857, lr: 0.000300, time: 22.308587\n","iter: 13960, loss: 0.109213, lr: 0.000300, time: 21.657032\n","iter: 13980, loss: 0.128873, lr: 0.000300, time: 21.612226\n","iter: 14000, loss: 0.088337, lr: 0.000300, time: 22.438089\n","iter: 14020, loss: 0.148459, lr: 0.000300, time: 21.608561\n","iter: 14040, loss: 0.101239, lr: 0.000300, time: 22.380971\n","iter: 14060, loss: 0.108322, lr: 0.000300, time: 21.611155\n","iter: 14080, loss: 0.054204, lr: 0.000300, time: 22.402760\n","iter: 14100, loss: 0.049655, lr: 0.000300, time: 21.622537\n","iter: 14120, loss: 0.077361, lr: 0.000300, time: 22.355691\n","iter: 14140, loss: 0.125941, lr: 0.000300, time: 21.642313\n","iter: 14160, loss: 0.153625, lr: 0.000300, time: 22.387712\n","iter: 14180, loss: 0.128991, lr: 0.000300, time: 21.635584\n","iter: 14200, loss: 0.070528, lr: 0.000300, time: 22.392443\n","iter: 14220, loss: 0.121139, lr: 0.000300, time: 21.655955\n","iter: 14240, loss: 0.086705, lr: 0.000300, time: 22.384045\n","iter: 14260, loss: 0.123659, lr: 0.000300, time: 21.650901\n","iter: 14280, loss: 0.128190, lr: 0.000300, time: 22.372184\n","iter: 14300, loss: 0.093565, lr: 0.000300, time: 21.647265\n","iter: 14320, loss: 0.093960, lr: 0.000300, time: 22.365775\n","iter: 14340, loss: 0.095380, lr: 0.000300, time: 21.664273\n","iter: 14360, loss: 0.092954, lr: 0.000300, time: 22.364919\n","iter: 14380, loss: 0.068156, lr: 0.000300, time: 21.690034\n","iter: 14400, loss: 0.082080, lr: 0.000300, time: 22.291292\n","iter: 14420, loss: 0.090865, lr: 0.000300, time: 21.702627\n","iter: 14440, loss: 0.077122, lr: 0.000300, time: 22.282254\n","iter: 14460, loss: 0.088467, lr: 0.000300, time: 21.698750\n","iter: 14480, loss: 0.079108, lr: 0.000300, time: 22.276451\n","iter: 14500, loss: 0.094819, lr: 0.000300, time: 21.675098\n","iter: 14520, loss: 0.107621, lr: 0.000300, time: 22.347773\n","iter: 14540, loss: 0.080319, lr: 0.000300, time: 21.660577\n","iter: 14560, loss: 0.058623, lr: 0.000300, time: 22.322616\n","iter: 14580, loss: 0.064687, lr: 0.000300, time: 21.686658\n","iter: 14600, loss: 0.096426, lr: 0.000300, time: 22.387701\n","iter: 14620, loss: 0.064242, lr: 0.000300, time: 21.699773\n","iter: 14640, loss: 0.069957, lr: 0.000300, time: 22.278261\n","iter: 14660, loss: 0.049323, lr: 0.000300, time: 21.708810\n","iter: 14680, loss: 0.083971, lr: 0.000300, time: 22.309979\n","iter: 14700, loss: 0.077641, lr: 0.000300, time: 21.686371\n","iter: 14720, loss: 0.064331, lr: 0.000300, time: 22.330615\n","iter: 14740, loss: 0.065402, lr: 0.000300, time: 21.681641\n","iter: 14760, loss: 0.082166, lr: 0.000300, time: 22.380343\n","iter: 14780, loss: 0.121810, lr: 0.000300, time: 21.670819\n","iter: 14800, loss: 0.096044, lr: 0.000300, time: 21.631688\n","iter: 14820, loss: 0.092176, lr: 0.000300, time: 22.356535\n","iter: 14840, loss: 0.058626, lr: 0.000300, time: 21.636323\n","iter: 14860, loss: 0.082503, lr: 0.000300, time: 22.319543\n","iter: 14880, loss: 0.108346, lr: 0.000300, time: 21.658048\n","iter: 14900, loss: 0.094152, lr: 0.000300, time: 22.375002\n","iter: 14920, loss: 0.064258, lr: 0.000300, time: 21.664924\n","iter: 14940, loss: 0.107790, lr: 0.000300, time: 22.427187\n","iter: 14960, loss: 0.098596, lr: 0.000300, time: 21.629151\n","iter: 14980, loss: 0.111049, lr: 0.000300, time: 22.418191\n","==> changing adam betas from (0.9, 0.999) to (0.5, 0.999)\n","==> start droping lr exponentially\n","iter: 15000, loss: 0.068277, lr: 0.000300, time: 21.643647\n","iter: 15020, loss: 0.151903, lr: 0.000295, time: 22.394538\n","iter: 15040, loss: 0.066762, lr: 0.000291, time: 21.622333\n","iter: 15060, loss: 0.084023, lr: 0.000287, time: 22.250960\n","iter: 15080, loss: 0.120362, lr: 0.000283, time: 21.602282\n","iter: 15100, loss: 0.077353, lr: 0.000280, time: 22.320581\n","iter: 15120, loss: 0.119420, lr: 0.000276, time: 21.615356\n","iter: 15140, loss: 0.100239, lr: 0.000272, time: 22.326159\n","iter: 15160, loss: 0.057862, lr: 0.000268, time: 21.647788\n","iter: 15180, loss: 0.084133, lr: 0.000265, time: 22.277011\n","iter: 15200, loss: 0.068646, lr: 0.000261, time: 21.661613\n","iter: 15220, loss: 0.110127, lr: 0.000257, time: 22.346887\n","iter: 15240, loss: 0.077775, lr: 0.000254, time: 21.625438\n","iter: 15260, loss: 0.072370, lr: 0.000250, time: 22.349728\n","iter: 15280, loss: 0.084910, lr: 0.000247, time: 21.661308\n","iter: 15300, loss: 0.064032, lr: 0.000244, time: 22.349933\n","iter: 15320, loss: 0.066011, lr: 0.000240, time: 21.650291\n","iter: 15340, loss: 0.061042, lr: 0.000237, time: 22.356730\n","iter: 15360, loss: 0.072459, lr: 0.000234, time: 21.631778\n","iter: 15380, loss: 0.067582, lr: 0.000230, time: 22.365802\n","iter: 15400, loss: 0.064280, lr: 0.000227, time: 21.625001\n","iter: 15420, loss: 0.053981, lr: 0.000224, time: 22.151486\n","iter: 15440, loss: 0.063676, lr: 0.000221, time: 21.630985\n","iter: 15460, loss: 0.101009, lr: 0.000218, time: 22.257951\n","iter: 15480, loss: 0.050212, lr: 0.000215, time: 21.641220\n","iter: 15500, loss: 0.062350, lr: 0.000212, time: 22.280694\n","iter: 15520, loss: 0.040291, lr: 0.000209, time: 21.669865\n","iter: 15540, loss: 0.058151, lr: 0.000206, time: 22.360573\n","iter: 15560, loss: 0.034485, lr: 0.000203, time: 21.626970\n","iter: 15580, loss: 0.060382, lr: 0.000201, time: 22.359791\n","iter: 15600, loss: 0.097010, lr: 0.000198, time: 21.637515\n","iter: 15620, loss: 0.045086, lr: 0.000195, time: 21.595414\n","iter: 15640, loss: 0.056413, lr: 0.000193, time: 22.290125\n","iter: 15660, loss: 0.058064, lr: 0.000190, time: 21.608749\n","iter: 15680, loss: 0.055663, lr: 0.000187, time: 22.423188\n","iter: 15700, loss: 0.054646, lr: 0.000185, time: 21.629706\n","iter: 15720, loss: 0.054577, lr: 0.000182, time: 22.322261\n","iter: 15740, loss: 0.071914, lr: 0.000180, time: 21.622439\n","iter: 15760, loss: 0.075955, lr: 0.000177, time: 22.207637\n","iter: 15780, loss: 0.053203, lr: 0.000175, time: 21.617864\n","iter: 15800, loss: 0.041977, lr: 0.000172, time: 22.329104\n","iter: 15820, loss: 0.077750, lr: 0.000170, time: 21.641846\n","iter: 15840, loss: 0.049812, lr: 0.000168, time: 22.379671\n","iter: 15860, loss: 0.058170, lr: 0.000165, time: 21.660399\n","iter: 15880, loss: 0.040895, lr: 0.000163, time: 22.383219\n","iter: 15900, loss: 0.058514, lr: 0.000161, time: 21.642267\n","iter: 15920, loss: 0.060964, lr: 0.000159, time: 22.358446\n","iter: 15940, loss: 0.060351, lr: 0.000157, time: 21.641959\n","iter: 15960, loss: 0.048979, lr: 0.000154, time: 22.329879\n","iter: 15980, loss: 0.055893, lr: 0.000152, time: 21.657730\n","iter: 16000, loss: 0.049454, lr: 0.000150, time: 22.367475\n","iter: 16020, loss: 0.048209, lr: 0.000148, time: 21.676242\n","iter: 16040, loss: 0.033592, lr: 0.000146, time: 22.202392\n","iter: 16060, loss: 0.033715, lr: 0.000144, time: 21.666940\n","iter: 16080, loss: 0.022110, lr: 0.000142, time: 22.174928\n","iter: 16100, loss: 0.031299, lr: 0.000140, time: 21.683531\n","iter: 16120, loss: 0.019729, lr: 0.000138, time: 22.398518\n","iter: 16140, loss: 0.031905, lr: 0.000136, time: 21.654198\n","iter: 16160, loss: 0.039765, lr: 0.000134, time: 22.331748\n","iter: 16180, loss: 0.020385, lr: 0.000133, time: 21.653336\n","iter: 16200, loss: 0.068020, lr: 0.000131, time: 22.405960\n","iter: 16220, loss: 0.033944, lr: 0.000129, time: 21.662107\n","iter: 16240, loss: 0.045935, lr: 0.000127, time: 22.316267\n","iter: 16260, loss: 0.037736, lr: 0.000125, time: 21.677026\n","iter: 16280, loss: 0.028599, lr: 0.000124, time: 22.368844\n","iter: 16300, loss: 0.032560, lr: 0.000122, time: 21.663729\n","iter: 16320, loss: 0.021572, lr: 0.000120, time: 22.379949\n","iter: 16340, loss: 0.026990, lr: 0.000119, time: 21.660412\n","iter: 16360, loss: 0.031148, lr: 0.000117, time: 22.219317\n","iter: 16380, loss: 0.038435, lr: 0.000115, time: 21.684645\n","iter: 16400, loss: 0.023580, lr: 0.000114, time: 22.439745\n","iter: 16420, loss: 0.041128, lr: 0.000112, time: 21.644323\n","iter: 16440, loss: 0.031162, lr: 0.000111, time: 21.602770\n","iter: 16460, loss: 0.027338, lr: 0.000109, time: 22.236749\n","iter: 16480, loss: 0.022073, lr: 0.000108, time: 21.635033\n","iter: 16500, loss: 0.031724, lr: 0.000106, time: 22.415875\n","iter: 16520, loss: 0.032398, lr: 0.000105, time: 21.640994\n","iter: 16540, loss: 0.041871, lr: 0.000103, time: 22.424556\n","iter: 16560, loss: 0.018842, lr: 0.000102, time: 21.621773\n","iter: 16580, loss: 0.055418, lr: 0.000101, time: 22.384640\n","iter: 16600, loss: 0.037627, lr: 0.000099, time: 21.644269\n","iter: 16620, loss: 0.042174, lr: 0.000098, time: 22.231580\n","iter: 16640, loss: 0.025474, lr: 0.000097, time: 21.618791\n","iter: 16660, loss: 0.077096, lr: 0.000095, time: 22.376699\n","iter: 16680, loss: 0.044034, lr: 0.000094, time: 21.626199\n","iter: 16700, loss: 0.028048, lr: 0.000093, time: 22.138206\n","iter: 16720, loss: 0.044282, lr: 0.000091, time: 21.667198\n","iter: 16740, loss: 0.024309, lr: 0.000090, time: 22.330254\n","iter: 16760, loss: 0.027415, lr: 0.000089, time: 21.646884\n","iter: 16780, loss: 0.038762, lr: 0.000088, time: 22.169163\n","iter: 16800, loss: 0.039931, lr: 0.000086, time: 21.630816\n","iter: 16820, loss: 0.028868, lr: 0.000085, time: 22.322502\n","iter: 16840, loss: 0.040619, lr: 0.000084, time: 21.653078\n","iter: 16860, loss: 0.035811, lr: 0.000083, time: 22.349414\n","iter: 16880, loss: 0.023232, lr: 0.000082, time: 21.655356\n","iter: 16900, loss: 0.024850, lr: 0.000081, time: 22.380743\n","iter: 16920, loss: 0.041536, lr: 0.000080, time: 21.634726\n","iter: 16940, loss: 0.042570, lr: 0.000078, time: 22.328108\n","iter: 16960, loss: 0.019698, lr: 0.000077, time: 21.649652\n","iter: 16980, loss: 0.046440, lr: 0.000076, time: 22.313355\n","iter: 17000, loss: 0.030570, lr: 0.000075, time: 21.635489\n","iter: 17020, loss: 0.027047, lr: 0.000074, time: 22.351978\n","iter: 17040, loss: 0.039843, lr: 0.000073, time: 21.604699\n","iter: 17060, loss: 0.014977, lr: 0.000072, time: 22.248414\n","iter: 17080, loss: 0.028151, lr: 0.000071, time: 21.512613\n","iter: 17100, loss: 0.046209, lr: 0.000070, time: 22.200057\n","iter: 17120, loss: 0.018840, lr: 0.000069, time: 21.512271\n","iter: 17140, loss: 0.027517, lr: 0.000068, time: 22.247745\n","iter: 17160, loss: 0.041919, lr: 0.000067, time: 21.530242\n","iter: 17180, loss: 0.031922, lr: 0.000066, time: 22.255008\n","iter: 17200, loss: 0.016463, lr: 0.000066, time: 21.541964\n","iter: 17220, loss: 0.025378, lr: 0.000065, time: 22.341332\n","iter: 17240, loss: 0.049646, lr: 0.000064, time: 21.671296\n","iter: 17260, loss: 0.029735, lr: 0.000063, time: 21.609812\n","iter: 17280, loss: 0.017071, lr: 0.000062, time: 22.419337\n","iter: 17300, loss: 0.018426, lr: 0.000061, time: 21.628132\n","iter: 17320, loss: 0.012454, lr: 0.000060, time: 22.423915\n","iter: 17340, loss: 0.025959, lr: 0.000060, time: 21.641500\n","iter: 17360, loss: 0.046130, lr: 0.000059, time: 22.340806\n","iter: 17380, loss: 0.039047, lr: 0.000058, time: 21.611906\n","iter: 17400, loss: 0.030588, lr: 0.000057, time: 22.411067\n","iter: 17420, loss: 0.018887, lr: 0.000056, time: 21.642114\n","iter: 17440, loss: 0.027625, lr: 0.000056, time: 22.309610\n","iter: 17460, loss: 0.028331, lr: 0.000055, time: 21.629462\n","iter: 17480, loss: 0.027887, lr: 0.000054, time: 22.392983\n","iter: 17500, loss: 0.025007, lr: 0.000053, time: 21.637790\n","iter: 17520, loss: 0.023929, lr: 0.000053, time: 22.366160\n","iter: 17540, loss: 0.033889, lr: 0.000052, time: 21.667961\n","iter: 17560, loss: 0.038912, lr: 0.000051, time: 22.376049\n","iter: 17580, loss: 0.022964, lr: 0.000050, time: 21.653588\n","iter: 17600, loss: 0.019969, lr: 0.000050, time: 22.372921\n","iter: 17620, loss: 0.059403, lr: 0.000049, time: 21.632728\n","iter: 17640, loss: 0.021651, lr: 0.000048, time: 22.353407\n","iter: 17660, loss: 0.025002, lr: 0.000048, time: 21.658324\n","iter: 17680, loss: 0.025262, lr: 0.000047, time: 22.190473\n","iter: 17700, loss: 0.024953, lr: 0.000046, time: 21.668876\n","iter: 17720, loss: 0.022442, lr: 0.000046, time: 22.180706\n","iter: 17740, loss: 0.026564, lr: 0.000045, time: 21.666097\n","iter: 17760, loss: 0.037718, lr: 0.000045, time: 22.263258\n","iter: 17780, loss: 0.016055, lr: 0.000044, time: 21.671190\n","iter: 17800, loss: 0.014021, lr: 0.000043, time: 22.274045\n","iter: 17820, loss: 0.024953, lr: 0.000043, time: 21.632731\n","iter: 17840, loss: 0.029032, lr: 0.000042, time: 22.384791\n","iter: 17860, loss: 0.028528, lr: 0.000042, time: 21.676785\n","iter: 17880, loss: 0.013682, lr: 0.000041, time: 22.400506\n","iter: 17900, loss: 0.021016, lr: 0.000040, time: 21.672290\n","iter: 17920, loss: 0.042279, lr: 0.000040, time: 22.263196\n","iter: 17940, loss: 0.055479, lr: 0.000039, time: 21.639347\n","iter: 17960, loss: 0.026916, lr: 0.000039, time: 22.252009\n","iter: 17980, loss: 0.014834, lr: 0.000038, time: 21.655473\n","iter: 18000, loss: 0.006548, lr: 0.000038, time: 22.324827\n","iter: 18020, loss: 0.023581, lr: 0.000037, time: 21.660766\n","iter: 18040, loss: 0.016069, lr: 0.000037, time: 22.362920\n","iter: 18060, loss: 0.022796, lr: 0.000036, time: 21.675135\n","iter: 18080, loss: 0.040047, lr: 0.000036, time: 21.595870\n","iter: 18100, loss: 0.016957, lr: 0.000035, time: 22.368388\n","iter: 18120, loss: 0.040210, lr: 0.000035, time: 21.584863\n","iter: 18140, loss: 0.017934, lr: 0.000034, time: 22.255060\n","iter: 18160, loss: 0.012412, lr: 0.000034, time: 21.620153\n","iter: 18180, loss: 0.017239, lr: 0.000033, time: 22.416088\n","iter: 18200, loss: 0.023447, lr: 0.000033, time: 21.633685\n","iter: 18220, loss: 0.034270, lr: 0.000032, time: 22.402681\n","iter: 18240, loss: 0.020725, lr: 0.000032, time: 21.624040\n","iter: 18260, loss: 0.038819, lr: 0.000032, time: 22.291720\n","iter: 18280, loss: 0.015369, lr: 0.000031, time: 21.638692\n","iter: 18300, loss: 0.019365, lr: 0.000031, time: 22.415337\n","iter: 18320, loss: 0.024127, lr: 0.000030, time: 21.665027\n","iter: 18340, loss: 0.014776, lr: 0.000030, time: 22.425744\n","iter: 18360, loss: 0.015846, lr: 0.000029, time: 21.652157\n","iter: 18380, loss: 0.023261, lr: 0.000029, time: 22.395525\n","iter: 18400, loss: 0.026378, lr: 0.000029, time: 21.652620\n","iter: 18420, loss: 0.033007, lr: 0.000028, time: 22.452860\n","iter: 18440, loss: 0.011616, lr: 0.000028, time: 21.673573\n","iter: 18460, loss: 0.010819, lr: 0.000027, time: 22.375725\n","iter: 18480, loss: 0.021047, lr: 0.000027, time: 21.697040\n","iter: 18500, loss: 0.012871, lr: 0.000027, time: 22.329178\n","iter: 18520, loss: 0.038698, lr: 0.000026, time: 21.648576\n","iter: 18540, loss: 0.014905, lr: 0.000026, time: 22.394937\n","iter: 18560, loss: 0.043201, lr: 0.000026, time: 21.666938\n","iter: 18580, loss: 0.053637, lr: 0.000025, time: 22.294351\n","iter: 18600, loss: 0.024192, lr: 0.000025, time: 21.671140\n","iter: 18620, loss: 0.035396, lr: 0.000025, time: 22.345789\n","iter: 18640, loss: 0.013987, lr: 0.000024, time: 21.668483\n","iter: 18660, loss: 0.021307, lr: 0.000024, time: 22.373597\n","iter: 18680, loss: 0.019665, lr: 0.000024, time: 21.659960\n","iter: 18700, loss: 0.036357, lr: 0.000023, time: 22.302841\n","iter: 18720, loss: 0.014274, lr: 0.000023, time: 21.676344\n","iter: 18740, loss: 0.020831, lr: 0.000023, time: 22.414459\n","iter: 18760, loss: 0.041888, lr: 0.000022, time: 21.659518\n","iter: 18780, loss: 0.014108, lr: 0.000022, time: 22.382415\n","iter: 18800, loss: 0.026828, lr: 0.000022, time: 21.674265\n","iter: 18820, loss: 0.023713, lr: 0.000021, time: 22.207369\n","iter: 18840, loss: 0.018370, lr: 0.000021, time: 21.650174\n","iter: 18860, loss: 0.025234, lr: 0.000021, time: 22.374640\n","iter: 18880, loss: 0.029548, lr: 0.000021, time: 21.656602\n","iter: 18900, loss: 0.013279, lr: 0.000020, time: 21.610841\n","iter: 18920, loss: 0.029361, lr: 0.000020, time: 22.276164\n","iter: 18940, loss: 0.016966, lr: 0.000020, time: 21.607502\n","iter: 18960, loss: 0.019142, lr: 0.000019, time: 22.326024\n","iter: 18980, loss: 0.039195, lr: 0.000019, time: 21.621646\n","iter: 19000, loss: 0.015555, lr: 0.000019, time: 22.368044\n","iter: 19020, loss: 0.021764, lr: 0.000019, time: 21.636739\n","iter: 19040, loss: 0.025620, lr: 0.000018, time: 22.416077\n","iter: 19060, loss: 0.021275, lr: 0.000018, time: 21.640821\n","iter: 19080, loss: 0.034181, lr: 0.000018, time: 22.360209\n","iter: 19100, loss: 0.017515, lr: 0.000018, time: 21.628096\n","iter: 19120, loss: 0.011483, lr: 0.000017, time: 22.396868\n","iter: 19140, loss: 0.031476, lr: 0.000017, time: 21.627116\n","iter: 19160, loss: 0.017804, lr: 0.000017, time: 22.343461\n","iter: 19180, loss: 0.023523, lr: 0.000017, time: 21.605964\n","iter: 19200, loss: 0.033722, lr: 0.000016, time: 22.217551\n","iter: 19220, loss: 0.030145, lr: 0.000016, time: 21.653328\n","iter: 19240, loss: 0.021535, lr: 0.000016, time: 22.296010\n","iter: 19260, loss: 0.012746, lr: 0.000016, time: 21.666242\n","iter: 19280, loss: 0.028334, lr: 0.000016, time: 22.363013\n","iter: 19300, loss: 0.012154, lr: 0.000015, time: 21.671375\n","iter: 19320, loss: 0.023294, lr: 0.000015, time: 22.248044\n","iter: 19340, loss: 0.036194, lr: 0.000015, time: 21.645478\n","iter: 19360, loss: 0.023463, lr: 0.000015, time: 22.358953\n","iter: 19380, loss: 0.026178, lr: 0.000015, time: 21.648004\n","iter: 19400, loss: 0.016843, lr: 0.000014, time: 22.244045\n","iter: 19420, loss: 0.023261, lr: 0.000014, time: 21.661202\n","iter: 19440, loss: 0.030727, lr: 0.000014, time: 22.416220\n","iter: 19460, loss: 0.022516, lr: 0.000014, time: 21.663041\n","iter: 19480, loss: 0.010164, lr: 0.000014, time: 22.373975\n","iter: 19500, loss: 0.018652, lr: 0.000013, time: 21.668414\n","iter: 19520, loss: 0.019355, lr: 0.000013, time: 22.330369\n","iter: 19540, loss: 0.012396, lr: 0.000013, time: 21.654186\n","iter: 19560, loss: 0.027987, lr: 0.000013, time: 22.369660\n","iter: 19580, loss: 0.017993, lr: 0.000013, time: 21.621171\n","iter: 19600, loss: 0.033212, lr: 0.000012, time: 22.381343\n","iter: 19620, loss: 0.032841, lr: 0.000012, time: 21.653041\n","iter: 19640, loss: 0.011462, lr: 0.000012, time: 22.271177\n","iter: 19660, loss: 0.014701, lr: 0.000012, time: 21.656894\n","iter: 19680, loss: 0.033276, lr: 0.000012, time: 22.267538\n","iter: 19700, loss: 0.030375, lr: 0.000012, time: 21.646704\n","iter: 19720, loss: 0.018237, lr: 0.000011, time: 21.641810\n","iter: 19740, loss: 0.023978, lr: 0.000011, time: 22.223066\n","iter: 19760, loss: 0.020855, lr: 0.000011, time: 21.629607\n","iter: 19780, loss: 0.016220, lr: 0.000011, time: 22.241187\n","iter: 19800, loss: 0.011721, lr: 0.000011, time: 21.640307\n","iter: 19820, loss: 0.013129, lr: 0.000011, time: 22.377709\n","iter: 19840, loss: 0.020006, lr: 0.000011, time: 21.660743\n","iter: 19860, loss: 0.022302, lr: 0.000010, time: 22.401620\n","iter: 19880, loss: 0.016582, lr: 0.000010, time: 21.634670\n","iter: 19900, loss: 0.014339, lr: 0.000010, time: 22.285733\n","iter: 19920, loss: 0.018034, lr: 0.000010, time: 21.683766\n","iter: 19940, loss: 0.007346, lr: 0.000010, time: 22.415068\n","iter: 19960, loss: 0.019002, lr: 0.000010, time: 21.591461\n","iter: 19980, loss: 0.012280, lr: 0.000010, time: 22.339654\n","iter: 20000, loss: 0.012368, lr: 0.000009, time: 21.551814\n","iter: 20020, loss: 0.021468, lr: 0.000009, time: 22.268736\n","iter: 20040, loss: 0.025272, lr: 0.000009, time: 21.601250\n","iter: 20060, loss: 0.019696, lr: 0.000009, time: 22.337098\n","iter: 20080, loss: 0.016500, lr: 0.000009, time: 21.601589\n","iter: 20100, loss: 0.026069, lr: 0.000009, time: 22.353470\n","iter: 20120, loss: 0.017105, lr: 0.000009, time: 21.651444\n","iter: 20140, loss: 0.016988, lr: 0.000009, time: 22.352022\n","iter: 20160, loss: 0.022703, lr: 0.000008, time: 21.605729\n","iter: 20180, loss: 0.016914, lr: 0.000008, time: 22.255741\n","iter: 20200, loss: 0.029627, lr: 0.000008, time: 21.598767\n","iter: 20220, loss: 0.024991, lr: 0.000008, time: 22.145457\n","iter: 20240, loss: 0.027557, lr: 0.000008, time: 21.648361\n","iter: 20260, loss: 0.021816, lr: 0.000008, time: 22.246322\n","iter: 20280, loss: 0.016789, lr: 0.000008, time: 21.610411\n","iter: 20300, loss: 0.019756, lr: 0.000008, time: 22.284658\n","iter: 20320, loss: 0.018699, lr: 0.000008, time: 21.570905\n","iter: 20340, loss: 0.011540, lr: 0.000007, time: 22.334972\n","iter: 20360, loss: 0.025984, lr: 0.000007, time: 21.612536\n","iter: 20380, loss: 0.018247, lr: 0.000007, time: 22.249193\n","iter: 20400, loss: 0.018673, lr: 0.000007, time: 21.681632\n","iter: 20420, loss: 0.018493, lr: 0.000007, time: 22.396914\n","iter: 20440, loss: 0.020444, lr: 0.000007, time: 21.684107\n","iter: 20460, loss: 0.027499, lr: 0.000007, time: 22.343210\n","iter: 20480, loss: 0.016875, lr: 0.000007, time: 21.680464\n","iter: 20500, loss: 0.016300, lr: 0.000007, time: 22.364514\n","iter: 20520, loss: 0.025339, lr: 0.000007, time: 21.680413\n","iter: 20540, loss: 0.046395, lr: 0.000007, time: 21.626735\n","iter: 20560, loss: 0.011640, lr: 0.000006, time: 22.464501\n","iter: 20580, loss: 0.026243, lr: 0.000006, time: 21.639624\n","iter: 20600, loss: 0.011150, lr: 0.000006, time: 22.377793\n","iter: 20620, loss: 0.013129, lr: 0.000006, time: 21.665939\n","iter: 20640, loss: 0.012056, lr: 0.000006, time: 22.425671\n","iter: 20660, loss: 0.023930, lr: 0.000006, time: 21.658975\n","iter: 20680, loss: 0.018312, lr: 0.000006, time: 22.418139\n","iter: 20700, loss: 0.022610, lr: 0.000006, time: 21.661186\n","iter: 20720, loss: 0.027936, lr: 0.000006, time: 22.287190\n","iter: 20740, loss: 0.009015, lr: 0.000006, time: 21.667381\n","iter: 20760, loss: 0.013002, lr: 0.000006, time: 22.217207\n","iter: 20780, loss: 0.010362, lr: 0.000006, time: 21.665868\n","iter: 20800, loss: 0.023880, lr: 0.000005, time: 22.362346\n","iter: 20820, loss: 0.026246, lr: 0.000005, time: 21.704808\n","iter: 20840, loss: 0.006478, lr: 0.000005, time: 22.428893\n","iter: 20860, loss: 0.014079, lr: 0.000005, time: 21.654177\n","iter: 20880, loss: 0.016363, lr: 0.000005, time: 22.283879\n","iter: 20900, loss: 0.011428, lr: 0.000005, time: 21.708315\n","iter: 20920, loss: 0.020041, lr: 0.000005, time: 22.375877\n","iter: 20940, loss: 0.019245, lr: 0.000005, time: 21.673767\n","iter: 20960, loss: 0.018293, lr: 0.000005, time: 22.400102\n","iter: 20980, loss: 0.026371, lr: 0.000005, time: 21.691199\n","iter: 21000, loss: 0.020924, lr: 0.000005, time: 22.391830\n","iter: 21020, loss: 0.022983, lr: 0.000005, time: 21.692315\n","iter: 21040, loss: 0.019739, lr: 0.000005, time: 22.401782\n","iter: 21060, loss: 0.015980, lr: 0.000005, time: 21.697360\n","iter: 21080, loss: 0.009211, lr: 0.000004, time: 22.285525\n","iter: 21100, loss: 0.005512, lr: 0.000004, time: 21.706960\n","iter: 21120, loss: 0.027220, lr: 0.000004, time: 22.438318\n","iter: 21140, loss: 0.017758, lr: 0.000004, time: 21.694520\n","iter: 21160, loss: 0.006742, lr: 0.000004, time: 22.446388\n","iter: 21180, loss: 0.016755, lr: 0.000004, time: 21.701201\n","iter: 21200, loss: 0.012428, lr: 0.000004, time: 22.295076\n","iter: 21220, loss: 0.018892, lr: 0.000004, time: 21.702393\n","iter: 21240, loss: 0.006496, lr: 0.000004, time: 22.421158\n","iter: 21260, loss: 0.048951, lr: 0.000004, time: 21.694814\n","iter: 21280, loss: 0.020997, lr: 0.000004, time: 22.470452\n","iter: 21300, loss: 0.024779, lr: 0.000004, time: 21.708975\n","iter: 21320, loss: 0.022795, lr: 0.000004, time: 22.340783\n","iter: 21340, loss: 0.022268, lr: 0.000004, time: 21.734930\n","iter: 21360, loss: 0.052829, lr: 0.000004, time: 21.644073\n","iter: 21380, loss: 0.028906, lr: 0.000004, time: 22.464974\n","iter: 21400, loss: 0.010668, lr: 0.000004, time: 21.679105\n","iter: 21420, loss: 0.015472, lr: 0.000004, time: 22.487856\n","iter: 21440, loss: 0.012051, lr: 0.000004, time: 21.648986\n","iter: 21460, loss: 0.018001, lr: 0.000003, time: 22.447531\n","iter: 21480, loss: 0.028134, lr: 0.000003, time: 21.666755\n","iter: 21500, loss: 0.008432, lr: 0.000003, time: 22.431523\n","iter: 21520, loss: 0.033612, lr: 0.000003, time: 21.643156\n","iter: 21540, loss: 0.018720, lr: 0.000003, time: 22.454760\n","iter: 21560, loss: 0.009902, lr: 0.000003, time: 21.668465\n","iter: 21580, loss: 0.008409, lr: 0.000003, time: 22.433547\n","iter: 21600, loss: 0.010716, lr: 0.000003, time: 21.656873\n","iter: 21620, loss: 0.030695, lr: 0.000003, time: 22.403980\n","iter: 21640, loss: 0.013365, lr: 0.000003, time: 21.677647\n","iter: 21660, loss: 0.008781, lr: 0.000003, time: 22.412460\n","iter: 21680, loss: 0.006117, lr: 0.000003, time: 21.675651\n","iter: 21700, loss: 0.011125, lr: 0.000003, time: 22.375595\n","iter: 21720, loss: 0.014766, lr: 0.000003, time: 21.673366\n","iter: 21740, loss: 0.015949, lr: 0.000003, time: 22.223746\n","iter: 21760, loss: 0.023926, lr: 0.000003, time: 21.663643\n","iter: 21780, loss: 0.019120, lr: 0.000003, time: 22.429696\n","iter: 21800, loss: 0.027076, lr: 0.000003, time: 21.880768\n","iter: 21820, loss: 0.019262, lr: 0.000003, time: 22.548407\n","iter: 21840, loss: 0.017818, lr: 0.000003, time: 21.867858\n","iter: 21860, loss: 0.013422, lr: 0.000003, time: 22.540576\n","iter: 21880, loss: 0.018550, lr: 0.000003, time: 21.890159\n","iter: 21900, loss: 0.008763, lr: 0.000003, time: 22.481351\n","iter: 21920, loss: 0.014570, lr: 0.000003, time: 21.843932\n","iter: 21940, loss: 0.016135, lr: 0.000002, time: 22.442564\n","iter: 21960, loss: 0.016261, lr: 0.000002, time: 21.796987\n","iter: 21980, loss: 0.012574, lr: 0.000002, time: 22.531765\n","iter: 22000, loss: 0.014628, lr: 0.000002, time: 21.834743\n","iter: 22020, loss: 0.021614, lr: 0.000002, time: 22.553453\n","iter: 22040, loss: 0.029435, lr: 0.000002, time: 21.824824\n","iter: 22060, loss: 0.035835, lr: 0.000002, time: 22.544524\n","iter: 22080, loss: 0.027013, lr: 0.000002, time: 21.811732\n","iter: 22100, loss: 0.034158, lr: 0.000002, time: 22.536848\n","iter: 22120, loss: 0.012499, lr: 0.000002, time: 21.842462\n","iter: 22140, loss: 0.033594, lr: 0.000002, time: 22.519708\n","iter: 22160, loss: 0.012466, lr: 0.000002, time: 21.827981\n","iter: 22180, loss: 0.030776, lr: 0.000002, time: 21.753627\n","iter: 22200, loss: 0.029249, lr: 0.000002, time: 22.559242\n","iter: 22220, loss: 0.027823, lr: 0.000002, time: 21.780790\n","iter: 22240, loss: 0.007346, lr: 0.000002, time: 22.500828\n","iter: 22260, loss: 0.029975, lr: 0.000002, time: 21.797801\n","iter: 22280, loss: 0.015881, lr: 0.000002, time: 22.611394\n","iter: 22300, loss: 0.018904, lr: 0.000002, time: 21.786340\n","iter: 22320, loss: 0.024201, lr: 0.000002, time: 22.591475\n","iter: 22340, loss: 0.020653, lr: 0.000002, time: 21.806339\n","iter: 22360, loss: 0.022003, lr: 0.000002, time: 22.541276\n","iter: 22380, loss: 0.013425, lr: 0.000002, time: 21.791965\n","iter: 22400, loss: 0.007706, lr: 0.000002, time: 22.432378\n","iter: 22420, loss: 0.014788, lr: 0.000002, time: 21.818071\n","iter: 22440, loss: 0.020922, lr: 0.000002, time: 22.581043\n","iter: 22460, loss: 0.006433, lr: 0.000002, time: 21.822053\n","iter: 22480, loss: 0.012306, lr: 0.000002, time: 22.395474\n","iter: 22500, loss: 0.027313, lr: 0.000002, time: 21.883158\n","iter: 22520, loss: 0.015172, lr: 0.000002, time: 22.474020\n","iter: 22540, loss: 0.009083, lr: 0.000002, time: 21.828023\n","iter: 22560, loss: 0.022427, lr: 0.000002, time: 22.457664\n","iter: 22580, loss: 0.014044, lr: 0.000002, time: 21.818721\n","iter: 22600, loss: 0.013791, lr: 0.000002, time: 22.428598\n","iter: 22620, loss: 0.008569, lr: 0.000002, time: 21.826547\n","iter: 22640, loss: 0.015794, lr: 0.000002, time: 22.549309\n","iter: 22660, loss: 0.015420, lr: 0.000002, time: 21.804746\n","iter: 22680, loss: 0.020834, lr: 0.000001, time: 22.538271\n","iter: 22700, loss: 0.028147, lr: 0.000001, time: 21.847766\n","iter: 22720, loss: 0.017836, lr: 0.000001, time: 22.483485\n","iter: 22740, loss: 0.025554, lr: 0.000001, time: 21.861967\n","iter: 22760, loss: 0.008193, lr: 0.000001, time: 22.407150\n","iter: 22780, loss: 0.009668, lr: 0.000001, time: 21.859976\n","iter: 22800, loss: 0.017743, lr: 0.000001, time: 22.452280\n","iter: 22820, loss: 0.013536, lr: 0.000001, time: 21.828448\n","iter: 22840, loss: 0.018802, lr: 0.000001, time: 22.393727\n","iter: 22860, loss: 0.006813, lr: 0.000001, time: 21.820962\n","iter: 22880, loss: 0.018110, lr: 0.000001, time: 22.429570\n","iter: 22900, loss: 0.013500, lr: 0.000001, time: 21.823003\n","iter: 22920, loss: 0.020621, lr: 0.000001, time: 22.337116\n","iter: 22940, loss: 0.017117, lr: 0.000001, time: 21.790947\n","iter: 22960, loss: 0.014035, lr: 0.000001, time: 22.553103\n","iter: 22980, loss: 0.010024, lr: 0.000001, time: 21.817258\n","iter: 23000, loss: 0.019502, lr: 0.000001, time: 21.753301\n","iter: 23020, loss: 0.021729, lr: 0.000001, time: 22.390347\n","iter: 23040, loss: 0.019363, lr: 0.000001, time: 21.753159\n","iter: 23060, loss: 0.011939, lr: 0.000001, time: 22.586072\n","iter: 23080, loss: 0.017589, lr: 0.000001, time: 21.786713\n","iter: 23100, loss: 0.010226, lr: 0.000001, time: 22.584542\n","iter: 23120, loss: 0.013092, lr: 0.000001, time: 21.762058\n","iter: 23140, loss: 0.011769, lr: 0.000001, time: 22.527963\n","iter: 23160, loss: 0.013430, lr: 0.000001, time: 21.777120\n","iter: 23180, loss: 0.027051, lr: 0.000001, time: 22.567282\n","iter: 23200, loss: 0.019687, lr: 0.000001, time: 21.804150\n","iter: 23220, loss: 0.017218, lr: 0.000001, time: 22.553942\n","iter: 23240, loss: 0.013457, lr: 0.000001, time: 21.809835\n","iter: 23260, loss: 0.018882, lr: 0.000001, time: 22.487131\n","iter: 23280, loss: 0.021888, lr: 0.000001, time: 21.804989\n","iter: 23300, loss: 0.026809, lr: 0.000001, time: 22.509340\n","iter: 23320, loss: 0.013560, lr: 0.000001, time: 21.817070\n","iter: 23340, loss: 0.026628, lr: 0.000001, time: 22.464038\n","iter: 23360, loss: 0.039285, lr: 0.000001, time: 21.821381\n","iter: 23380, loss: 0.012436, lr: 0.000001, time: 22.555402\n","iter: 23400, loss: 0.014358, lr: 0.000001, time: 21.830826\n","iter: 23420, loss: 0.022080, lr: 0.000001, time: 22.510440\n","iter: 23440, loss: 0.019615, lr: 0.000001, time: 21.805016\n","iter: 23460, loss: 0.022780, lr: 0.000001, time: 22.538505\n","iter: 23480, loss: 0.016072, lr: 0.000001, time: 21.803382\n","iter: 23500, loss: 0.021343, lr: 0.000001, time: 22.383999\n","iter: 23520, loss: 0.010635, lr: 0.000001, time: 21.805601\n","iter: 23540, loss: 0.017454, lr: 0.000001, time: 22.502213\n","iter: 23560, loss: 0.012806, lr: 0.000001, time: 21.828882\n","iter: 23580, loss: 0.034124, lr: 0.000001, time: 22.586816\n","iter: 23600, loss: 0.005935, lr: 0.000001, time: 21.826901\n","iter: 23620, loss: 0.008392, lr: 0.000001, time: 22.536198\n","iter: 23640, loss: 0.015139, lr: 0.000001, time: 21.837931\n","iter: 23660, loss: 0.014303, lr: 0.000001, time: 22.351512\n","iter: 23680, loss: 0.016065, lr: 0.000001, time: 21.808656\n","iter: 23700, loss: 0.027004, lr: 0.000001, time: 22.569344\n","iter: 23720, loss: 0.009580, lr: 0.000001, time: 21.835494\n","iter: 23740, loss: 0.021204, lr: 0.000001, time: 22.533438\n","iter: 23760, loss: 0.019168, lr: 0.000001, time: 21.803681\n","iter: 23780, loss: 0.009479, lr: 0.000001, time: 22.506651\n","iter: 23800, loss: 0.009704, lr: 0.000001, time: 21.830413\n","iter: 23820, loss: 0.010592, lr: 0.000001, time: 21.744704\n","iter: 23840, loss: 0.008935, lr: 0.000001, time: 22.588480\n","iter: 23860, loss: 0.020228, lr: 0.000001, time: 21.770253\n","iter: 23880, loss: 0.008676, lr: 0.000001, time: 22.490842\n","iter: 23900, loss: 0.020654, lr: 0.000001, time: 21.693755\n","iter: 23920, loss: 0.012256, lr: 0.000001, time: 22.459842\n","iter: 23940, loss: 0.021808, lr: 0.000001, time: 21.655242\n","iter: 23960, loss: 0.034548, lr: 0.000001, time: 22.491448\n","iter: 23980, loss: 0.026570, lr: 0.000001, time: 21.670671\n","iter: 24000, loss: 0.011658, lr: 0.000001, time: 22.227890\n","iter: 24020, loss: 0.017027, lr: 0.000001, time: 21.649792\n","iter: 24040, loss: 0.014149, lr: 0.000001, time: 22.406217\n","iter: 24060, loss: 0.007799, lr: 0.000001, time: 21.645141\n","iter: 24080, loss: 0.022382, lr: 0.000001, time: 22.422796\n","iter: 24100, loss: 0.028815, lr: 0.000001, time: 21.684979\n","iter: 24120, loss: 0.013648, lr: 0.000001, time: 22.406014\n","iter: 24140, loss: 0.008602, lr: 0.000001, time: 21.672127\n","iter: 24160, loss: 0.032156, lr: 0.000001, time: 22.435305\n","iter: 24180, loss: 0.010741, lr: 0.000001, time: 21.706898\n","iter: 24200, loss: 0.011758, lr: 0.000001, time: 22.468460\n","iter: 24220, loss: 0.016892, lr: 0.000001, time: 21.699469\n","iter: 24240, loss: 0.027977, lr: 0.000001, time: 22.415420\n","iter: 24260, loss: 0.011378, lr: 0.000000, time: 21.675823\n","iter: 24280, loss: 0.018809, lr: 0.000000, time: 22.418899\n","iter: 24300, loss: 0.011870, lr: 0.000000, time: 21.702063\n","iter: 24320, loss: 0.026485, lr: 0.000000, time: 22.341429\n","iter: 24340, loss: 0.018489, lr: 0.000000, time: 21.643594\n","iter: 24360, loss: 0.008274, lr: 0.000000, time: 22.412726\n","iter: 24380, loss: 0.016240, lr: 0.000000, time: 21.702829\n","iter: 24400, loss: 0.022152, lr: 0.000000, time: 22.465497\n","iter: 24420, loss: 0.031943, lr: 0.000000, time: 21.701140\n","iter: 24440, loss: 0.014188, lr: 0.000000, time: 22.493934\n","iter: 24460, loss: 0.023468, lr: 0.000000, time: 21.734250\n","iter: 24480, loss: 0.018982, lr: 0.000000, time: 22.506183\n","iter: 24500, loss: 0.020053, lr: 0.000000, time: 21.749271\n","iter: 24520, loss: 0.006204, lr: 0.000000, time: 22.515690\n","iter: 24540, loss: 0.010157, lr: 0.000000, time: 21.761018\n","iter: 24560, loss: 0.028682, lr: 0.000000, time: 22.523288\n","iter: 24580, loss: 0.024105, lr: 0.000000, time: 21.756445\n","iter: 24600, loss: 0.019157, lr: 0.000000, time: 22.432233\n","iter: 24620, loss: 0.017099, lr: 0.000000, time: 21.787215\n","iter: 24640, loss: 0.017077, lr: 0.000000, time: 21.708031\n","iter: 24660, loss: 0.027463, lr: 0.000000, time: 22.500162\n","iter: 24680, loss: 0.038719, lr: 0.000000, time: 21.610490\n","iter: 24700, loss: 0.039197, lr: 0.000000, time: 22.461195\n","iter: 24720, loss: 0.010688, lr: 0.000000, time: 21.630118\n","iter: 24740, loss: 0.015081, lr: 0.000000, time: 22.393560\n","iter: 24760, loss: 0.011066, lr: 0.000000, time: 21.654981\n","iter: 24780, loss: 0.019288, lr: 0.000000, time: 22.372463\n","iter: 24800, loss: 0.022684, lr: 0.000000, time: 21.659051\n","iter: 24820, loss: 0.020643, lr: 0.000000, time: 22.448531\n","iter: 24840, loss: 0.028487, lr: 0.000000, time: 21.660005\n","iter: 24860, loss: 0.006693, lr: 0.000000, time: 22.427550\n","iter: 24880, loss: 0.016099, lr: 0.000000, time: 21.698875\n","iter: 24900, loss: 0.017792, lr: 0.000000, time: 22.303716\n","iter: 24920, loss: 0.010526, lr: 0.000000, time: 21.660802\n","iter: 24940, loss: 0.018029, lr: 0.000000, time: 22.308239\n","iter: 24960, loss: 0.025609, lr: 0.000000, time: 21.688433\n","iter: 24980, loss: 0.015611, lr: 0.000000, time: 22.383122\n","saving trained model\n","everything finished\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"41752697-12ff-4967-d0d3-9a540e11178a","executionInfo":{"status":"ok","timestamp":1574144525525,"user_tz":-60,"elapsed":181358,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"H5nZYyLshrHb","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["embed(dataset = X_query, fids = labels_query.fid, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Python 3.6.8 (default, Oct  7 2019, 12:59:55) \n","Type \"copyright\", \"credits\" or \"license\" for more information.\n","\n","IPython 5.5.0 -- An enhanced Interactive Python.\n","?         -> Introduction and overview of IPython's features.\n","%quickref -> Quick reference.\n","help      -> Python's own help system.\n","object?   -> Details about 'object', use 'object??' for extra details.\n","\n","In [1]: ?\n","\n","IPython -- An enhanced Interactive Python\n","=========================================\n","\n","IPython offers a fully compatible replacement for the standard Python\n","interpreter, with convenient shell features, special commands, command\n","history mechanism and output results caching.\n","\n","At your system command line, type 'ipython -h' to see the command line\n","options available. This document only describes interactive features.\n","\n","MAIN FEATURES\n","-------------\n","\n","* Access to the standard Python help with object docstrings and the Python\n","  manuals. Simply type 'help' (no quotes) to invoke it.\n","\n","* Magic commands: type %magic for information on the magic subsystem.\n","\n","* System command aliases, via the %alias command or the configuration file(s).\n","\n","* Dynamic object information:\n","\n","  Typing ?word or word? prints detailed information about an object. Certain\n","  long strings (code, etc.) get snipped in the center for brevity.\n","\n","  Typing ??word or word?? gives access to the full information without\n","  snipping long strings. Strings that are longer than the screen are printed\n","  through the less pager.\n","\n","  The ?/?? system gives access to the full source code for any object (if\n","  available), shows function prototypes and other useful information.\n","\n","  If you just want to see an object's docstring, type '%pdoc object' (without\n","  quotes, and without % if you have automagic on).\n","\n","* Tab completion in the local namespace:\n","\n","  At any time, hitting tab will complete any available python commands or\n","  variable names, and show you a list of the possible completions if there's\n","  no unambiguous one. It will also complete filenames in the current directory.\n","\n","* Search previous command history in multiple ways:\n","\n","  - Start typing, and then use arrow keys up/down or (Ctrl-p/Ctrl-n) to search\n","    through the history items that match what you've typed so far.\n","\n","  - Hit Ctrl-r: opens a search prompt. Begin typing and the system searches\n","    your history for lines that match what you've typed so far, completing as\n","    much as it can.\n","\n","  - %hist: search history by index.\n","\n","* Persistent command history across sessions.\n","\n","* Logging of input with the ability to save and restore a working session.\n","\n","* System shell with !. Typing !ls will run 'ls' in the current directory.\n","\n","* The reload command does a 'deep' reload of a module: changes made to the\n","  module since you imported will actually be available without having to exit.\n","\n","* Verbose and colored exception traceback printouts. See the magic xmode and\n","  xcolor functions for details (just type %magic).\n","\n","* Input caching system:\n","\n","  IPython offers numbered prompts (In/Out) with input and output caching. All\n","  input is saved and can be retrieved as variables (besides the usual arrow\n","  key recall).\n","\n","  The following GLOBAL variables always exist (so don't overwrite them!):\n","  _i: stores previous input.\n","  _ii: next previous.\n","  _iii: next-next previous.\n","  _ih : a list of all input _ih[n] is the input from line n.\n","\n","  Additionally, global variables named _i<n> are dynamically created (<n>\n","  being the prompt counter), such that _i<n> == _ih[<n>]\n","\n","  For example, what you typed at prompt 14 is available as _i14 and _ih[14].\n","\n","  You can create macros which contain multiple input lines from this history,\n","  for later re-execution, with the %macro function.\n","\n","  The history function %hist allows you to see any part of your input history\n","  by printing a range of the _i variables. Note that inputs which contain\n","  magic functions (%) appear in the history with a prepended comment. This is\n","  because they aren't really valid Python code, so you can't exec them.\n","\n","* Output caching system:\n","\n","  For output that is returned from actions, a system similar to the input\n","  cache exists but using _ instead of _i. Only actions that produce a result\n","  (NOT assignments, for example) are cached. If you are familiar with\n","  Mathematica, IPython's _ variables behave exactly like Mathematica's %\n","  variables.\n","\n","  The following GLOBAL variables always exist (so don't overwrite them!):\n","  _ (one underscore): previous output.\n","  __ (two underscores): next previous.\n","  ___ (three underscores): next-next previous.\n","\n","  Global variables named _<n> are dynamically created (<n> being the prompt\n","  counter), such that the result of output <n> is always available as _<n>.\n","\n","  Finally, a global dictionary named _oh exists with entries for all lines\n","  which generated output.\n","\n","* Directory history:\n","\n","  Your history of visited directories is kept in the global list _dh, and the\n","  magic %cd command can be used to go to any entry in that list.\n","\n","* Auto-parentheses and auto-quotes (adapted from Nathan Gray's LazyPython)\n","\n","  1. Auto-parentheses\n","        \n","     Callable objects (i.e. functions, methods, etc) can be invoked like\n","     this (notice the commas between the arguments)::\n","       \n","         In [1]: callable_ob arg1, arg2, arg3\n","       \n","     and the input will be translated to this::\n","       \n","         callable_ob(arg1, arg2, arg3)\n","       \n","     This feature is off by default (in rare cases it can produce\n","     undesirable side-effects), but you can activate it at the command-line\n","     by starting IPython with `--autocall 1`, set it permanently in your\n","     configuration file, or turn on at runtime with `%autocall 1`.\n","\n","     You can force auto-parentheses by using '/' as the first character\n","     of a line.  For example::\n","       \n","          In [1]: /globals             # becomes 'globals()'\n","       \n","     Note that the '/' MUST be the first character on the line!  This\n","     won't work::\n","       \n","          In [2]: print /globals    # syntax error\n","\n","     In most cases the automatic algorithm should work, so you should\n","     rarely need to explicitly invoke /. One notable exception is if you\n","     are trying to call a function with a list of tuples as arguments (the\n","     parenthesis will confuse IPython)::\n","       \n","          In [1]: zip (1,2,3),(4,5,6)  # won't work\n","       \n","     but this will work::\n","       \n","          In [2]: /zip (1,2,3),(4,5,6)\n","          ------> zip ((1,2,3),(4,5,6))\n","          Out[2]= [(1, 4), (2, 5), (3, 6)]\n","\n","     IPython tells you that it has altered your command line by\n","     displaying the new command line preceded by -->.  e.g.::\n","       \n","          In [18]: callable list\n","          -------> callable (list)\n","\n","  2. Auto-Quoting\n","    \n","     You can force auto-quoting of a function's arguments by using ',' as\n","     the first character of a line.  For example::\n","       \n","          In [1]: ,my_function /home/me   # becomes my_function(\"/home/me\")\n","\n","     If you use ';' instead, the whole argument is quoted as a single\n","     string (while ',' splits on whitespace)::\n","       \n","          In [2]: ,my_function a b c   # becomes my_function(\"a\",\"b\",\"c\")\n","          In [3]: ;my_function a b c   # becomes my_function(\"a b c\")\n","\n","     Note that the ',' MUST be the first character on the line!  This\n","     won't work::\n","       \n","          In [4]: x = ,my_function /home/me    # syntax error\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"ba1e9ee4-7556-42f5-8593-415a2b5e66c5","executionInfo":{"status":"ok","timestamp":1574145554268,"user_tz":-60,"elapsed":1210092,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"5buITy7mhrHf","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["embed(dataset = X_test, fids = labels_test.fid, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 616 / 617  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"8233be9a-c372-4ad5-d16f-5a6a80775814","executionInfo":{"status":"ok","timestamp":1574145715143,"user_tz":-60,"elapsed":1370944,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"id":"tqNX7KDChrHh","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:35<00:00, 21.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.7430370803791962, cmc is: [0.8770784  0.9141924  0.93171024 0.94447744 0.9513064 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J9OrIRc2k4Sd","colab_type":"text"},"source":["### Model 5\n"]},{"cell_type":"code","metadata":{"id":"2qYrMMLp4pSK","colab_type":"code","colab":{}},"source":["torch.multiprocessing.set_sharing_strategy('file_system')\n","if not os.path.exists('./res'): os.makedirs('./res')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KW7xCSIFk3g7","colab_type":"code","colab":{}},"source":["model_num = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zTX9DX0Mk_oZ","colab":{}},"source":["filter_size = 3\n","net = models_lpf.resnet.resnet50(filter_size=filter_size)\n","net.load_state_dict(torch.load('models_lpf/resnet50_lpf%i.pth.tar'%filter_size)['state_dict'])\n","model = torch.nn.Sequential(*(list(net.children())[:-2]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XjUlncFAk_od","colab":{}},"source":["class DenseNormReLU(nn.Module):\n","    def __init__(self, in_feats, out_feats, *args, **kwargs):\n","        super(DenseNormReLU, self).__init__(*args, **kwargs)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.dense = nn.Linear(in_features = in_feats, out_features = out_feats).to(self.device)\n","        self.bn = nn.BatchNorm1d(out_feats).to(self.device)\n","        self.relu = nn.ReLU(inplace = True).to(self.device)\n","\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return x\n","  \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fc_head = DenseNormReLU(in_feats = 2048, out_feats = 1024)\n","embed = nn.Linear(in_features = 1024, out_features = 128).to(device)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SiwkK5yhlPi4","colab_type":"code","colab":{}},"source":["blur = nn.Sequential(Downsample(filt_size = filter_size, channels = 2048, stride = 1),nn.ReLU(inplace=True), )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_L73vzaBk_of","colab":{}},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.base = model\n","    self.blur = blur\n","    self.fc_head = fc_head\n","    self.embed = embed\n","\n","  def forward(self, x):\n","    # shape [N, C, H, W]\n","    x = self.base(x)\n","    x = self.blur(x)\n","    x = F.avg_pool2d(x, x.size()[2:])\n","    x = x.contiguous().view(-1, 2048)\n","    # shape [N, C]\n","    x = self.fc_head(x)\n","    x = self.embed(x)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cdNLcz3pk_oh","outputId":"41cac7fc-165e-4d0c-f979-9c95b383f67e","executionInfo":{"status":"ok","timestamp":1574757468659,"user_tz":-60,"elapsed":967,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model()\n","#model = Model().to(device)\n","model = model.cuda()\n","net = nn.DataParallel(model)\n","summary(net, (3, 128,64))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 32]           9,408\n","       BatchNorm2d-2           [-1, 64, 64, 32]             128\n","              ReLU-3           [-1, 64, 64, 32]               0\n","         MaxPool2d-4           [-1, 64, 63, 31]               0\n","   ReflectionPad2d-5           [-1, 64, 65, 33]               0\n","        Downsample-6           [-1, 64, 32, 16]               0\n","            Conv2d-7           [-1, 64, 32, 16]           4,096\n","       BatchNorm2d-8           [-1, 64, 32, 16]             128\n","              ReLU-9           [-1, 64, 32, 16]               0\n","           Conv2d-10           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-11           [-1, 64, 32, 16]             128\n","             ReLU-12           [-1, 64, 32, 16]               0\n","           Conv2d-13          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-14          [-1, 256, 32, 16]             512\n","           Conv2d-15          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-16          [-1, 256, 32, 16]             512\n","             ReLU-17          [-1, 256, 32, 16]               0\n","       Bottleneck-18          [-1, 256, 32, 16]               0\n","           Conv2d-19           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-20           [-1, 64, 32, 16]             128\n","             ReLU-21           [-1, 64, 32, 16]               0\n","           Conv2d-22           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-23           [-1, 64, 32, 16]             128\n","             ReLU-24           [-1, 64, 32, 16]               0\n","           Conv2d-25          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-26          [-1, 256, 32, 16]             512\n","             ReLU-27          [-1, 256, 32, 16]               0\n","       Bottleneck-28          [-1, 256, 32, 16]               0\n","           Conv2d-29           [-1, 64, 32, 16]          16,384\n","      BatchNorm2d-30           [-1, 64, 32, 16]             128\n","             ReLU-31           [-1, 64, 32, 16]               0\n","           Conv2d-32           [-1, 64, 32, 16]          36,864\n","      BatchNorm2d-33           [-1, 64, 32, 16]             128\n","             ReLU-34           [-1, 64, 32, 16]               0\n","           Conv2d-35          [-1, 256, 32, 16]          16,384\n","      BatchNorm2d-36          [-1, 256, 32, 16]             512\n","             ReLU-37          [-1, 256, 32, 16]               0\n","       Bottleneck-38          [-1, 256, 32, 16]               0\n","           Conv2d-39          [-1, 128, 32, 16]          32,768\n","      BatchNorm2d-40          [-1, 128, 32, 16]             256\n","             ReLU-41          [-1, 128, 32, 16]               0\n","           Conv2d-42          [-1, 128, 32, 16]         147,456\n","      BatchNorm2d-43          [-1, 128, 32, 16]             256\n","             ReLU-44          [-1, 128, 32, 16]               0\n","  ReflectionPad2d-45          [-1, 128, 34, 18]               0\n","       Downsample-46           [-1, 128, 16, 8]               0\n","           Conv2d-47           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-48           [-1, 512, 16, 8]           1,024\n","  ReflectionPad2d-49          [-1, 256, 34, 18]               0\n","       Downsample-50           [-1, 256, 16, 8]               0\n","           Conv2d-51           [-1, 512, 16, 8]         131,072\n","      BatchNorm2d-52           [-1, 512, 16, 8]           1,024\n","             ReLU-53           [-1, 512, 16, 8]               0\n","       Bottleneck-54           [-1, 512, 16, 8]               0\n","           Conv2d-55           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-56           [-1, 128, 16, 8]             256\n","             ReLU-57           [-1, 128, 16, 8]               0\n","           Conv2d-58           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-59           [-1, 128, 16, 8]             256\n","             ReLU-60           [-1, 128, 16, 8]               0\n","           Conv2d-61           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-62           [-1, 512, 16, 8]           1,024\n","             ReLU-63           [-1, 512, 16, 8]               0\n","       Bottleneck-64           [-1, 512, 16, 8]               0\n","           Conv2d-65           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-66           [-1, 128, 16, 8]             256\n","             ReLU-67           [-1, 128, 16, 8]               0\n","           Conv2d-68           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-69           [-1, 128, 16, 8]             256\n","             ReLU-70           [-1, 128, 16, 8]               0\n","           Conv2d-71           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-72           [-1, 512, 16, 8]           1,024\n","             ReLU-73           [-1, 512, 16, 8]               0\n","       Bottleneck-74           [-1, 512, 16, 8]               0\n","           Conv2d-75           [-1, 128, 16, 8]          65,536\n","      BatchNorm2d-76           [-1, 128, 16, 8]             256\n","             ReLU-77           [-1, 128, 16, 8]               0\n","           Conv2d-78           [-1, 128, 16, 8]         147,456\n","      BatchNorm2d-79           [-1, 128, 16, 8]             256\n","             ReLU-80           [-1, 128, 16, 8]               0\n","           Conv2d-81           [-1, 512, 16, 8]          65,536\n","      BatchNorm2d-82           [-1, 512, 16, 8]           1,024\n","             ReLU-83           [-1, 512, 16, 8]               0\n","       Bottleneck-84           [-1, 512, 16, 8]               0\n","           Conv2d-85           [-1, 256, 16, 8]         131,072\n","      BatchNorm2d-86           [-1, 256, 16, 8]             512\n","             ReLU-87           [-1, 256, 16, 8]               0\n","           Conv2d-88           [-1, 256, 16, 8]         589,824\n","      BatchNorm2d-89           [-1, 256, 16, 8]             512\n","             ReLU-90           [-1, 256, 16, 8]               0\n","  ReflectionPad2d-91          [-1, 256, 18, 10]               0\n","       Downsample-92            [-1, 256, 8, 4]               0\n","           Conv2d-93           [-1, 1024, 8, 4]         262,144\n","      BatchNorm2d-94           [-1, 1024, 8, 4]           2,048\n","  ReflectionPad2d-95          [-1, 512, 18, 10]               0\n","       Downsample-96            [-1, 512, 8, 4]               0\n","           Conv2d-97           [-1, 1024, 8, 4]         524,288\n","      BatchNorm2d-98           [-1, 1024, 8, 4]           2,048\n","             ReLU-99           [-1, 1024, 8, 4]               0\n","      Bottleneck-100           [-1, 1024, 8, 4]               0\n","          Conv2d-101            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-102            [-1, 256, 8, 4]             512\n","            ReLU-103            [-1, 256, 8, 4]               0\n","          Conv2d-104            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-105            [-1, 256, 8, 4]             512\n","            ReLU-106            [-1, 256, 8, 4]               0\n","          Conv2d-107           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-108           [-1, 1024, 8, 4]           2,048\n","            ReLU-109           [-1, 1024, 8, 4]               0\n","      Bottleneck-110           [-1, 1024, 8, 4]               0\n","          Conv2d-111            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-112            [-1, 256, 8, 4]             512\n","            ReLU-113            [-1, 256, 8, 4]               0\n","          Conv2d-114            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-115            [-1, 256, 8, 4]             512\n","            ReLU-116            [-1, 256, 8, 4]               0\n","          Conv2d-117           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-118           [-1, 1024, 8, 4]           2,048\n","            ReLU-119           [-1, 1024, 8, 4]               0\n","      Bottleneck-120           [-1, 1024, 8, 4]               0\n","          Conv2d-121            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-122            [-1, 256, 8, 4]             512\n","            ReLU-123            [-1, 256, 8, 4]               0\n","          Conv2d-124            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-125            [-1, 256, 8, 4]             512\n","            ReLU-126            [-1, 256, 8, 4]               0\n","          Conv2d-127           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-128           [-1, 1024, 8, 4]           2,048\n","            ReLU-129           [-1, 1024, 8, 4]               0\n","      Bottleneck-130           [-1, 1024, 8, 4]               0\n","          Conv2d-131            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-132            [-1, 256, 8, 4]             512\n","            ReLU-133            [-1, 256, 8, 4]               0\n","          Conv2d-134            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-135            [-1, 256, 8, 4]             512\n","            ReLU-136            [-1, 256, 8, 4]               0\n","          Conv2d-137           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-138           [-1, 1024, 8, 4]           2,048\n","            ReLU-139           [-1, 1024, 8, 4]               0\n","      Bottleneck-140           [-1, 1024, 8, 4]               0\n","          Conv2d-141            [-1, 256, 8, 4]         262,144\n","     BatchNorm2d-142            [-1, 256, 8, 4]             512\n","            ReLU-143            [-1, 256, 8, 4]               0\n","          Conv2d-144            [-1, 256, 8, 4]         589,824\n","     BatchNorm2d-145            [-1, 256, 8, 4]             512\n","            ReLU-146            [-1, 256, 8, 4]               0\n","          Conv2d-147           [-1, 1024, 8, 4]         262,144\n","     BatchNorm2d-148           [-1, 1024, 8, 4]           2,048\n","            ReLU-149           [-1, 1024, 8, 4]               0\n","      Bottleneck-150           [-1, 1024, 8, 4]               0\n","          Conv2d-151            [-1, 512, 8, 4]         524,288\n","     BatchNorm2d-152            [-1, 512, 8, 4]           1,024\n","            ReLU-153            [-1, 512, 8, 4]               0\n","          Conv2d-154            [-1, 512, 8, 4]       2,359,296\n","     BatchNorm2d-155            [-1, 512, 8, 4]           1,024\n","            ReLU-156            [-1, 512, 8, 4]               0\n"," ReflectionPad2d-157           [-1, 512, 10, 6]               0\n","      Downsample-158            [-1, 512, 4, 2]               0\n","          Conv2d-159           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 4, 2]           4,096\n"," ReflectionPad2d-161          [-1, 1024, 10, 6]               0\n","      Downsample-162           [-1, 1024, 4, 2]               0\n","          Conv2d-163           [-1, 2048, 4, 2]       2,097,152\n","     BatchNorm2d-164           [-1, 2048, 4, 2]           4,096\n","            ReLU-165           [-1, 2048, 4, 2]               0\n","      Bottleneck-166           [-1, 2048, 4, 2]               0\n","          Conv2d-167            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-168            [-1, 512, 4, 2]           1,024\n","            ReLU-169            [-1, 512, 4, 2]               0\n","          Conv2d-170            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-171            [-1, 512, 4, 2]           1,024\n","            ReLU-172            [-1, 512, 4, 2]               0\n","          Conv2d-173           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-174           [-1, 2048, 4, 2]           4,096\n","            ReLU-175           [-1, 2048, 4, 2]               0\n","      Bottleneck-176           [-1, 2048, 4, 2]               0\n","          Conv2d-177            [-1, 512, 4, 2]       1,048,576\n","     BatchNorm2d-178            [-1, 512, 4, 2]           1,024\n","            ReLU-179            [-1, 512, 4, 2]               0\n","          Conv2d-180            [-1, 512, 4, 2]       2,359,296\n","     BatchNorm2d-181            [-1, 512, 4, 2]           1,024\n","            ReLU-182            [-1, 512, 4, 2]               0\n","          Conv2d-183           [-1, 2048, 4, 2]       1,048,576\n","     BatchNorm2d-184           [-1, 2048, 4, 2]           4,096\n","            ReLU-185           [-1, 2048, 4, 2]               0\n","      Bottleneck-186           [-1, 2048, 4, 2]               0\n"," ReflectionPad2d-187           [-1, 2048, 6, 4]               0\n","      Downsample-188           [-1, 2048, 4, 2]               0\n","            ReLU-189           [-1, 2048, 4, 2]               0\n","          Linear-190                 [-1, 1024]       2,098,176\n","     BatchNorm1d-191                 [-1, 1024]           2,048\n","            ReLU-192                 [-1, 1024]               0\n","   DenseNormReLU-193                 [-1, 1024]               0\n","          Linear-194                  [-1, 128]         131,200\n","           Model-195                  [-1, 128]               0\n","================================================================\n","Total params: 25,739,456\n","Trainable params: 25,739,456\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.09\n","Forward/backward pass size (MB): 55.62\n","Params size (MB): 98.19\n","Estimated Total Size (MB): 153.90\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CKagf4GS4OLV","outputId":"b325919e-407e-4880-df9b-c369c404c6af","executionInfo":{"status":"error","timestamp":1574757479421,"user_tz":-60,"elapsed":5482,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["train(net = net, model_num = model_num)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["creating optimizer\n","start training ...\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-9268b36008b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-c610138c4bb9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, model_num)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Thesis re-id/triplet-reid-master/own_code/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# adjust optimizer parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ymTf9cYG4OLZ","outputId":"4881e30d-d4a4-4579-9ad0-f7de668b780c","executionInfo":{"status":"ok","timestamp":1574757889889,"user_tz":-60,"elapsed":388487,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["create_emb(dataset = X_query, fids = labels_query.fid, model_num = model_num, store_path= \"./res/emb_query{}.pkl\".format(model_num))\n","create_emb(dataset = X_test, fids = labels_test.fid, model_num = model_num, store_path=\"./res/emb_test{}.pkl\".format(model_num))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["=======>  processing iter 105 / 106  ...   completed\n","=======>  processing iter 616 / 617  ...   completed\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p6a6Kpq94OLf","outputId":"9e960042-313d-4e82-fca3-139b1027ee56","executionInfo":{"status":"ok","timestamp":1574758048655,"user_tz":-60,"elapsed":539935,"user":{"displayName":"Rens Lokerse","photoUrl":"","userId":"09374055632339743955"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["test_embs = \"./res/emb_test{}.pkl\".format(model_num)\n","query_embs = \"./res/emb_query{}.pkl\".format(model_num)\n","cmc_rank = 5\n","evaluate(test_embs = test_embs, query_embs = query_embs, cmc_rank = cmc_rank)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 3368/3368 [02:32<00:00, 22.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["mAP is: 0.747139206882985, cmc is: [0.87321854 0.91567695 0.9340855  0.94447744 0.952791  ]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}